"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[2361],{3492:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module-3/Cognitive-Architectures/lesson-3.1-cognitive-architectures-for-robot-intelligence","title":"Lesson 3.1 - Cognitive Architectures for Robot Intelligence","description":"Design cognitive architectures for humanoid robot decision-making","source":"@site/docs/module-3/03-Cognitive-Architectures/lesson-3.1-cognitive-architectures-for-robot-intelligence.md","sourceDirName":"module-3/03-Cognitive-Architectures","slug":"/module-3/Cognitive-Architectures/lesson-3.1-cognitive-architectures-for-robot-intelligence","permalink":"/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-3/Cognitive-Architectures/lesson-3.1-cognitive-architectures-for-robot-intelligence","draft":false,"unlisted":false,"editUrl":"https://github.com/AmanNazim/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/edit/main/physical-ai-humanoid-robotics-book/docs/module-3/03-Cognitive-Architectures/lesson-3.1-cognitive-architectures-for-robot-intelligence.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Lesson 3.1 - Cognitive Architectures for Robot Intelligence","sidebar_position":1,"description":"Design cognitive architectures for humanoid robot decision-making"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: Cognitive Architectures","permalink":"/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-3/Cognitive-Architectures/"},"next":{"title":"Lesson 3.2 - Perception Processing Pipelines","permalink":"/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-3/Cognitive-Architectures/lesson-3.2-perception-processing-pipelines"}}');var s=i(4848),o=i(8453);const r={title:"Lesson 3.1 - Cognitive Architectures for Robot Intelligence",sidebar_position:1,description:"Design cognitive architectures for humanoid robot decision-making"},a="Lesson 3.1: Cognitive Architectures for Robot Intelligence",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Cognitive Architectures",id:"introduction-to-cognitive-architectures",level:2},{value:"Understanding Cognitive Architecture Components",id:"understanding-cognitive-architecture-components",level:2},{value:"Core Components of Cognitive Architecture",id:"core-components-of-cognitive-architecture",level:3},{value:"Types of Cognitive Architectures",id:"types-of-cognitive-architectures",level:3},{value:"Designing Cognitive Architectures for Humanoid Robots",id:"designing-cognitive-architectures-for-humanoid-robots",level:2},{value:"Architecture Design Principles",id:"architecture-design-principles",level:3},{value:"Cognitive Architecture Patterns",id:"cognitive-architecture-patterns",level:3},{value:"Implementing Cognitive Architecture with NVIDIA Isaac",id:"implementing-cognitive-architecture-with-nvidia-isaac",level:2},{value:"Setting Up the Environment",id:"setting-up-the-environment",level:3},{value:"Basic Cognitive Component Structure",id:"basic-cognitive-component-structure",level:3},{value:"Advanced Cognitive Architecture with NVIDIA GPU Acceleration",id:"advanced-cognitive-architecture-with-nvidia-gpu-acceleration",level:3},{value:"ROS2 Launch Configuration",id:"ros2-launch-configuration",level:3},{value:"Modular Cognitive Components",id:"modular-cognitive-components",level:2},{value:"Creating Reusable Cognitive Modules",id:"creating-reusable-cognitive-modules",level:3},{value:"Navigation Cognitive Module",id:"navigation-cognitive-module",level:4},{value:"Manipulation Cognitive Module",id:"manipulation-cognitive-module",level:4},{value:"Integrating Cognitive Components with Isaac Tools",id:"integrating-cognitive-components-with-isaac-tools",level:2},{value:"Using Isaac ROS for Cognitive Processing",id:"using-isaac-ros-for-cognitive-processing",level:3},{value:"Cognitive Architecture Validation",id:"cognitive-architecture-validation",level:3},{value:"Best Practices for Cognitive Architecture Design",id:"best-practices-for-cognitive-architecture-design",level:2},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Safety Considerations",id:"safety-considerations",level:3},{value:"Modularity and Scalability",id:"modularity-and-scalability",level:3},{value:"Summary",id:"summary",level:2}];function g(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"lesson-31-cognitive-architectures-for-robot-intelligence",children:"Lesson 3.1: Cognitive Architectures for Robot Intelligence"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Design cognitive architectures for humanoid robot decision-making"}),"\n",(0,s.jsx)(n.li,{children:"Implement AI reasoning systems for autonomous behavior"}),"\n",(0,s.jsx)(n.li,{children:"Create modular cognitive components for different robot tasks"}),"\n",(0,s.jsx)(n.li,{children:"Understand cognitive architecture frameworks and decision-making components"}),"\n",(0,s.jsx)(n.li,{children:"Utilize Isaac cognitive architecture tools, ROS2, and NVIDIA GPU for AI processing"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"introduction-to-cognitive-architectures",children:"Introduction to Cognitive Architectures"}),"\n",(0,s.jsx)(n.p,{children:"Cognitive architectures represent the foundational framework that enables humanoid robots to exhibit intelligent behavior. Unlike simple reactive systems, cognitive architectures incorporate memory, reasoning, planning, and learning capabilities that allow robots to process complex information and make informed decisions in dynamic environments."}),"\n",(0,s.jsx)(n.p,{children:'In the context of humanoid robotics, cognitive architectures serve as the "brain" of the robot, orchestrating perception, reasoning, and action in a coordinated manner. These architectures must be capable of handling multiple concurrent processes, managing attention and resources, and adapting to changing environmental conditions.'}),"\n",(0,s.jsx)(n.p,{children:"The NVIDIA Isaac ecosystem provides powerful tools for implementing cognitive architectures that leverage hardware acceleration for real-time performance. This lesson will guide you through the design and implementation of cognitive architectures specifically tailored for humanoid robot decision-making."}),"\n",(0,s.jsx)(n.h2,{id:"understanding-cognitive-architecture-components",children:"Understanding Cognitive Architecture Components"}),"\n",(0,s.jsx)(n.h3,{id:"core-components-of-cognitive-architecture",children:"Core Components of Cognitive Architecture"}),"\n",(0,s.jsx)(n.p,{children:"A cognitive architecture for humanoid robots typically consists of several interconnected components:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Perception Module"}),": Processes sensor data from cameras, LiDAR, IMUs, and other sensors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory Systems"}),": Short-term and long-term memory for storing experiences and knowledge"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reasoning Engine"}),": Logic-based or neural systems for decision-making"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Planning Module"}),": Generates sequences of actions to achieve goals"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Selection"}),": Determines which actions to execute based on current state and goals"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Learning Mechanisms"}),": Adaptation systems that improve performance over time"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"types-of-cognitive-architectures",children:"Types of Cognitive Architectures"}),"\n",(0,s.jsx)(n.p,{children:"There are several approaches to cognitive architectures, each with distinct advantages:"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Subsumption Architecture"}),": Hierarchical layers where higher-level behaviors can interrupt lower-level ones. This architecture is excellent for reactive behaviors and ensures safety by having emergency responses at the lowest level."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Three-Layer Architecture"}),": Divides functionality into reactive, executive, and deliberative layers. The reactive layer handles immediate responses, the executive layer manages ongoing activities, and the deliberative layer performs complex planning."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Hybrid Deliberative/Reactive Architecture"}),": Combines symbolic reasoning with reactive systems, allowing for both complex planning and quick responses to environmental changes."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Blackboard Architecture"}),": Multiple knowledge sources contribute to a shared workspace, with different specialists solving parts of the problem cooperatively."]}),"\n",(0,s.jsx)(n.h2,{id:"designing-cognitive-architectures-for-humanoid-robots",children:"Designing Cognitive Architectures for Humanoid Robots"}),"\n",(0,s.jsx)(n.h3,{id:"architecture-design-principles",children:"Architecture Design Principles"}),"\n",(0,s.jsx)(n.p,{children:"When designing cognitive architectures for humanoid robots, several key principles must be considered:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Modularity"}),": Components should be loosely coupled and independently replaceable"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time Performance"}),": The architecture must handle sensor data and generate responses within required timeframes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scalability"}),": The system should accommodate additional capabilities without major redesign"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robustness"}),": The architecture must continue functioning despite component failures"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Maintainability"}),": Clear interfaces and documentation facilitate system updates"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"cognitive-architecture-patterns",children:"Cognitive Architecture Patterns"}),"\n",(0,s.jsx)(n.p,{children:"Let's explore a practical cognitive architecture pattern suitable for humanoid robots using the NVIDIA Isaac ecosystem:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"                    +------------------+\n                    |   Goal Manager   |\n                    +--------+---------+\n                             |\n                    +--------v---------+\n                    |  Planner/Reasoner|\n                    +--------+---------+\n                             |\n         +--------------------+--------------------+\n         |                    |                    |\n+--------v---------+  +-------v----------+  +------v-------+\n|  Navigation Task |  | Manipulation Task|  | Interaction  |\n|     Handler      |  |     Handler      |  |    Handler   |\n+--------+---------+  +-------+----------+  +------+-------+\n         |                    |                   |\n         +--------------------+-------------------+\n                             |\n                    +--------v---------+\n                    |  Action Executor |\n                    +------------------+\n                             |\n                    +--------v---------+\n                    |   Hardware I/O   |\n                    +------------------+\n"})}),"\n",(0,s.jsx)(n.p,{children:"This pattern separates concerns into specialized modules while maintaining coordination through the central planner/reasoner."}),"\n",(0,s.jsx)(n.h2,{id:"implementing-cognitive-architecture-with-nvidia-isaac",children:"Implementing Cognitive Architecture with NVIDIA Isaac"}),"\n",(0,s.jsx)(n.h3,{id:"setting-up-the-environment",children:"Setting Up the Environment"}),"\n",(0,s.jsx)(n.p,{children:"To implement cognitive architectures using NVIDIA Isaac, we need to establish the foundational environment:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Ensure Isaac ROS packages are installed\nsudo apt-get update\nsudo apt-get install ros-humble-isaac-ros-common\nsudo apt-get install ros-humble-isaac-ros-visual-slam\nsudo apt-get install ros-humble-isaac-ros-augment-rtx\n"})}),"\n",(0,s.jsx)(n.h3,{id:"basic-cognitive-component-structure",children:"Basic Cognitive Component Structure"}),"\n",(0,s.jsx)(n.p,{children:"Let's create a basic cognitive component that implements the perception-processing-action cycle:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-cpp",children:"// cognitive_component.hpp\n#ifndef COGNITIVE_COMPONENT_HPP\n#define COGNITIVE_COMPONENT_HPP\n\n#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <geometry_msgs/msg/twist.hpp>\n#include <std_msgs/msg/string.hpp>\n#include <memory>\n\nnamespace cognitive_architecture {\n\nclass CognitiveComponent : public rclcpp::Node {\npublic:\n    CognitiveComponent();\n\nprivate:\n    // Publishers and subscribers\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr perception_sub_;\n    rclcpp::Publisher<geometry_msgs::msg::Twist>::SharedPtr action_pub_;\n    rclcpp::Publisher<std_msgs::msg::String>::SharedPtr status_pub_;\n\n    // Memory and reasoning components\n    std::vector<float> working_memory_;\n    std::vector<float> long_term_memory_;\n\n    // Callback functions\n    void perceptionCallback(const sensor_msgs::msg::Image::SharedPtr msg);\n    void executeReasoning();\n    void publishAction();\n    void updateMemory();\n};\n\n} // namespace cognitive_architecture\n\n#endif // COGNITIVE_COMPONENT_HPP\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-cpp",children:'// cognitive_component.cpp\n#include "cognitive_component.hpp"\n#include <opencv2/opencv.hpp>\n#include <cv_bridge/cv_bridge.h>\n\nnamespace cognitive_architecture {\n\nCognitiveComponent::CognitiveComponent()\n    : Node("cognitive_component") {\n\n    // Initialize publishers and subscribers\n    perception_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n        "camera/image_raw", 10,\n        std::bind(&CognitiveComponent::perceptionCallback, this, std::placeholders::_1));\n\n    action_pub_ = this->create_publisher<geometry_msgs::msg::Twist>(\n        "cmd_vel", 10);\n\n    status_pub_ = this->create_publisher<std_msgs::msg::String>(\n        "cognitive_status", 10);\n\n    RCLCPP_INFO(this->get_logger(), "Cognitive Component initialized");\n}\n\nvoid CognitiveComponent::perceptionCallback(const sensor_msgs::msg::Image::SharedPtr msg) {\n    try {\n        cv_bridge::CvImagePtr cv_ptr = cv_bridge::toCvCopy(msg, sensor_msgs::image_encodings::BGR8);\n\n        // Process the image data\n        cv::Mat image = cv_ptr->image;\n\n        // Extract features for cognitive processing\n        std::vector<cv::KeyPoint> keypoints;\n        cv::Mat descriptors;\n        cv::Ptr<cv::ORB> detector = cv::ORB::create();\n        detector->detectAndCompute(image, cv::noArray(), keypoints, descriptors);\n\n        // Store features in working memory\n        working_memory_.resize(keypoints.size() * 2);\n        for (size_t i = 0; i < keypoints.size(); ++i) {\n            working_memory_[i * 2] = keypoints[i].pt.x;\n            working_memory_[i * 2 + 1] = keypoints[i].pt.y;\n        }\n\n        // Trigger reasoning process\n        executeReasoning();\n\n    } catch (cv_bridge::Exception& e) {\n        RCLCPP_ERROR(this->get_logger(), "cv_bridge exception: %s", e.what());\n    }\n}\n\nvoid CognitiveComponent::executeReasoning() {\n    // Simple reasoning logic\n    // In a real system, this would interface with more complex AI models\n\n    if (!working_memory_.empty()) {\n        // Calculate center of interest based on features\n        float avg_x = 0, avg_y = 0;\n        int count = 0;\n\n        for (size_t i = 0; i < working_memory_.size(); i += 2) {\n            avg_x += working_memory_[i];\n            avg_y += working_memory_[i + 1];\n            count++;\n        }\n\n        if (count > 0) {\n            avg_x /= count;\n            avg_y /= count;\n\n            // Determine action based on feature position\n            geometry_msgs::msg::Twist cmd_vel;\n\n            // Move towards the center of interest\n            if (avg_x < 200) {\n                cmd_vel.angular.z = 0.5;  // Turn right\n            } else if (avg_x > 400) {\n                cmd_vel.angular.z = -0.5; // Turn left\n            } else {\n                cmd_vel.linear.x = 0.2;   // Move forward\n            }\n\n            action_pub_->publish(cmd_vel);\n        }\n    }\n\n    // Update memory with current state\n    updateMemory();\n}\n\nvoid CognitiveComponent::publishAction() {\n    // Actions are published in executeReasoning()\n    // This method could handle additional action publishing logic\n}\n\nvoid CognitiveComponent::updateMemory() {\n    // Update long-term memory with significant observations\n    if (working_memory_.size() > 10) {  // Significant observation threshold\n        long_term_memory_ = working_memory_;  // Store in long-term memory\n\n        // Publish status update\n        std_msgs::msg::String status_msg;\n        status_msg.data = "Cognitive component updated memory with " +\n                         std::to_string(working_memory_.size()/2) + " features";\n        status_pub_->publish(status_msg);\n    }\n}\n\n} // namespace cognitive_architecture\n\nint main(int argc, char * argv[]) {\n    rclcpp::init(argc, argv);\n    rclcpp::spin(std::make_shared<cognitive_architecture::CognitiveComponent>());\n    rclcpp::shutdown();\n    return 0;\n}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"advanced-cognitive-architecture-with-nvidia-gpu-acceleration",children:"Advanced Cognitive Architecture with NVIDIA GPU Acceleration"}),"\n",(0,s.jsx)(n.p,{children:"For more sophisticated cognitive architectures, we can leverage NVIDIA GPUs for AI processing. Here's an example of integrating deep learning components:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import Twist\nfrom std_msgs.msg import String\nimport numpy as np\nimport cv2\nfrom cv_bridge import CvBridge\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DeepCognitiveArchitecture(Node):\n    def __init__(self):\n        super().__init__('deep_cognitive_architecture')\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Subscribers and publishers\n        self.perception_sub = self.create_subscription(\n            Image, 'camera/image_raw', self.perception_callback, 10)\n        self.action_pub = self.create_publisher(Twist, 'cmd_vel', 10)\n        self.status_pub = self.create_publisher(String, 'cognitive_status', 10)\n\n        # Initialize cognitive components\n        self.perception_processor = PerceptionProcessor()\n        self.reasoning_engine = ReasoningEngine()\n        self.action_selector = ActionSelector()\n\n        # Working memory\n        self.working_memory = {}\n\n        # Check for GPU availability\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.get_logger().info(f'Cognitive architecture using device: {self.device}')\n\n        # Load deep learning models to GPU\n        self.perception_processor.to(self.device)\n        self.reasoning_engine.to(self.device)\n\n        self.get_logger().info('Deep Cognitive Architecture initialized')\n\n    def perception_callback(self, msg):\n        try:\n            # Convert ROS image to OpenCV format\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Preprocess image for deep learning\n            image_tensor = self.preprocess_image(cv_image)\n\n            # Run perception processing on GPU\n            with torch.no_grad():\n                features = self.perception_processor(image_tensor.to(self.device))\n\n            # Store in working memory\n            self.working_memory['features'] = features.cpu().numpy()\n            self.working_memory['timestamp'] = self.get_clock().now().nanoseconds\n\n            # Execute reasoning\n            self.execute_reasoning()\n\n        except Exception as e:\n            self.get_logger().error(f'Perception callback error: {str(e)}')\n\n    def preprocess_image(self, image):\n        # Resize and normalize image\n        resized = cv2.resize(image, (224, 224))\n        normalized = resized.astype(np.float32) / 255.0\n        tensor = torch.from_numpy(normalized).permute(2, 0, 1).unsqueeze(0)\n        return tensor\n\n    def execute_reasoning(self):\n        if 'features' not in self.working_memory:\n            return\n\n        # Convert features back to tensor and move to GPU\n        features_tensor = torch.from_numpy(self.working_memory['features']).to(self.device)\n\n        # Run reasoning on GPU\n        with torch.no_grad():\n            decision = self.reasoning_engine(features_tensor)\n\n        # Select appropriate action\n        action = self.action_selector.select_action(decision)\n\n        # Publish action\n        twist_msg = Twist()\n        twist_msg.linear.x = action['linear']\n        twist_msg.angular.z = action['angular']\n        self.action_pub.publish(twist_msg)\n\n        # Log status\n        status_msg = String()\n        status_msg.data = f'Decision made: linear={action[\"linear\"]:.2f}, angular={action[\"angular\"]:.2f}'\n        self.status_pub.publish(status_msg)\n\nclass PerceptionProcessor(nn.Module):\n    def __init__(self):\n        super(PerceptionProcessor, self).__init__()\n\n        # Simple CNN for feature extraction\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((7, 7))\n        )\n\n        self.fc_layer = nn.Linear(128 * 7 * 7, 512)\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = x.view(x.size(0), -1)  # Flatten\n        x = self.fc_layer(x)\n        return x\n\nclass ReasoningEngine(nn.Module):\n    def __init__(self):\n        super(ReasoningEngine, self).__init__()\n\n        # Decision-making network\n        self.reasoning_net = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 2)  # Output: [linear_speed, angular_speed]\n        )\n\n    def forward(self, x):\n        return torch.tanh(self.reasoning_net(x))  # tanh to bound outputs\n\nclass ActionSelector:\n    def __init__(self):\n        self.max_linear = 0.5\n        self.max_angular = 1.0\n\n    def select_action(self, decision_output):\n        # Decision output shape: [batch_size, 2] where [linear, angular]\n        if len(decision_output.shape) > 1:\n            decision = decision_output[0]  # Take first item if batched\n        else:\n            decision = decision_output\n\n        linear = float(decision[0]) * self.max_linear\n        angular = float(decision[1]) * self.max_angular\n\n        return {'linear': linear, 'angular': angular}\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = DeepCognitiveArchitecture()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"ros2-launch-configuration",children:"ROS2 Launch Configuration"}),"\n",(0,s.jsx)(n.p,{children:"Create a launch file to bring up the cognitive architecture:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:"\x3c!-- cognitive_architecture.launch.py --\x3e\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.substitutions import LaunchConfiguration\nfrom launch.actions import DeclareLaunchArgument\n\ndef generate_launch_description():\n    use_sim_time = LaunchConfiguration('use_sim_time', default='false')\n\n    return LaunchDescription([\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='false',\n            description='Use simulation (Gazebo) clock if true'),\n\n        Node(\n            package='cognitive_architecture',\n            executable='cognitive_component',\n            name='cognitive_component',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'),\n\n        Node(\n            package='cognitive_architecture',\n            executable='deep_cognitive_architecture',\n            name='deep_cognitive_architecture',\n            parameters=[{'use_sim_time': use_sim_time}],\n            output='screen'),\n    ])\n"})}),"\n",(0,s.jsx)(n.h2,{id:"modular-cognitive-components",children:"Modular Cognitive Components"}),"\n",(0,s.jsx)(n.h3,{id:"creating-reusable-cognitive-modules",children:"Creating Reusable Cognitive Modules"}),"\n",(0,s.jsx)(n.p,{children:"One of the key advantages of cognitive architectures is their modularity. Let's create specific cognitive modules for different robot tasks:"}),"\n",(0,s.jsx)(n.h4,{id:"navigation-cognitive-module",children:"Navigation Cognitive Module"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom nav_msgs.msg import Odometry\nimport numpy as np\n\nclass NavigationCognitiveModule(Node):\n    def __init__(self):\n        super().__init__('navigation_cognitive_module')\n\n        # Publishers and subscribers\n        self.scan_sub = self.create_subscription(LaserScan, 'scan', self.scan_callback, 10)\n        self.odom_sub = self.create_subscription(Odometry, 'odom', self.odom_callback, 10)\n        self.cmd_pub = self.create_publisher(Twist, 'cmd_vel', 10)\n\n        # State variables\n        self.current_pose = None\n        self.obstacles = []\n        self.goal_reached = False\n\n        # Navigation parameters\n        self.safe_distance = 0.5\n        self.target_distance = 1.0\n\n        self.get_logger().info('Navigation Cognitive Module initialized')\n\n    def scan_callback(self, msg):\n        # Process laser scan data\n        ranges = np.array(msg.ranges)\n        ranges = ranges[np.isfinite(ranges)]  # Remove infinite values\n\n        # Detect obstacles\n        self.obstacles = ranges[ranges < self.safe_distance]\n\n        # Make navigation decisions\n        self.make_navigation_decision()\n\n    def odom_callback(self, msg):\n        # Update current pose\n        self.current_pose = msg.pose.pose\n\n    def make_navigation_decision(self):\n        if len(self.obstacles) > 0:\n            # Obstacle detected - avoid\n            min_dist_idx = np.argmin(self.obstacles)\n            cmd_vel = Twist()\n\n            # Turn away from closest obstacle\n            if min_dist_idx < len(self.obstacles) // 2:\n                cmd_vel.angular.z = 0.5  # Turn right\n            else:\n                cmd_vel.angular.z = -0.5  # Turn left\n\n            cmd_vel.linear.x = 0.0  # Stop moving forward\n        else:\n            # No obstacles - move forward\n            cmd_vel = Twist()\n            cmd_vel.linear.x = 0.3\n            cmd_vel.angular.z = 0.0\n\n        self.cmd_pub.publish(cmd_vel)\n"})}),"\n",(0,s.jsx)(n.h4,{id:"manipulation-cognitive-module",children:"Manipulation Cognitive Module"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState\nfrom geometry_msgs.msg import PointStamped\nfrom std_msgs.msg import String\nimport numpy as np\n\nclass ManipulationCognitiveModule(Node):\n    def __init__(self):\n        super().__init__('manipulation_cognitive_module')\n\n        # Publishers and subscribers\n        self.joint_state_sub = self.create_subscription(\n            JointState, 'joint_states', self.joint_state_callback, 10)\n        self.target_sub = self.create_subscription(\n            PointStamped, 'target_point', self.target_callback, 10)\n        self.status_pub = self.create_publisher(String, 'manipulation_status', 10)\n\n        # Joint state storage\n        self.joint_positions = {}\n        self.target_point = None\n\n        self.get_logger().info('Manipulation Cognitive Module initialized')\n\n    def joint_state_callback(self, msg):\n        # Update joint positions\n        for i, name in enumerate(msg.name):\n            self.joint_positions[name] = msg.position[i]\n\n        # Process manipulation tasks if target is available\n        if self.target_point is not None:\n            self.execute_manipulation()\n\n    def target_callback(self, msg):\n        # Update target point\n        self.target_point = msg.point\n        self.get_logger().info(f'Target received: ({self.target_point.x}, {self.target_point.y}, {self.target_point.z})')\n\n    def execute_manipulation(self):\n        # Simple inverse kinematics approach\n        # In a real system, this would use sophisticated IK solvers\n        if self.target_point:\n            status_msg = String()\n            status_msg.data = f'Manipulation task in progress - targeting: ({self.target_point.x:.2f}, {self.target_point.y:.2f}, {self.target_point.z:.2f})'\n            self.status_pub.publish(status_msg)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"integrating-cognitive-components-with-isaac-tools",children:"Integrating Cognitive Components with Isaac Tools"}),"\n",(0,s.jsx)(n.h3,{id:"using-isaac-ros-for-cognitive-processing",children:"Using Isaac ROS for Cognitive Processing"}),"\n",(0,s.jsx)(n.p,{children:"The NVIDIA Isaac ROS packages provide specialized nodes for cognitive processing. Here's how to integrate them:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# cognitive_pipeline_config.yaml\ncognitive_pipeline:\n  ros__parameters:\n    # Perception parameters\n    perception_rate: 10.0\n    detection_threshold: 0.5\n\n    # Cognitive parameters\n    memory_size: 100\n    reasoning_frequency: 5.0\n\n    # Action parameters\n    max_linear_velocity: 0.5\n    max_angular_velocity: 1.0\n"})}),"\n",(0,s.jsx)(n.h3,{id:"cognitive-architecture-validation",children:"Cognitive Architecture Validation"}),"\n",(0,s.jsx)(n.p,{children:"To validate your cognitive architecture, create a test script that verifies the integration of all components:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom diagnostic_msgs.msg import DiagnosticArray\nimport time\n\nclass CognitiveArchitectureValidator(Node):\n    def __init__(self):\n        super().__init__('cognitive_architecture_validator')\n\n        # Subscribers for monitoring cognitive components\n        self.status_subs = [\n            self.create_subscription(String, 'cognitive_status', self.status_callback, 10),\n            self.create_subscription(String, 'navigation_status', self.nav_status_callback, 10),\n            self.create_subscription(String, 'manipulation_status', self.manip_status_callback, 10)\n        ]\n\n        # Publisher for validation results\n        self.diag_pub = self.create_publisher(DiagnosticArray, 'cognitive_diagnostics', 10)\n\n        # Status tracking\n        self.component_statuses = {}\n\n        # Timer for periodic validation\n        self.timer = self.create_timer(1.0, self.validate_system)\n\n        self.get_logger().info('Cognitive Architecture Validator initialized')\n\n    def status_callback(self, msg):\n        self.component_statuses['cognitive'] = {'status': msg.data, 'timestamp': time.time()}\n\n    def nav_status_callback(self, msg):\n        self.component_statuses['navigation'] = {'status': msg.data, 'timestamp': time.time()}\n\n    def manip_status_callback(self, msg):\n        self.component_statuses['manipulation'] = {'status': msg.data, 'timestamp': time.time()}\n\n    def validate_system(self):\n        # Check if all components are active\n        diag_array = DiagnosticArray()\n        diag_array.header.stamp = self.get_clock().now().to_msg()\n\n        for component, info in self.component_statuses.items():\n            # Check if component is responding (within 5 seconds)\n            if time.time() - info['timestamp'] < 5.0:\n                status = 'OK'\n            else:\n                status = 'TIMEOUT'\n\n            # Create diagnostic status\n            diag_status = DiagnosticStatus()\n            diag_status.name = f'Cognitive_{component}_Status'\n            diag_status.message = f'{status}: {info[\"status\"]}'\n            diag_status.level = 0 if status == 'OK' else 2  # 0=OK, 2=ERROR\n\n            diag_array.status.append(diag_status)\n\n        self.diag_pub.publish(diag_array)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    validator = CognitiveArchitectureValidator()\n\n    try:\n        rclpy.spin(validator)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        validator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices-for-cognitive-architecture-design",children:"Best Practices for Cognitive Architecture Design"}),"\n",(0,s.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPU Utilization"}),": Maximize GPU utilization by batching operations when possible"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory Management"}),": Efficiently manage GPU memory to prevent out-of-memory errors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Threading"}),": Use appropriate threading models to prevent blocking operations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Communication"}),": Optimize ROS2 communication patterns for low-latency"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fail-Safe Mechanisms"}),": Implement graceful degradation when components fail"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Emergency Responses"}),": Ensure emergency stop capabilities bypass cognitive reasoning"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Validation"}),": Continuously validate cognitive decisions before execution"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monitoring"}),": Monitor cognitive system health and performance"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"modularity-and-scalability",children:"Modularity and Scalability"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Component Interfaces"}),": Define clear interfaces between cognitive components"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Configuration Management"}),": Use parameter servers for flexible configuration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Testing Frameworks"}),": Develop comprehensive testing for individual components"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Logging"}),": Implement detailed logging for debugging and analysis"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"In this lesson, we explored cognitive architectures for humanoid robot intelligence, covering:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The fundamental components of cognitive architectures and their roles"}),"\n",(0,s.jsx)(n.li,{children:"Design principles for creating effective cognitive systems"}),"\n",(0,s.jsx)(n.li,{children:"Implementation of cognitive components using NVIDIA Isaac tools"}),"\n",(0,s.jsx)(n.li,{children:"Integration of GPU-accelerated AI processing in cognitive architectures"}),"\n",(0,s.jsx)(n.li,{children:"Modular cognitive components for different robot tasks"}),"\n",(0,s.jsx)(n.li,{children:"Validation and testing approaches for cognitive systems"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Cognitive architectures form the backbone of intelligent robot behavior, enabling humanoid robots to process information, make decisions, and execute complex tasks. The NVIDIA Isaac ecosystem provides powerful tools and frameworks that leverage hardware acceleration to create real-time cognitive systems capable of supporting sophisticated autonomous behaviors."}),"\n",(0,s.jsx)(n.p,{children:"The modular approach to cognitive architecture design allows for scalable and maintainable robot intelligence systems that can be extended and adapted for different tasks and environments. As you progress through this module, you'll build upon these foundations to create more sophisticated perception-processing-action pipelines and AI decision-making systems."})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(g,{...e})}):g(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const s={},o=t.createContext(s);function r(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);