"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[8034],{3141:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>_,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module-3/Visual-SLAM-&-Navigation/lesson-2.3-ai-enhanced-navigation-and-obstacle-avoidance","title":"Lesson 2.3 - AI Enhanced Navigation and Obstacle Avoidance","description":"Learn to combine AI reasoning with navigation for intelligent path planning and obstacle avoidance in humanoid robots","source":"@site/docs/module-3/02-Visual-SLAM-&-Navigation/lesson-2.3-ai-enhanced-navigation-and-obstacle-avoidance.md","sourceDirName":"module-3/02-Visual-SLAM-&-Navigation","slug":"/module-3/Visual-SLAM-&-Navigation/lesson-2.3-ai-enhanced-navigation-and-obstacle-avoidance","permalink":"/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-3/Visual-SLAM-&-Navigation/lesson-2.3-ai-enhanced-navigation-and-obstacle-avoidance","draft":false,"unlisted":false,"editUrl":"https://github.com/AmanNazim/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/edit/main/physical-ai-humanoid-robotics-book/docs/module-3/02-Visual-SLAM-&-Navigation/lesson-2.3-ai-enhanced-navigation-and-obstacle-avoidance.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Lesson 2.3 - AI Enhanced Navigation and Obstacle Avoidance","sidebar_position":3,"description":"Learn to combine AI reasoning with navigation for intelligent path planning and obstacle avoidance in humanoid robots"},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 2.2 - Visual SLAM with Isaac ROS","permalink":"/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-3/Visual-SLAM-&-Navigation/lesson-2.2-visual-slam-with-isaac-ros"},"next":{"title":"Chapter 3: Cognitive Architectures","permalink":"/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-3/Cognitive-Architectures/"}}');var t=a(4848),o=a(8453);const s={title:"Lesson 2.3 - AI Enhanced Navigation and Obstacle Avoidance",sidebar_position:3,description:"Learn to combine AI reasoning with navigation for intelligent path planning and obstacle avoidance in humanoid robots"},r="Lesson 2.3 \u2013 AI-Enhanced Navigation and Obstacle Avoidance",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Understanding AI-Enhanced Navigation Systems",id:"understanding-ai-enhanced-navigation-systems",level:2},{value:"Traditional vs. AI-Enhanced Navigation",id:"traditional-vs-ai-enhanced-navigation",level:3},{value:"Components of AI-Enhanced Navigation",id:"components-of-ai-enhanced-navigation",level:3},{value:"Setting Up AI-Enhanced Navigation Framework",id:"setting-up-ai-enhanced-navigation-framework",level:2},{value:"Installing Required Dependencies",id:"installing-required-dependencies",level:3},{value:"Creating the AI Navigation Configuration",id:"creating-the-ai-navigation-configuration",level:3},{value:"Implementing AI-Enhanced Behavior Trees",id:"implementing-ai-enhanced-behavior-trees",level:2},{value:"Creating AI-Enhanced Behavior Tree",id:"creating-ai-enhanced-behavior-tree",level:3},{value:"Advanced Behavior Tree for Dynamic Obstacle Handling",id:"advanced-behavior-tree-for-dynamic-obstacle-handling",level:3},{value:"Implementing AI-Enhanced Obstacle Avoidance Algorithms",id:"implementing-ai-enhanced-obstacle-avoidance-algorithms",level:2},{value:"Creating the AI Obstacle Avoidance Node",id:"creating-the-ai-obstacle-avoidance-node",level:3},{value:"Creating the AI Perception Integration Node",id:"creating-the-ai-perception-integration-node",level:3},{value:"Configuring the AI-Enhanced Navigation Launch File",id:"configuring-the-ai-enhanced-navigation-launch-file",level:2},{value:"Testing AI-Enhanced Navigation in Isaac Sim",id:"testing-ai-enhanced-navigation-in-isaac-sim",level:2},{value:"Setting up the Isaac Sim Environment",id:"setting-up-the-isaac-sim-environment",level:3},{value:"Integrating Isaac ROS Hardware Acceleration",id:"integrating-isaac-ros-hardware-acceleration",level:2},{value:"Using Isaac ROS for AI-Enhanced Perception",id:"using-isaac-ros-for-ai-enhanced-perception",level:3},{value:"Creating an Isaac ROS Integration Launch File",id:"creating-an-isaac-ros-integration-launch-file",level:3},{value:"Practical Implementation Steps",id:"practical-implementation-steps",level:2},{value:"Step 1: Build and Compile the AI Nodes",id:"step-1-build-and-compile-the-ai-nodes",level:3},{value:"Step 2: Launch the Complete AI-Enhanced Navigation System",id:"step-2-launch-the-complete-ai-enhanced-navigation-system",level:3},{value:"Step 3: Test Navigation with Dynamic Obstacles",id:"step-3-test-navigation-with-dynamic-obstacles",level:3},{value:"Step 4: Monitor AI Performance",id:"step-4-monitor-ai-performance",level:3},{value:"Advanced AI Reasoning Techniques",id:"advanced-ai-reasoning-techniques",level:2},{value:"Predictive Obstacle Tracking",id:"predictive-obstacle-tracking",level:3},{value:"Multi-Objective Path Optimization",id:"multi-objective-path-optimization",level:3},{value:"Performance Validation and Tuning",id:"performance-validation-and-tuning",level:2},{value:"Testing Different Scenarios",id:"testing-different-scenarios",level:3},{value:"Performance Metrics",id:"performance-metrics",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"lesson-23--ai-enhanced-navigation-and-obstacle-avoidance",children:"Lesson 2.3 \u2013 AI-Enhanced Navigation and Obstacle Avoidance"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Integrate AI reasoning with navigation systems for intelligent path planning"}),"\n",(0,t.jsx)(n.li,{children:"Implement advanced obstacle avoidance algorithms for humanoid robots"}),"\n",(0,t.jsx)(n.li,{children:"Design adaptive behavior capabilities that integrate perception and navigation"}),"\n",(0,t.jsx)(n.li,{children:"Create AI-enhanced navigation systems that respond intelligently to dynamic environments"}),"\n",(0,t.jsx)(n.li,{children:"Validate and test AI-enhanced navigation performance in simulation"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:"In this lesson, we'll explore the integration of artificial intelligence with navigation systems to create intelligent path planning and obstacle avoidance capabilities for humanoid robots. Building upon the Nav2 path planning system from Lesson 2.1 and the Visual SLAM implementation from Lesson 2.2, we'll now enhance our navigation system with AI reasoning capabilities that enable adaptive and intelligent behavior."}),"\n",(0,t.jsx)(n.p,{children:"AI-enhanced navigation goes beyond traditional path planning by incorporating perception data, learning from environmental patterns, and making intelligent decisions about navigation strategies. This is particularly important for humanoid robots that must navigate complex environments while maintaining balance and adapting to dynamic obstacles."}),"\n",(0,t.jsx)(n.h2,{id:"understanding-ai-enhanced-navigation-systems",children:"Understanding AI-Enhanced Navigation Systems"}),"\n",(0,t.jsx)(n.h3,{id:"traditional-vs-ai-enhanced-navigation",children:"Traditional vs. AI-Enhanced Navigation"}),"\n",(0,t.jsx)(n.p,{children:"Traditional navigation systems rely on predefined algorithms and fixed parameters to plan paths and avoid obstacles. While effective in controlled environments, they often struggle with:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Dynamic obstacles that move unpredictably"}),"\n",(0,t.jsx)(n.li,{children:"Complex environments with multiple variables"}),"\n",(0,t.jsx)(n.li,{children:"Situations requiring adaptive behavior"}),"\n",(0,t.jsx)(n.li,{children:"Long-term strategic planning beyond immediate pathfinding"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"AI-enhanced navigation addresses these limitations by incorporating:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Learning capabilities"}),": Systems that adapt to new environments and situations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Predictive modeling"}),": Anticipating obstacle movements and environmental changes"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-objective optimization"}),": Balancing multiple navigation goals simultaneously"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Context awareness"}),": Understanding environmental context and making informed decisions"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"components-of-ai-enhanced-navigation",children:"Components of AI-Enhanced Navigation"}),"\n",(0,t.jsx)(n.p,{children:"An AI-enhanced navigation system typically includes:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perception Integration"}),": Combining data from multiple sensors and SLAM systems"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"AI Reasoning Engine"}),": Processing perception data to make navigation decisions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Adaptive Path Planning"}),": Dynamically adjusting routes based on real-time conditions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Obstacle Classification"}),": Understanding different types of obstacles and their behaviors"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Behavior Prediction"}),": Predicting future positions of dynamic obstacles"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Risk Assessment"}),": Evaluating navigation safety and feasibility"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"setting-up-ai-enhanced-navigation-framework",children:"Setting Up AI-Enhanced Navigation Framework"}),"\n",(0,t.jsx)(n.h3,{id:"installing-required-dependencies",children:"Installing Required Dependencies"}),"\n",(0,t.jsx)(n.p,{children:"First, ensure you have the necessary AI and navigation packages installed:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Install AI navigation packages\nsudo apt update\nsudo apt install ros-humble-nav2-behaviors ros-humble-nav2-dwb-controller ros-humble-nav2-gradient-path-planner\nsudo apt install ros-humble-isaac-ros-peoplesegnet ros-humble-isaac-ros-dnn-image-encoder\n"})}),"\n",(0,t.jsx)(n.h3,{id:"creating-the-ai-navigation-configuration",children:"Creating the AI Navigation Configuration"}),"\n",(0,t.jsx)(n.p,{children:"Create a new configuration file for AI-enhanced navigation in your workspace:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Navigate to your robot's navigation configuration directory\ncd ~/humanoid_robot_ws/src/humanoid_navigation/config\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Create ",(0,t.jsx)(n.code,{children:"ai_nav_params.yaml"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'# AI-Enhanced Navigation Parameters\namcl:\n  ros__parameters:\n    use_sim_time: True\n    alpha1: 0.2\n    alpha2: 0.2\n    alpha3: 0.2\n    alpha4: 0.2\n    alpha5: 0.2\n    base_frame_id: "base_link"\n    beam_skip_distance: 0.5\n    beam_skip_error_threshold: 0.9\n    beam_skip_threshold: 0.3\n    do_beamskip: false\n    global_frame_id: "map"\n    lambda_short: 0.1\n    likelihood_max_dist: 2.0\n    max_beams: 60\n    max_particles: 2000\n    min_particles: 500\n    odom_frame_id: "odom"\n    pf_err: 0.05\n    pf_z: 0.99\n    recovery_alpha_fast: 0.0\n    recovery_alpha_slow: 0.0\n    resample_interval: 1\n    robot_model_type: "nav2_amcl::DifferentialMotionModel"\n    save_pose_enabled: True\n    save_pose_file: ""\n    set_initial_pose: False\n    sigma_hit: 0.2\n    tf_broadcast: True\n    transform_tolerance: 1.0\n    update_min_a: 0.2\n    update_min_d: 0.25\n    z_hit: 0.5\n    z_max: 0.05\n    z_rand: 0.5\n    z_short: 0.05\n\nbt_navigator:\n  ros__parameters:\n    use_sim_time: True\n    global_frame: "map"\n    robot_base_frame: "base_link"\n    odom_topic: "/odom"\n    bt_loop_duration: 10\n    default_server_timeout: 20\n    enable_groot_monitoring: True\n    groot_zmq_publisher_port: 1666\n    groot_zmq_server_port: 1667\n    # AI-enhanced behavior tree\n    default_nav_through_poses_bt_xml: "ai_nav_through_poses.xml"\n    default_nav_to_pose_bt_xml: "ai_nav_to_pose.xml"\n\ncontroller_server:\n  ros__parameters:\n    use_sim_time: True\n    controller_frequency: 20.0\n    min_x_velocity_threshold: 0.001\n    min_y_velocity_threshold: 0.5\n    min_theta_velocity_threshold: 0.001\n    # AI-Enhanced Controller\n    progress_checker_plugin: "progress_checker"\n    goal_checker_plugin: "goal_checker"\n    controller_plugins: ["FollowPath"]\n\n    # DWB Controller with AI Integration\n    FollowPath:\n      plugin: "nav2_mppi_controller::MPPIController"\n      time_steps: 50\n      control_horizon: 2.0\n      dt: 0.05\n      vx_std: 0.2\n      vy_std: 0.05\n      wz_std: 0.3\n      vtheta_max: 1.0\n      vtheta_min: -1.0\n      vx_samples: 20\n      vy_samples: 5\n      wz_samples: 20\n      lambda: 0.05\n      mu: 0.05\n      collision_cost: 1.0\n      cost_scaling_factor: 10.0\n      inflation_cost_scaling_factor: 3.0\n      replan_on_exception: true\n      trajectory_visualization_plugin: "nav2_trajectory_utils::MPPIVisualization"\n      critic_names: [\n        "ConstraintCritic",\n        "GoalCritic",\n        "GoalAngleCritic",\n        "PathAlignCritic",\n        "PathFollowCritic",\n        "PathDistanceCritic",\n        "GoalDistanceCritic",\n        "OscillationCritic",\n        "PreferForwardCritic",\n        "ObstacleFootprintCritic",\n        "DynamicObstacleCritic"  # AI-enhanced dynamic obstacle handling\n      ]\n\n    ConstraintCritic:\n      enabled: true\n      penalty: 1.0\n    GoalCritic:\n      enabled: true\n      penalty: 2.0\n      threshold_to_consider: 0.25\n    GoalAngleCritic:\n      enabled: true\n      penalty: 3.0\n      threshold_to_consider: 0.25\n    PathAlignCritic:\n      enabled: true\n      penalty: 3.0\n      threshold_to_consider: 0.25\n      path_step_size: 0.5\n      curv_step_size: 0.5\n      forward_penalty_mult: 0.5\n    PathFollowCritic:\n      enabled: true\n      penalty: 3.0\n      threshold_to_consider: 0.5\n    PathDistanceCritic:\n      enabled: true\n      penalty: 2.0\n      threshold_to_consider: 0.5\n    GoalDistanceCritic:\n      enabled: true\n      penalty: 2.0\n      scaling_param: 1.0\n      threshold_to_consider: 0.5\n    OscillationCritic:\n      enabled: true\n      penalty: 2.0\n      oscillation_reset_time: 0.3\n      oscillation_threshold: 0.1\n    PreferForwardCritic:\n      enabled: true\n      penalty: 0.2\n      threshold_to_consider: 0.5\n    ObstacleFootprintCritic:\n      enabled: true\n      penalty: 1.5\n      threshold_to_consider: 0.5\n      inflation_cost_scaling_factor: 3.0\n    DynamicObstacleCritic:\n      enabled: true\n      penalty: 5.0\n      threshold_to_consider: 0.5\n      velocity_scale: 0.5\n      min_distance_threshold: 0.5\n      max_expansion_from_footprint: 0.5\n\nlocal_costmap:\n  local_costmap:\n    ros__parameters:\n      use_sim_time: True\n      global_frame: "odom"\n      robot_base_frame: "base_link"\n      update_frequency: 5.0\n      publish_frequency: 2.0\n      resolution: 0.05\n      robot_radius: 0.3\n      static_map: false\n      rolling_window: true\n      width: 6\n      height: 6\n      transform_tolerance: 0.5\n      plugins: ["obstacle_layer", "voxel_layer", "inflation_layer"]\n      inflation_layer:\n        plugin: "nav2_costmap_2d::InflationLayer"\n        cost_scaling_factor: 3.0\n        inflation_radius: 0.55\n      obstacle_layer:\n        plugin: "nav2_costmap_2d::ObstacleLayer"\n        enabled: True\n        observation_sources: scan\n        scan:\n          topic: "/scan"\n          max_obstacle_height: 2.0\n          clearing: True\n          marking: True\n          data_type: "LaserScan"\n          raytrace_max_range: 3.0\n          raytrace_min_range: 0.0\n          obstacle_max_range: 2.5\n          obstacle_min_range: 0.0\n      voxel_layer:\n        plugin: "nav2_costmap_2d::VoxelLayer"\n        enabled: True\n        publish_voxel_map: True\n        origin_z: 0.0\n        z_resolution: 0.2\n        z_voxels: 10\n        max_obstacle_height: 2.0\n        mark_threshold: 0\n        observation_sources: pointcloud\n        pointcloud:\n          topic: "/intel_realsense_r200_depth/points"\n          max_obstacle_height: 2.0\n          clearing: True\n          marking: True\n          data_type: "PointCloud2"\n          raytrace_max_range: 3.0\n          raytrace_min_range: 0.0\n          obstacle_max_range: 2.5\n          obstacle_min_range: 0.0\n      always_send_full_costmap: True\n\nglobal_costmap:\n  global_costmap:\n    ros__parameters:\n      use_sim_time: True\n      global_frame: "map"\n      robot_base_frame: "base_link"\n      update_frequency: 1.0\n      publish_frequency: 0.5\n      resolution: 0.05\n      robot_radius: 0.3\n      static_map: true\n      only_publish_static_layers: true\n      track_unknown_space: true\n      plugins: ["static_layer", "obstacle_layer", "voxel_layer", "inflation_layer"]\n      obstacle_layer:\n        plugin: "nav2_costmap_2d::ObstacleLayer"\n        enabled: True\n        observation_sources: scan\n        scan:\n          topic: "/scan"\n          max_obstacle_height: 2.0\n          clearing: True\n          marking: True\n          data_type: "LaserScan"\n          raytrace_max_range: 3.0\n          raytrace_min_range: 0.0\n          obstacle_max_range: 2.5\n          obstacle_min_range: 0.0\n      voxel_layer:\n        plugin: "nav2_costmap_2d::VoxelLayer"\n        enabled: True\n        publish_voxel_map: True\n        origin_z: 0.0\n        z_resolution: 0.2\n        z_voxels: 10\n        max_obstacle_height: 2.0\n        mark_threshold: 0\n        observation_sources: pointcloud\n        pointcloud:\n          topic: "/intel_realsense_r200_depth/points"\n          max_obstacle_height: 2.0\n          clearing: True\n          marking: True\n          data_type: "PointCloud2"\n          raytrace_max_range: 3.0\n          raytrace_min_range: 0.0\n          obstacle_max_range: 2.5\n          obstacle_min_range: 0.0\n      static_layer:\n        plugin: "nav2_costmap_2d::StaticLayer"\n        map_subscribe_transient_local: True\n      inflation_layer:\n        plugin: "nav2_costmap_2d::InflationLayer"\n        cost_scaling_factor: 3.0\n        inflation_radius: 0.55\n      always_send_full_costmap: True\n\nplanner_server:\n  ros__parameters:\n    use_sim_time: True\n    planner_plugins: ["GridBased"]\n    GridBased:\n      plugin: "nav2_navfn_planner::NavfnPlanner"\n      tolerance: 0.5\n      use_astar: false\n      allow_unknown: true\n      planner_thread_number: 2\n\nsmoother_server:\n  ros__parameters:\n    use_sim_time: True\n    smoother_plugins: ["simple_smoother"]\n    simple_smoother:\n      plugin: "nav2_smoother::SimpleSmoother"\n      tolerance: 1.0e-10\n      max_its: 1000\n      w_smooth: 0.9\n      w_data: 0.1\n      do_refinement: True\n\nbehavior_server:\n  ros__parameters:\n    use_sim_time: True\n    local_frame: "odom"\n    global_frame: "map"\n    robot_base_frame: "base_link"\n    transform_tolerance: 0.1\n    # AI-Enhanced Recovery Behaviors\n    recovery_plugins: ["spin", "backup", "wait", "clear_costmap", "assisted_teleop"]\n    spin:\n      plugin: "nav2_recoveries::Spin"\n      sim_frequency: 10\n      cycle_frequency: 10\n      spin_dist: 1.57\n      time_allowance: 10\n    backup:\n      plugin: "nav2_recoveries::BackUp"\n      sim_frequency: 10\n      cycle_frequency: 10\n      safety_factor: 1.0\n      backup_vel: -0.1\n      backup_dist: 0.15\n      time_allowance: 10\n    wait:\n      plugin: "nav2_recoveries::Wait"\n      sim_frequency: 10\n      cycle_frequency: 5\n      time_allowance: 10\n    clear_costmap:\n      plugin: "nav2_recoveries::ClearCostmap"\n      sim_frequency: 10\n      cycle_frequency: 10\n      reason: "default"\n      restore_defaults: true\n      service_name_transform: "clear_entirely"\n      track_unknown_space: true\n      marking: true\n      clearing: true\n      service_name_marking: "clear_entirely_marking"\n      service_name_clearing: "clear_entirely_global"\n    assisted_teleop:\n      plugin: "nav2_recoveries::AssistedTeleop"\n      sim_frequency: 10\n      cycle_frequency: 10\n      linear_vel_max: 0.5\n      linear_vel_min: 0.1\n      angular_vel_max: 0.3\n      angular_vel_min: 0.1\n      translation_weight: 1.0\n      rotation_weight: 1.0\n      min_obstacle_dist: 0.1\n      use_unknown_as_free: true\n      direction: "both"\n\nwaypoint_follower:\n  ros__parameters:\n    use_sim_time: True\n    loop_rate: 20\n    stop_on_failure: false\n    waypoint_task_executor_plugin: "wait_at_waypoint"\n    wait_at_waypoint:\n      plugin: "nav2_waypoint_follower::WaitAtWaypoint"\n      enabled: true\n      waypoint_pause_duration: 200\n'})}),"\n",(0,t.jsx)(n.h2,{id:"implementing-ai-enhanced-behavior-trees",children:"Implementing AI-Enhanced Behavior Trees"}),"\n",(0,t.jsx)(n.h3,{id:"creating-ai-enhanced-behavior-tree",children:"Creating AI-Enhanced Behavior Tree"}),"\n",(0,t.jsx)(n.p,{children:"Create a custom behavior tree that incorporates AI reasoning for navigation decisions:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-xml",children:'\x3c!-- ai_nav_to_pose.xml --\x3e\n<root main_tree_to_execute="MainTree">\n    <BehaviorTree ID="MainTree">\n        <Sequence name="NavigateToPose">\n            <GoalUpdated/>\n            <ComputePathToPose goal="{goal}" path="{path}" planner_id="GridBased"/>\n            <ReactiveSequence name="FollowPathReady">\n                <IsPathValid path="{path}"/>\n                <FollowPath path="{path}" controller_id="FollowPath" speed="1.0"/>\n            </ReactiveSequence>\n            <ReactiveFallback name="GoalReachingBehavior">\n                <GoalReached goal="{goal}" tolerance="0.25"/>\n                <RecoveryNode name="RecoveryFallback" recovery_behavior_id="backup_once">\n                    <RoundRobin name="RoundRobin">\n                        <AssistedTeleop max_retries="1" timeout="5"/>\n                        <ClearEntireCostmap service_name="global_costmap/clear_entirely_global_costmap" service_timeout="2000"/>\n                        <ClearEntireCostmap service_name="local_costmap/clear_entirely_local_costmap" service_timeout="2000"/>\n                    </RoundRobin>\n                </RecoveryNode>\n                <PipelineSequence name="PlanningAndControl">\n                    <ComputePathToPose goal="{goal}" path="{path}" planner_id="GridBased"/>\n                    <ReactiveSequence name="ControlSequence">\n                        <IsPathValid path="{path}"/>\n                        <FollowPath path="{path}" controller_id="FollowPath" speed="1.0"/>\n                    </ReactiveSequence>\n                </PipelineSequence>\n            </ReactiveFallback>\n        </Sequence>\n    </BehaviorTree>\n</root>\n'})}),"\n",(0,t.jsx)(n.h3,{id:"advanced-behavior-tree-for-dynamic-obstacle-handling",children:"Advanced Behavior Tree for Dynamic Obstacle Handling"}),"\n",(0,t.jsx)(n.p,{children:"Create a more sophisticated behavior tree that handles dynamic obstacles with AI reasoning:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-xml",children:'\x3c!-- ai_nav_through_poses.xml --\x3e\n<root main_tree_to_execute="MainTree">\n    <BehaviorTree ID="MainTree">\n        <ReactiveSequence name="NavigateThroughPoses">\n            <GoalUpdated/>\n            <PipelineSequence name="ComputeAndExecute">\n                <ComputePathThroughPoses goals="{goals}" path="{path}" planner_id="GridBased"/>\n                <ReactiveSequence name="ExecutePath">\n                    <IsPathValid path="{path}"/>\n                    <FollowPath path="{path}" controller_id="FollowPath" speed="1.0"/>\n                </ReactiveSequence>\n            </PipelineSequence>\n\n            \x3c!-- AI-Enhanced Dynamic Obstacle Detection --\x3e\n            <ReactiveFallback name="ObstacleAvoidanceSequence">\n                <CheckObstaclesAhead distance="1.0" min_points="5" layer_name="obstacle_layer"/>\n\n                \x3c!-- If obstacles detected, use AI reasoning to decide best course of action --\x3e\n                <Sequence name="AI_Dynamic_Obstacle_Handler">\n                    <ClassifyObstacleType threshold="0.5" classification="dynamic"/>\n\n                    \x3c!-- If dynamic obstacle, predict movement and adjust path --\x3e\n                    <ConditionalSequence condition="is_dynamic_obstacle">\n                        <PredictObstacleTrajectory prediction_time="2.0" confidence_threshold="0.8"/>\n\n                        \x3c!-- Decide best action based on prediction --\x3e\n                        <ReactiveFallback name="DynamicActionSelection">\n                            <IsSafeToWait wait_time="3.0" safety_margin="0.5"/>\n                            <Wait wait_duration="3.0"/>\n\n                            <Sequence name="PathRecalculation">\n                                <ClearEntireCostmap service_name="local_costmap/clear_entirely_local_costmap" service_timeout="2000"/>\n                                <ComputePathToPose goal="{current_goal}" path="{new_path}" planner_id="GridBased"/>\n                                <FollowPath path="{new_path}" controller_id="FollowPath" speed="0.5"/>\n                            </Sequence>\n                        </ReactiveFallback>\n                    </ConditionalSequence>\n\n                    \x3c!-- If static obstacle, use traditional avoidance --\x3e\n                    <ConditionalSequence condition="is_static_obstacle">\n                        <RecoveryNode name="StaticObstacleRecovery" recovery_behavior_id="clear_costmap">\n                            <ClearEntireCostmap service_name="local_costmap/clear_entirely_local_costmap" service_timeout="2000"/>\n                            <ComputePathToPose goal="{current_goal}" path="{new_path}" planner_id="GridBased"/>\n                            <FollowPath path="{new_path}" controller_id="FollowPath" speed="1.0"/>\n                        </RecoveryNode>\n                    </ConditionalSequence>\n                </Sequence>\n            </ReactiveFallback>\n\n            \x3c!-- Final goal check --\x3e\n            <GoalReached goal="{current_goal}" tolerance="0.25"/>\n        </ReactiveSequence>\n    </BehaviorTree>\n</root>\n'})}),"\n",(0,t.jsx)(n.h2,{id:"implementing-ai-enhanced-obstacle-avoidance-algorithms",children:"Implementing AI-Enhanced Obstacle Avoidance Algorithms"}),"\n",(0,t.jsx)(n.h3,{id:"creating-the-ai-obstacle-avoidance-node",children:"Creating the AI Obstacle Avoidance Node"}),"\n",(0,t.jsx)(n.p,{children:"Now let's create a ROS2 node that implements AI-enhanced obstacle detection and avoidance:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-cpp",children:'// ai_obstacle_avoidance.cpp\n#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/laser_scan.hpp>\n#include <geometry_msgs/msg/twist.hpp>\n#include <nav_msgs/msg/odometry.hpp>\n#include <tf2_ros/transform_listener.h>\n#include <tf2_geometry_msgs/tf2_geometry_msgs.h>\n#include <std_msgs/msg/float32.hpp>\n#include <vector>\n#include <algorithm>\n#include <cmath>\n#include <memory>\n\nclass AIEnhancedObstacleAvoidance : public rclcpp::Node\n{\npublic:\n    AIEnhancedObstacleAvoidance() : Node("ai_obstacle_avoidance_node")\n    {\n        // Publishers and subscribers\n        cmd_vel_pub_ = this->create_publisher<geometry_msgs::msg::Twist>("cmd_vel", 10);\n        laser_sub_ = this->create_subscription<sensor_msgs::msg::LaserScan>(\n            "scan", 10, std::bind(&AIEnhancedObstacleAvoidance::laserCallback, this, std::placeholders::_1));\n\n        // AI parameters\n        this->declare_parameter("safe_distance", 0.8);\n        this->declare_parameter("ai_reaction_threshold", 0.6);\n        this->declare_parameter("max_linear_speed", 0.5);\n        this->declare_parameter("max_angular_speed", 1.0);\n        this->declare_parameter("prediction_horizon", 2.0);\n\n        safe_distance_ = this->get_parameter("safe_distance").as_double();\n        ai_reaction_threshold_ = this->get_parameter("ai_reaction_threshold").as_double();\n        max_linear_speed_ = this->get_parameter("max_linear_speed").as_double();\n        max_angular_speed_ = this->get_parameter("max_angular_speed").as_double();\n        prediction_horizon_ = this->get_parameter("prediction_horizon").as_double();\n\n        RCLCPP_INFO(this->get_logger(), "AI Enhanced Obstacle Avoidance Node Initialized");\n    }\n\nprivate:\n    void laserCallback(const sensor_msgs::msg::LaserScan::SharedPtr msg)\n    {\n        geometry_msgs::msg::Twist cmd_vel;\n\n        // Process laser scan data using AI reasoning\n        std::vector<double> ranges = msg->ranges;\n\n        // Calculate distances in different sectors (front, left, right)\n        auto front_distances = getSectorDistances(ranges, -30, 30);\n        auto left_distances = getSectorDistances(ranges, 60, 120);\n        auto right_distances = getSectorDistances(ranges, -120, -60);\n\n        // AI-based obstacle assessment\n        auto front_analysis = analyzeSector(front_distances, "front");\n        auto left_analysis = analyzeSector(left_distances, "left");\n        auto right_analysis = analyzeSector(right_distances, "right");\n\n        // Make intelligent navigation decision\n        cmd_vel = makeIntelligentDecision(front_analysis, left_analysis, right_analysis);\n\n        cmd_vel_pub_->publish(cmd_vel);\n    }\n\n    std::vector<double> getSectorDistances(const std::vector<float>& ranges, int start_angle, int end_angle)\n    {\n        std::vector<double> sector_ranges;\n        int angle_min = static_cast<int>((start_angle + 180) * ranges.size() / 360.0);\n        int angle_max = static_cast<int>((end_angle + 180) * ranges.size() / 360.0);\n\n        for (int i = angle_min; i <= angle_max && i < static_cast<int>(ranges.size()); ++i)\n        {\n            if (i >= 0 && !std::isnan(ranges[i]) && !std::isinf(ranges[i]))\n            {\n                sector_ranges.push_back(static_cast<double>(ranges[i]));\n            }\n        }\n        return sector_ranges;\n    }\n\n    struct SectorAnalysis {\n        double min_distance;\n        double avg_distance;\n        bool has_close_obstacle;\n        double obstacle_density;\n        bool is_dynamic;\n    };\n\n    SectorAnalysis analyzeSector(const std::vector<double>& distances, const std::string& sector_name)\n    {\n        SectorAnalysis analysis;\n\n        if (distances.empty()) {\n            analysis.min_distance = 10.0;\n            analysis.avg_distance = 10.0;\n            analysis.has_close_obstacle = false;\n            analysis.obstacle_density = 0.0;\n            analysis.is_dynamic = false;\n            return analysis;\n        }\n\n        analysis.min_distance = *std::min_element(distances.begin(), distances.end());\n        analysis.avg_distance = std::accumulate(distances.begin(), distances.end(), 0.0) / distances.size();\n        analysis.has_close_obstacle = analysis.min_distance < safe_distance_;\n\n        // Calculate obstacle density (how many readings are close to the minimum)\n        int dense_count = 0;\n        for (double dist : distances) {\n            if (dist < safe_distance_ * 1.5) {\n                dense_count++;\n            }\n        }\n        analysis.obstacle_density = static_cast<double>(dense_count) / distances.size();\n\n        // Simple dynamic obstacle detection based on variance\n        double variance = 0.0;\n        for (double dist : distances) {\n            variance += std::pow(dist - analysis.avg_distance, 2);\n        }\n        variance /= distances.size();\n        analysis.is_dynamic = variance > 0.1; // Threshold for dynamic detection\n\n        return analysis;\n    }\n\n    geometry_msgs::msg::Twist makeIntelligentDecision(\n        const SectorAnalysis& front,\n        const SectorAnalysis& left,\n        const SectorAnalysis& right)\n    {\n        geometry_msgs::msg::Twist cmd_vel;\n\n        // Primary obstacle avoidance logic with AI reasoning\n        if (front.has_close_obstacle) {\n            // Front obstacle detected - apply AI reasoning\n\n            // If front obstacle is dynamic, predict and react\n            if (front.is_dynamic) {\n                // Dynamic obstacle: slow down and prepare for maneuver\n                cmd_vel.linear.x = std::max(0.0, max_linear_speed_ * 0.3);\n\n                // Choose turn direction based on left/right availability\n                if (left.min_distance > right.min_distance && left.min_distance > safe_distance_) {\n                    cmd_vel.angular.z = max_angular_speed_ * 0.5; // Turn left gently\n                } else if (right.min_distance > safe_distance_) {\n                    cmd_vel.angular.z = -max_angular_speed_ * 0.5; // Turn right gently\n                } else {\n                    cmd_vel.angular.z = max_angular_speed_; // Sharp turn if necessary\n                }\n            } else {\n                // Static obstacle: more aggressive avoidance\n                cmd_vel.linear.x = 0.0; // Stop moving forward\n\n                // Choose best escape route\n                if (left.min_distance > right.min_distance && left.min_distance > safe_distance_) {\n                    cmd_vel.angular.z = max_angular_speed_; // Turn left\n                } else if (right.min_distance > safe_distance_) {\n                    cmd_vel.angular.z = -max_angular_speed_; // Turn right\n                } else {\n                    cmd_vel.angular.z = max_angular_speed_ * 0.8; // Emergency turn\n                }\n            }\n        } else {\n            // No immediate obstacles - AI can make strategic decisions\n\n            // If there are obstacles on both sides but front is clear, go straight but cautiously\n            if (left.has_close_obstacle && right.has_close_obstacle) {\n                cmd_vel.linear.x = max_linear_speed_ * 0.7; // Reduced speed\n                cmd_vel.angular.z = 0.0;\n            } else {\n                // Clear path - move at normal speed\n                cmd_vel.linear.x = max_linear_speed_;\n                cmd_vel.angular.z = 0.0;\n            }\n        }\n\n        // Apply safety limits\n        cmd_vel.linear.x = std::max(-max_linear_speed_, std::min(max_linear_speed_, cmd_vel.linear.x));\n        cmd_vel.angular.z = std::max(-max_angular_speed_, std::min(max_angular_speed_, cmd_vel.angular.z));\n\n        return cmd_vel;\n    }\n\n    rclcpp::Publisher<geometry_msgs::msg::Twist>::SharedPtr cmd_vel_pub_;\n    rclcpp::Subscription<sensor_msgs::msg::LaserScan>::SharedPtr laser_sub_;\n\n    double safe_distance_;\n    double ai_reaction_threshold_;\n    double max_linear_speed_;\n    double max_angular_speed_;\n    double prediction_horizon_;\n};\n\nint main(int argc, char * argv[])\n{\n    rclcpp::init(argc, argv);\n    rclcpp::spin(std::make_shared<AIEnhancedObstacleAvoidance>());\n    rclcpp::shutdown();\n    return 0;\n}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"creating-the-ai-perception-integration-node",children:"Creating the AI Perception Integration Node"}),"\n",(0,t.jsx)(n.p,{children:"Let's create a node that integrates Visual SLAM data with the AI navigation system:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-cpp",children:'// ai_perception_integration.cpp\n#include <rclcpp/rclcpp.hpp>\n#include <nav_msgs/msg/occupancy_grid.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <geometry_msgs/msg/pose_stamped.hpp>\n#include <visualization_msgs/msg/marker_array.hpp>\n#include <tf2_ros/buffer.h>\n#include <tf2_ros/transform_listener.h>\n#include <opencv2/opencv.hpp>\n#include <cv_bridge/cv_bridge.h>\n#include <vector>\n#include <memory>\n\nclass AIPerceptionIntegration : public rclcpp::Node\n{\npublic:\n    AIPerceptionIntegration() : Node("ai_perception_integration_node")\n    {\n        // Subscribers for SLAM and perception data\n        slam_map_sub_ = this->create_subscription<nav_msgs::msg::OccupancyGrid>(\n            "map", 10, std::bind(&AIPerceptionIntegration::slamMapCallback, this, std::placeholders::_1));\n\n        camera_image_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            "camera/image_raw", 10, std::bind(&AIPerceptionIntegration::imageCallback, this, std::placeholders::_1));\n\n        // Publisher for AI-enhanced perception markers\n        ai_markers_pub_ = this->create_publisher<visualization_msgs::msg::MarkerArray>("ai_perception_markers", 10);\n\n        tf_buffer_ = std::make_unique<tf2_ros::Buffer>(this->get_clock());\n        tf_listener_ = std::make_unique<tf2_ros::TransformListener>(*tf_buffer_);\n\n        RCLCPP_INFO(this->get_logger(), "AI Perception Integration Node Initialized");\n    }\n\nprivate:\n    void slamMapCallback(const nav_msgs::msg::OccupancyGrid::SharedPtr msg)\n    {\n        // Process SLAM map data for AI reasoning\n        current_map_ = *msg;\n\n        // Extract semantic information from the map\n        auto semantic_features = extractSemanticFeatures(msg);\n\n        // Publish visualization markers for AI reasoning\n        publishSemanticMarkers(semantic_features);\n    }\n\n    void imageCallback(const sensor_msgs::msg::Image::SharedPtr msg)\n    {\n        // Convert ROS image to OpenCV format\n        cv_bridge::CvImagePtr cv_ptr;\n        try {\n            cv_ptr = cv_bridge::toCvCopy(msg, sensor_msgs::image_encodings::BGR8);\n        } catch (cv_bridge::Exception& e) {\n            RCLCPP_ERROR(this->get_logger(), "cv_bridge exception: %s", e.what());\n            return;\n        }\n\n        // Perform AI-based object detection and classification\n        auto detections = performObjectDetection(cv_ptr->image);\n\n        // Integrate object information with navigation system\n        integrateObjectInfo(detections);\n    }\n\n    struct SemanticFeature {\n        geometry_msgs::msg::Point position;\n        std::string type; // "obstacle", "free_space", "dynamic_object", etc.\n        double certainty;\n        double temporal_stability;\n    };\n\n    std::vector<SemanticFeature> extractSemanticFeatures(const nav_msgs::msg::OccupancyGrid::SharedPtr map_msg)\n    {\n        std::vector<SemanticFeature> features;\n\n        // Process occupancy grid to extract semantic information\n        for (size_t i = 0; i < map_msg->data.size(); ++i) {\n            int8_t value = map_msg->data[i];\n\n            if (value == -1) continue; // Unknown space\n\n            SemanticFeature feature;\n\n            // Convert grid index to world coordinates\n            int row = i / map_msg->info.width;\n            int col = i % map_msg->info.width;\n\n            feature.position.x = map_msg->info.origin.position.x + col * map_msg->info.resolution;\n            feature.position.y = map_msg->info.origin.position.y + row * map_msg->info.resolution;\n            feature.position.z = map_msg->info.origin.position.z;\n\n            // Classify based on occupancy value\n            if (value > 80) {\n                feature.type = "obstacle";\n                feature.certainty = 0.9;\n            } else if (value < 20) {\n                feature.type = "free_space";\n                feature.certainty = 0.95;\n            } else {\n                feature.type = "uncertain";\n                feature.certainty = 0.6;\n            }\n\n            // Temporal stability analysis could be added here\n            feature.temporal_stability = 0.8; // Placeholder\n\n            features.push_back(feature);\n        }\n\n        return features;\n    }\n\n    void publishSemanticMarkers(const std::vector<SemanticFeature>& features)\n    {\n        visualization_msgs::msg::MarkerArray marker_array;\n\n        for (size_t i = 0; i < features.size(); ++i) {\n            visualization_msgs::msg::Marker marker;\n            marker.header.frame_id = "map";\n            marker.header.stamp = this->now();\n            marker.ns = "ai_semantic_features";\n            marker.id = static_cast<int>(i);\n            marker.type = visualization_msgs::msg::Marker::SPHERE;\n            marker.action = visualization_msgs::msg::Marker::ADD;\n\n            marker.pose.position = features[i].position;\n            marker.pose.orientation.w = 1.0;\n\n            marker.scale.x = 0.2;\n            marker.scale.y = 0.2;\n            marker.scale.z = 0.2;\n\n            // Color based on feature type\n            if (features[i].type == "obstacle") {\n                marker.color.r = 1.0;\n                marker.color.g = 0.0;\n                marker.color.b = 0.0;\n                marker.color.a = 0.8;\n            } else if (features[i].type == "free_space") {\n                marker.color.r = 0.0;\n                marker.color.g = 1.0;\n                marker.color.b = 0.0;\n                marker.color.a = 0.6;\n            } else {\n                marker.color.r = 1.0;\n                marker.color.g = 1.0;\n                marker.color.b = 0.0;\n                marker.color.a = 0.5;\n            }\n\n            marker_array.markers.push_back(marker);\n        }\n\n        ai_markers_pub_->publish(marker_array);\n    }\n\n    struct ObjectDetection {\n        geometry_msgs::msg::Point position;\n        std::string class_name;\n        double confidence;\n        bool is_dynamic;\n    };\n\n    std::vector<ObjectDetection> performObjectDetection(const cv::Mat& image)\n    {\n        std::vector<ObjectDetection> detections;\n\n        // This is a simplified version - in practice, you\'d use Isaac ROS DNN nodes\n        // or similar deep learning frameworks\n\n        // For demonstration, detect colored regions as objects\n        cv::Mat hsv_image;\n        cv::cvtColor(image, hsv_image, cv::COLOR_BGR2HSV);\n\n        // Detect red regions (potential obstacles)\n        cv::Mat red_mask;\n        cv::inRange(hsv_image, cv::Scalar(0, 50, 50), cv::Scalar(10, 255, 255), red_mask);\n        cv::inRange(hsv_image, cv::Scalar(170, 50, 50), cv::Scalar(180, 255, 255), red_mask);\n\n        // Find contours of detected objects\n        std::vector<std::vector<cv::Point>> contours;\n        cv::findContours(red_mask, contours, cv::RETR_EXTERNAL, cv::CHAIN_APPROX_SIMPLE);\n\n        for (const auto& contour : contours) {\n            if (cv::contourArea(contour) > 500) { // Filter small noise\n                ObjectDetection obj;\n\n                // Get bounding box center (approximate position)\n                cv::Rect bbox = cv::boundingRect(contour);\n                obj.position.x = bbox.x + bbox.width / 2.0;\n                obj.position.y = bbox.y + bbox.height / 2.0;\n                obj.position.z = 0.0; // Depth would come from stereo or depth camera\n\n                obj.class_name = "red_object";\n                obj.confidence = 0.7; // Placeholder\n                obj.is_dynamic = false; // Would be determined by temporal analysis\n\n                detections.push_back(obj);\n            }\n        }\n\n        return detections;\n    }\n\n    void integrateObjectInfo(const std::vector<ObjectDetection>& detections)\n    {\n        // Integrate object information with navigation system\n        // This would update costmaps, modify behavior trees, etc.\n\n        for (const auto& detection : detections) {\n            RCLCPP_INFO(this->get_logger(),\n                "Detected %s at (%.2f, %.2f) with confidence %.2f",\n                detection.class_name.c_str(),\n                detection.position.x,\n                detection.position.y,\n                detection.confidence);\n        }\n    }\n\n    rclcpp::Subscription<nav_msgs::msg::OccupancyGrid>::SharedPtr slam_map_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr camera_image_sub_;\n    rclcpp::Publisher<visualization_msgs::msg::MarkerArray>::SharedPtr ai_markers_pub_;\n\n    std::unique_ptr<tf2_ros::Buffer> tf_buffer_;\n    std::unique_ptr<tf2_ros::TransformListener> tf_listener_;\n\n    nav_msgs::msg::OccupancyGrid current_map_;\n};\n\nint main(int argc, char * argv[])\n{\n    rclcpp::init(argc, argv);\n    rclcpp::spin(std::make_shared<AIPerceptionIntegration>());\n    rclcpp::shutdown();\n    return 0;\n}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"configuring-the-ai-enhanced-navigation-launch-file",children:"Configuring the AI-Enhanced Navigation Launch File"}),"\n",(0,t.jsx)(n.p,{children:"Create a launch file that brings together all the AI-enhanced navigation components:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# ai_enhanced_navigation.launch.py\nimport os\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, RegisterEventHandler, EmitEvent\nfrom launch.conditions import IfCondition\nfrom launch.event_handlers import OnProcessExit\nfrom launch.events import Shutdown\nfrom launch.substitutions import LaunchConfiguration, PythonExpression\nfrom launch_ros.actions import Node\nfrom launch_ros.parameter_descriptions import ParameterValue\nfrom nav2_common.launch import RewrittenYaml\n\n\ndef generate_launch_description():\n    # Launch configurations\n    namespace = LaunchConfiguration('namespace')\n    use_sim_time = LaunchConfiguration('use_sim_time')\n    autostart = LaunchConfiguration('autostart')\n    params_file = LaunchConfiguration('params_file')\n    default_bt_xml_filename = LaunchConfiguration('default_bt_xml_filename')\n    map_subscribe_transient_local = LaunchConfiguration('map_subscribe_transient_local')\n\n    # Launch arguments\n    declare_namespace_cmd = DeclareLaunchArgument(\n        'namespace',\n        default_value='',\n        description='Top-level namespace')\n\n    declare_use_sim_time_cmd = DeclareLaunchArgument(\n        'use_sim_time',\n        default_value='true',\n        description='Use simulation (Gazebo) clock if true')\n\n    declare_autostart_cmd = DeclareLaunchArgument(\n        'autostart',\n        default_value='true',\n        description='Automatically startup the nav2 stack')\n\n    declare_params_file_cmd = DeclareLaunchArgument(\n        'params_file',\n        default_value=os.path.join(\n            get_package_share_directory('humanoid_navigation'),\n            'config',\n            'ai_nav_params.yaml'),\n        description='Full path to the ROS2 parameters file to use for all launched nodes')\n\n    declare_bt_xml_cmd = DeclareLaunchArgument(\n        'default_bt_xml_filename',\n        default_value=os.path.join(\n            get_package_share_directory('nav2_bt_navigator'),\n            'behavior_trees',\n            'ai_nav_to_pose.xml'),\n        description='Full path to the behavior tree xml file to use')\n\n    declare_map_subscribe_transient_local_cmd = DeclareLaunchArgument(\n        'map_subscribe_transient_local',\n        default_value='false',\n        description='Whether to set the map subscriber QoS to transient local')\n\n    # Create our own temporary YAML files that include substitutions\n    param_substitutions = {\n        'use_sim_time': use_sim_time,\n        'autostart': autostart,\n        'default_bt_xml_filename': default_bt_xml_filename,\n        'map_subscribe_transient_local': map_subscribe_transient_local}\n\n    configured_params = RewrittenYaml(\n        source_file=params_file,\n        root_key=namespace,\n        param_rewrites=param_substitutions,\n        convert_types=True)\n\n    # Nodes\n    lifecycle_nodes = ['controller_server',\n                       'planner_server',\n                       'recoveries_server',\n                       'bt_navigator',\n                       'waypoint_follower']\n\n    controller_server_node = Node(\n        package='nav2_controller',\n        executable='controller_server',\n        output='screen',\n        parameters=[configured_params])\n\n    planner_server_node = Node(\n        package='nav2_planner',\n        executable='planner_server',\n        name='planner_server',\n        output='screen',\n        parameters=[configured_params])\n\n    recoveries_server_node = Node(\n        package='nav2_recoveries',\n        executable='recoveries_server',\n        name='recoveries_server',\n        output='screen',\n        parameters=[configured_params])\n\n    bt_navigator_node = Node(\n        package='nav2_bt_navigator',\n        executable='bt_navigator',\n        name='bt_navigator',\n        output='screen',\n        parameters=[configured_params])\n\n    waypoint_follower_node = Node(\n        package='nav2_waypoint_follower',\n        executable='waypoint_follower',\n        name='waypoint_follower',\n        output='screen',\n        parameters=[configured_params])\n\n    lifecycle_manager_node = Node(\n        package='nav2_lifecycle_manager',\n        executable='lifecycle_manager',\n        name='lifecycle_manager',\n        output='screen',\n        parameters=[{'use_sim_time': use_sim_time},\n                    {'autostart': autostart},\n                    {'node_names': lifecycle_nodes}])\n\n    # AI-Enhanced Navigation Nodes\n    ai_obstacle_avoidance_node = Node(\n        package='humanoid_navigation',\n        executable='ai_obstacle_avoidance',\n        name='ai_obstacle_avoidance_node',\n        output='screen',\n        parameters=[\n            {'safe_distance': 0.8},\n            {'ai_reaction_threshold': 0.6},\n            {'max_linear_speed': 0.5},\n            {'max_angular_speed': 1.0},\n            {'prediction_horizon': 2.0}\n        ])\n\n    ai_perception_integration_node = Node(\n        package='humanoid_navigation',\n        executable='ai_perception_integration',\n        name='ai_perception_integration_node',\n        output='screen')\n\n    # Isaac ROS Perception Nodes (if using Isaac ROS)\n    isaac_ros_peoplesegnet_node = Node(\n        package='isaac_ros_peoplesegnet',\n        executable='isaac_ros_peoplesegnet',\n        name='isaac_ros_peoplesegnet',\n        parameters=[\n            {'input_image_width': 1920},\n            {'input_image_height': 1080},\n            {'network_image_width': 640},\n            {'network_image_height': 360},\n            {'threshold': 0.5}\n        ])\n\n    return LaunchDescription([\n        declare_namespace_cmd,\n        declare_use_sim_time_cmd,\n        declare_autostart_cmd,\n        declare_params_file_cmd,\n        declare_bt_xml_cmd,\n        declare_map_subscribe_transient_local_cmd,\n        controller_server_node,\n        planner_server_node,\n        recoveries_server_node,\n        bt_navigator_node,\n        waypoint_follower_node,\n        lifecycle_manager_node,\n        ai_obstacle_avoidance_node,\n        ai_perception_integration_node,\n        # isaac_ros_peoplesegnet_node  # Uncomment if using Isaac ROS\n    ])\n"})}),"\n",(0,t.jsx)(n.h2,{id:"testing-ai-enhanced-navigation-in-isaac-sim",children:"Testing AI-Enhanced Navigation in Isaac Sim"}),"\n",(0,t.jsx)(n.h3,{id:"setting-up-the-isaac-sim-environment",children:"Setting up the Isaac Sim Environment"}),"\n",(0,t.jsx)(n.p,{children:"Create a test scenario in Isaac Sim to validate your AI-enhanced navigation system:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# ai_navigation_test_scenario.py\nimport omni\nfrom pxr import Gf, UsdGeom, PhysxSchema\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.isaac.core.utils.carb import carb_settings_get\nfrom omni.isaac.core.objects import DynamicCuboid\nfrom omni.isaac.range_sensor import _range_sensor\nfrom omni.isaac.core.articulations import Articulation\nfrom omni.isaac.core.prims import RigidPrim\nimport numpy as np\nimport math\nimport carb\n\nclass AIEnhancedNavigationTestScenario:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self._setup_scene()\n\n    def _setup_scene(self):\n        """Setup the test scene with obstacles and navigation challenges"""\n        # Add ground plane\n        self.world.scene.add_default_ground_plane()\n\n        # Add humanoid robot (assuming you have one imported)\n        asset_path = get_assets_root_path() + "/Isaac/Robots/NVIDIA/IsaacLab/unitree_a1.usd"\n        add_reference_to_stage(usd_path=asset_path, prim_path="/World/humanoid_robot")\n\n        # Add static obstacles\n        self.world.scene.add(DynamicCuboid(\n            prim_path="/World/obstacle1",\n            name="obstacle1",\n            position=np.array([2.0, 0.0, 0.5]),\n            size=0.5,\n            color=np.array([0.8, 0.2, 0.2])\n        ))\n\n        self.world.scene.add(DynamicCuboid(\n            prim_path="/World/obstacle2",\n            name="obstacle2",\n            position=np.array([3.5, 1.0, 0.5]),\n            size=0.5,\n            color=np.array([0.2, 0.8, 0.2])\n        ))\n\n        # Add dynamic obstacles that move during simulation\n        self.world.scene.add(DynamicCuboid(\n            prim_path="/World/dynamic_obstacle",\n            name="dynamic_obstacle",\n            position=np.array([1.5, 2.0, 0.5]),\n            size=0.3,\n            color=np.array([0.2, 0.2, 0.8])\n        ))\n\n        # Add goal position marker\n        self.world.scene.add(DynamicCuboid(\n            prim_path="/World/goal_marker",\n            name="goal_marker",\n            position=np.array([5.0, 0.0, 0.1]),\n            size=0.2,\n            color=np.array([0.0, 1.0, 0.0])\n        ))\n\n    def run_simulation(self):\n        """Run the AI-enhanced navigation simulation"""\n        self.world.reset()\n\n        # Start simulation\n        while simulation_app.is_running():\n            self.world.step(render=True)\n\n            # Get current robot pose\n            robot = self.world.scene.get_object("humanoid_robot")\n            if robot:\n                current_pose = robot.get_world_pose()\n\n                # Send navigation goal to AI system\n                if self.world.current_time_step_index % 100 == 0:  # Every 100 steps\n                    goal = [5.0, 0.0, 0.0]  # Goal position\n                    self.send_navigation_goal(current_pose, goal)\n\n            # Move dynamic obstacle periodically\n            if self.world.current_time_step_index % 200 == 0:\n                self.move_dynamic_obstacle()\n\n    def send_navigation_goal(self, current_pose, goal):\n        """Send navigation goal to the AI system"""\n        # This would interface with your ROS2 navigation system\n        print(f"Sending navigation goal: {goal} from current pose: {current_pose}")\n\n    def move_dynamic_obstacle(self):\n        """Move the dynamic obstacle to simulate real-world conditions"""\n        obstacle = self.world.scene.get_object("dynamic_obstacle")\n        if obstacle:\n            current_pos = obstacle.get_world_pose()[0]\n            new_pos = [current_pos[0], current_pos[1] + 0.1, current_pos[2]]  # Move upward\n            obstacle.set_world_pose(position=new_pos)\n\n# Main execution\nsimulation_app = omni.kit.acquire_kit("AIEnhancedNavigationTest")\nscenario = AIEnhancedNavigationTestScenario()\nscenario.run_simulation()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"integrating-isaac-ros-hardware-acceleration",children:"Integrating Isaac ROS Hardware Acceleration"}),"\n",(0,t.jsx)(n.h3,{id:"using-isaac-ros-for-ai-enhanced-perception",children:"Using Isaac ROS for AI-Enhanced Perception"}),"\n",(0,t.jsx)(n.p,{children:"To leverage Isaac ROS hardware acceleration for your AI-enhanced navigation, you can integrate Isaac ROS perception nodes:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Install Isaac ROS packages for perception\nsudo apt install ros-humble-isaac-ros-pointcloud-utils\nsudo apt install ros-humble-isaac-ros-stereo-rectifier\nsudo apt install ros-humble-isaac-ros-visual- slam\nsudo apt install ros-humble-isaac-ros-segmentation\n"})}),"\n",(0,t.jsx)(n.h3,{id:"creating-an-isaac-ros-integration-launch-file",children:"Creating an Isaac ROS Integration Launch File"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# isaac_ros_ai_navigation.launch.py\nimport os\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import ComposableNodeContainer\nfrom launch_ros.descriptions import ComposableNode\n\n\ndef generate_launch_description():\n    # Launch arguments\n    namespace = LaunchConfiguration('namespace')\n    use_sim_time = LaunchConfiguration('use_sim_time')\n\n    # Isaac ROS Perception Container\n    perception_container = ComposableNodeContainer(\n        name='isaac_ros_perception_container',\n        namespace=namespace,\n        package='rclcpp_components',\n        executable='component_container_mt',\n        composable_node_descriptions=[\n            # Image Rectification for stereo cameras\n            ComposableNode(\n                package='isaac_ros_stereo_rectifier',\n                plugin='nvidia::isaac_ros::stereo_rectifier::RectifierNode',\n                name='rectifier_node',\n                parameters=[{\n                    'use_sim_time': use_sim_time,\n                    'left_camera_namespace': 'left_camera',\n                    'right_camera_namespace': 'right_camera'\n                }],\n                remappings=[\n                    ('left_image', 'left_camera/image_raw'),\n                    ('right_image', 'right_camera/image_raw'),\n                    ('left_camera_info', 'left_camera/camera_info'),\n                    ('right_camera_info', 'right_camera/camera_info'),\n                    ('left_rectified_image', 'left_camera/image_rect'),\n                    ('right_rectified_image', 'right_camera/image_rect')\n                ]\n            ),\n\n            # Segmentation for object detection\n            ComposableNode(\n                package='isaac_ros_peoplesegnet',\n                plugin='nvidia::isaac_ros::dnn_image_encoder::DnnImageEncoderNode',\n                name='dnn_encoder',\n                parameters=[{\n                    'use_sim_time': use_sim_time,\n                    'network_image_width': 640,\n                    'network_image_height': 360,\n                    'input_tensor_names': ['input'],\n                    'output_tensor_names': ['output'],\n                    'input_binding_names': ['input'],\n                    'output_binding_names': ['output']\n                }],\n                remappings=[\n                    ('encoded_tensor', 'tensor_sub'),\n                    ('image', 'left_camera/image_rect')\n                ]\n            ),\n\n            # Point cloud processing for 3D obstacle detection\n            ComposableNode(\n                package='isaac_ros_pointcloud_utils',\n                plugin='nvidia::isaac_ros::pointcloud_utils::PointCloudFilterNode',\n                name='pointcloud_filter',\n                parameters=[{\n                    'use_sim_time': use_sim_time,\n                    'min_distance': 0.5,\n                    'max_distance': 5.0\n                }],\n                remappings=[\n                    ('pointcloud', 'lidar/points'),\n                    ('filtered_pointcloud', 'filtered_obstacles')\n                ]\n            )\n        ],\n        output='screen'\n    )\n\n    return LaunchDescription([\n        DeclareLaunchArgument(\n            'namespace',\n            default_value='',\n            description='Top-level namespace'),\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='true',\n            description='Use simulation (Gazebo) clock if true'),\n        perception_container\n    ])\n"})}),"\n",(0,t.jsx)(n.h2,{id:"practical-implementation-steps",children:"Practical Implementation Steps"}),"\n",(0,t.jsx)(n.h3,{id:"step-1-build-and-compile-the-ai-nodes",children:"Step 1: Build and Compile the AI Nodes"}),"\n",(0,t.jsx)(n.p,{children:"First, compile your custom AI navigation nodes:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd ~/humanoid_robot_ws\ncolcon build --packages-select humanoid_navigation\nsource install/setup.bash\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-2-launch-the-complete-ai-enhanced-navigation-system",children:"Step 2: Launch the Complete AI-Enhanced Navigation System"}),"\n",(0,t.jsx)(n.p,{children:"Create a combined launch file that starts all necessary components:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Launch the complete system\nros2 launch humanoid_navigation ai_enhanced_navigation.launch.py\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-3-test-navigation-with-dynamic-obstacles",children:"Step 3: Test Navigation with Dynamic Obstacles"}),"\n",(0,t.jsx)(n.p,{children:"In a separate terminal, send navigation goals to test the AI system:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Send a navigation goal\nros2 action send_goal /navigate_to_pose nav2_msgs/action/NavigateToPose \"{pose: {header: {frame_id: 'map'}, pose: {position: {x: 5.0, y: 0.0, z: 0.0}, orientation: {w: 1.0}}}}\"\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-4-monitor-ai-performance",children:"Step 4: Monitor AI Performance"}),"\n",(0,t.jsx)(n.p,{children:"Monitor the AI-enhanced navigation performance using various tools:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# View the navigation costmaps\nros2 run rviz2 rviz2 -d ~/humanoid_robot_ws/src/humanoid_navigation/rviz/ai_navigation.rviz\n\n# Monitor AI perception markers\nros2 topic echo /ai_perception_markers\n\n# Monitor obstacle detection\nros2 topic echo /cmd_vel\n"})}),"\n",(0,t.jsx)(n.h2,{id:"advanced-ai-reasoning-techniques",children:"Advanced AI Reasoning Techniques"}),"\n",(0,t.jsx)(n.h3,{id:"predictive-obstacle-tracking",children:"Predictive Obstacle Tracking"}),"\n",(0,t.jsx)(n.p,{children:"Enhance your system with predictive tracking capabilities:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-cpp",children:"// Enhanced obstacle prediction algorithm\nclass ObstaclePredictor {\npublic:\n    struct TrajectoryPrediction {\n        std::vector<geometry_msgs::msg::Point> predicted_path;\n        double confidence;\n        double time_horizon;\n    };\n\n    TrajectoryPrediction predictMovement(\n        const geometry_msgs::msg::Point& current_pos,\n        const geometry_msgs::msg::Vector3& velocity,\n        double time_horizon)\n    {\n        TrajectoryPrediction prediction;\n        prediction.time_horizon = time_horizon;\n        prediction.confidence = 0.8; // Base confidence\n\n        // Simple constant velocity prediction\n        for (double t = 0.1; t <= time_horizon; t += 0.1) {\n            geometry_msgs::msg::Point predicted_pos;\n            predicted_pos.x = current_pos.x + velocity.x * t;\n            predicted_pos.y = current_pos.y + velocity.y * t;\n            predicted_pos.z = current_pos.z + velocity.z * t;\n\n            prediction.predicted_path.push_back(predicted_pos);\n        }\n\n        return prediction;\n    }\n};\n"})}),"\n",(0,t.jsx)(n.h3,{id:"multi-objective-path-optimization",children:"Multi-Objective Path Optimization"}),"\n",(0,t.jsx)(n.p,{children:"Implement a system that considers multiple objectives simultaneously:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-cpp",children:"// Multi-objective path optimizer\nclass MultiObjectiveOptimizer {\npublic:\n    enum ObjectiveType {\n        MINIMIZE_DISTANCE,\n        MAXIMIZE_SAFETY,\n        MINIMIZE_ENERGY,\n        MAXIMIZE_COMFORT\n    };\n\n    struct OptimizationResult {\n        std::vector<geometry_msgs::msg::PoseStamped> optimal_path;\n        std::map<ObjectiveType, double> objective_values;\n        double overall_score;\n    };\n\n    OptimizationResult optimizePath(\n        const std::vector<geometry_msgs::msg::PoseStamped>& candidate_paths,\n        const std::map<ObjectiveType, double>& weights)\n    {\n        OptimizationResult best_result;\n        double best_score = -std::numeric_limits<double>::infinity();\n\n        for (const auto& path : candidate_paths) {\n            OptimizationResult result = evaluatePath(path, weights);\n            if (result.overall_score > best_score) {\n                best_score = result.overall_score;\n                best_result = result;\n            }\n        }\n\n        return best_result;\n    }\n\nprivate:\n    OptimizationResult evaluatePath(\n        const std::vector<geometry_msgs::msg::PoseStamped>& path,\n        const std::map<ObjectiveType, double>& weights)\n    {\n        OptimizationResult result;\n\n        // Calculate individual objective values\n        result.objective_values[MINIMIZE_DISTANCE] = calculatePathDistance(path);\n        result.objective_values[MAXIMIZE_SAFETY] = calculatePathSafety(path);\n        result.objective_values[MINIMIZE_ENERGY] = calculateEnergyEfficiency(path);\n        result.objective_values[MAXIMIZE_COMFORT] = calculateComfort(path);\n\n        // Weighted sum for overall score\n        result.overall_score = 0.0;\n        for (const auto& weight_pair : weights) {\n            result.overall_score += weight_pair.second * result.objective_values[weight_pair.first];\n        }\n\n        return result;\n    }\n\n    double calculatePathDistance(const std::vector<geometry_msgs::msg::PoseStamped>& path) {\n        // Implementation for path distance calculation\n        double distance = 0.0;\n        for (size_t i = 1; i < path.size(); ++i) {\n            double dx = path[i].pose.position.x - path[i-1].pose.position.x;\n            double dy = path[i].pose.position.y - path[i-1].pose.position.y;\n            distance += std::sqrt(dx*dx + dy*dy);\n        }\n        return -distance; // Negative because minimizing distance is better\n    }\n\n    double calculatePathSafety(const std::vector<geometry_msgs::msg::PoseStamped>& path) {\n        // Implementation for path safety calculation\n        // Higher values indicate safer paths\n        double safety_score = 0.0;\n        for (const auto& pose : path) {\n            // Check proximity to obstacles, etc.\n            safety_score += 1.0; // Placeholder\n        }\n        return safety_score / path.size();\n    }\n\n    double calculateEnergyEfficiency(const std::vector<geometry_msgs::msg::PoseStamped>& path) {\n        // Implementation for energy efficiency calculation\n        return 1.0; // Placeholder\n    }\n\n    double calculateComfort(const std::vector<geometry_msgs::msg::PoseStamped>& path) {\n        // Implementation for comfort calculation\n        return 1.0; // Placeholder\n    }\n};\n"})}),"\n",(0,t.jsx)(n.h2,{id:"performance-validation-and-tuning",children:"Performance Validation and Tuning"}),"\n",(0,t.jsx)(n.h3,{id:"testing-different-scenarios",children:"Testing Different Scenarios"}),"\n",(0,t.jsx)(n.p,{children:"Test your AI-enhanced navigation system under various conditions:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Static Environments"}),": Test navigation in environments with only static obstacles"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dynamic Environments"}),": Test with moving obstacles and people"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cluttered Environments"}),": Test in narrow passages and crowded spaces"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Long-Distance Navigation"}),": Test navigation over long distances"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Emergency Scenarios"}),": Test sudden obstacle appearance and emergency stops"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,t.jsx)(n.p,{children:"Monitor these key performance indicators:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Navigation Success Rate"}),": Percentage of successful goal reaches"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Average Path Efficiency"}),": Ratio of actual path length to optimal path length"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Collision Avoidance Rate"}),": Percentage of successful obstacle avoidance"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Computational Load"}),": CPU and GPU usage during navigation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Response Time"}),": Time to react to dynamic obstacles"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"In this lesson, you've learned to implement AI-enhanced navigation and obstacle avoidance systems for humanoid robots. You've covered:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"AI Integration"}),": How to combine AI reasoning with traditional navigation systems"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Advanced Obstacle Avoidance"}),": Implementing intelligent algorithms that handle both static and dynamic obstacles"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perception Integration"}),": Connecting visual SLAM data with navigation decision-making"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Behavior Trees"}),": Creating sophisticated navigation behaviors with AI reasoning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hardware Acceleration"}),": Leveraging Isaac ROS for performance optimization"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Testing and Validation"}),": Methods for validating AI-enhanced navigation performance"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"These AI-enhanced navigation capabilities form a crucial foundation for the cognitive architectures you'll implement in Module 4, where the navigation system will be integrated with higher-level decision-making and reasoning systems. The adaptive behavior and intelligent obstacle avoidance you've developed will enable humanoid robots to navigate complex, dynamic environments with human-like intelligence and adaptability."})]})}function _(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>s,x:()=>r});var i=a(6540);const t={},o=i.createContext(t);function s(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);