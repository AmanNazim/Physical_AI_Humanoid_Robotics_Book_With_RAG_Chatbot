"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[560],{4280:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>c,frontMatter:()=>t,metadata:()=>i,toc:()=>m});const i=JSON.parse('{"id":"module-2/Physics-&-Sensors/lesson-2.3-depth-camera-and-imu-simulation","title":"Lesson 2.3 \u2013 Depth Camera and IMU Simulation","description":"Learning Objectives","source":"@site/docs/module-2/02-Physics-&-Sensors/lesson-2.3-depth-camera-and-imu-simulation.md","sourceDirName":"module-2/02-Physics-&-Sensors","slug":"/module-2/Physics-&-Sensors/lesson-2.3-depth-camera-and-imu-simulation","permalink":"/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-2/Physics-&-Sensors/lesson-2.3-depth-camera-and-imu-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/AmanNazim/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/edit/main/physical-ai-humanoid-robotics-book/docs/module-2/02-Physics-&-Sensors/lesson-2.3-depth-camera-and-imu-simulation.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Lesson 2.3 \u2013 Depth Camera and IMU Simulation","sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 2.2 \u2013 LiDAR Simulation in Virtual Environments","permalink":"/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-2/Physics-&-Sensors/lesson-2.2-lidar-simulation-in-virtual-environments"},"next":{"title":"Chapter 3 \u2013 Unity Digital Twin","permalink":"/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-2/Unity-Digital-Twin/"}}');var r=a(4848),o=a(8453);const t={title:"Lesson 2.3 \u2013 Depth Camera and IMU Simulation",sidebar_position:5},s="Lesson 2.3 \u2013 Depth Camera and IMU Simulation",l={},m=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Depth Camera and IMU Simulation",id:"introduction-to-depth-camera-and-imu-simulation",level:2},{value:"Depth Camera Simulation in Gazebo",id:"depth-camera-simulation-in-gazebo",level:2},{value:"Understanding Depth Cameras",id:"understanding-depth-cameras",level:3},{value:"Configuring Depth Cameras in Gazebo",id:"configuring-depth-cameras-in-gazebo",level:3},{value:"Advanced Depth Camera Configuration",id:"advanced-depth-camera-configuration",level:3},{value:"IMU Sensor Simulation",id:"imu-sensor-simulation",level:2},{value:"Understanding IMU Sensors",id:"understanding-imu-sensors",level:3},{value:"Configuring IMU Sensors in Gazebo",id:"configuring-imu-sensors-in-gazebo",level:3},{value:"Advanced IMU Configuration",id:"advanced-imu-configuration",level:3},{value:"Processing Depth Camera and IMU Data",id:"processing-depth-camera-and-imu-data",level:2},{value:"Depth Camera Data Processing",id:"depth-camera-data-processing",level:3},{value:"IMU Data Processing",id:"imu-data-processing",level:3},{value:"Sensor Fusion: Combining Depth Camera and IMU Data",id:"sensor-fusion-combining-depth-camera-and-imu-data",level:2},{value:"Understanding Sensor Fusion",id:"understanding-sensor-fusion",level:3},{value:"Implementing Basic Sensor Fusion",id:"implementing-basic-sensor-fusion",level:3},{value:"Advanced Sensor Fusion with Kalman Filtering",id:"advanced-sensor-fusion-with-kalman-filtering",level:2},{value:"Validation and Testing",id:"validation-and-testing",level:2},{value:"Testing Depth Camera Performance",id:"testing-depth-camera-performance",level:3},{value:"Testing IMU Performance",id:"testing-imu-performance",level:3},{value:"Practical Exercise: Complete Sensor Integration",id:"practical-exercise-complete-sensor-integration",level:2},{value:"Step 1: Add All Sensors to Your Robot Model",id:"step-1-add-all-sensors-to-your-robot-model",level:3},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"lesson-23--depth-camera-and-imu-simulation",children:"Lesson 2.3 \u2013 Depth Camera and IMU Simulation"})}),"\n",(0,r.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(e.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Implement depth cameras in Gazebo simulation environment with realistic depth image generation"}),"\n",(0,r.jsx)(e.li,{children:"Simulate IMU sensors for orientation sensing capabilities in humanoid robotics"}),"\n",(0,r.jsx)(e.li,{children:"Integrate depth camera and IMU data for sensor fusion in humanoid robotics applications"}),"\n",(0,r.jsx)(e.li,{children:"Process multiple sensor types using ROS 2 communication patterns"}),"\n",(0,r.jsx)(e.li,{children:"Validate depth camera and IMU performance in simulation"}),"\n",(0,r.jsx)(e.li,{children:"Create sensor fusion algorithms that combine depth and IMU data"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"introduction-to-depth-camera-and-imu-simulation",children:"Introduction to Depth Camera and IMU Simulation"}),"\n",(0,r.jsx)(e.p,{children:"In this lesson, we'll complete the sensor suite for your humanoid robot by implementing depth cameras and IMU sensors. These sensors provide complementary information:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Depth Cameras"}),": Generate 3D spatial information from visual data, enabling object recognition, scene understanding, and 3D mapping"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"IMU (Inertial Measurement Unit)"}),": Track orientation, acceleration, and angular velocity, providing crucial motion and stability information"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"Together with the LiDAR sensors from the previous lesson, these sensors form a comprehensive perception system for your humanoid robot."}),"\n",(0,r.jsx)(e.h2,{id:"depth-camera-simulation-in-gazebo",children:"Depth Camera Simulation in Gazebo"}),"\n",(0,r.jsx)(e.h3,{id:"understanding-depth-cameras",children:"Understanding Depth Cameras"}),"\n",(0,r.jsx)(e.p,{children:"Depth cameras provide both color (RGB) and depth information for each pixel, creating rich 3D representations of the environment. In simulation, depth cameras must accurately model:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Intrinsic parameters"}),": Focal length, principal point, distortion coefficients"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Extrinsic parameters"}),": Position and orientation relative to the robot"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Depth accuracy"}),": Noise characteristics and measurement precision"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Field of view"}),": Horizontal and vertical viewing angles"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"configuring-depth-cameras-in-gazebo",children:"Configuring Depth Cameras in Gazebo"}),"\n",(0,r.jsx)(e.p,{children:"Let's add a depth camera to your humanoid robot model:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Depth camera link --\x3e\n<link name="depth_camera_link">\n  <inertial>\n    <mass value="0.1" />\n    <origin xyz="0 0 0" rpy="0 0 0" />\n    <inertia ixx="0.0001" ixy="0" ixz="0" iyy="0.0001" iyz="0" izz="0.0001" />\n  </inertial>\n\n  <visual>\n    <origin xyz="0 0 0" rpy="0 0 0" />\n    <geometry>\n      <box size="0.02 0.04 0.02" />\n    </geometry>\n    <material name="black">\n      <color rgba="0.1 0.1 0.1 1" />\n    </material>\n  </visual>\n\n  <collision>\n    <origin xyz="0 0 0" rpy="0 0 0" />\n    <geometry>\n      <box size="0.02 0.04 0.02" />\n    </geometry>\n  </collision>\n</link>\n\n\x3c!-- Joint to connect depth camera to robot --\x3e\n<joint name="depth_camera_joint" type="fixed">\n  <parent link="base_link" />\n  <child link="depth_camera_link" />\n  <origin xyz="0.05 0 1.0" rpy="0 0 0" />  \x3c!-- Positioned on robot\'s head --\x3e\n</joint>\n\n\x3c!-- Gazebo plugin for depth camera --\x3e\n<gazebo reference="depth_camera_link">\n  <sensor name="depth_camera" type="depth">\n    <pose>0 0 0 0 0 0</pose>\n    <visualize>true</visualize>\n    <update_rate>30</update_rate>\n    <camera name="head">\n      <horizontal_fov>1.047</horizontal_fov>  \x3c!-- 60 degrees in radians --\x3e\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>10.0</far>\n      </clip>\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.007</stddev>\n      </noise>\n    </camera>\n    <plugin name="depth_camera_controller" filename="libgazebo_ros_openni_kinect.so">\n      <ros>\n        <namespace>/humanoid_robot</namespace>\n        <remapping>rgb/image_raw:=rgb/image_raw</remapping>\n        <remapping>rgb/camera_info:=rgb/camera_info</remapping>\n        <remapping>depth/image_raw:=depth/image_raw</remapping>\n        <remapping>depth/camera_info:=depth/camera_info</remapping>\n      </ros>\n      <frame_name>depth_camera_link</frame_name>\n      <baseline>0.1</baseline>\n      <distortion_k1>0.0</distortion_k1>\n      <distortion_k2>0.0</distortion_k2>\n      <distortion_k3>0.0</distortion_k3>\n      <distortion_t1>0.0</distortion_t1>\n      <distortion_t2>0.0</distortion_t2>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,r.jsx)(e.h3,{id:"advanced-depth-camera-configuration",children:"Advanced Depth Camera Configuration"}),"\n",(0,r.jsx)(e.p,{children:"For more sophisticated applications, you might want to configure stereo cameras or higher-resolution depth sensors:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-xml",children:'\x3c!-- High-resolution depth camera --\x3e\n<gazebo reference="high_res_depth_camera_link">\n  <sensor name="high_res_depth_camera" type="depth">\n    <pose>0 0 0 0 0 0</pose>\n    <visualize>true</visualize>\n    <update_rate>15</update_rate>\n    <camera name="high_res">\n      <horizontal_fov>1.396</horizontal_fov>  \x3c!-- 80 degrees --\x3e\n      <image>\n        <width>1280</width>  \x3c!-- Higher resolution --\x3e\n        <height>720</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.05</near>    \x3c!-- Closer minimum range --\x3e\n        <far>15.0</far>\n      </clip>\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.005</stddev>  \x3c!-- Lower noise for high-res --\x3e\n      </noise>\n    </camera>\n    <plugin name="high_res_depth_controller" filename="libgazebo_ros_openni_kinect.so">\n      <ros>\n        <namespace>/humanoid_robot/high_res</namespace>\n        <remapping>rgb/image_raw:=rgb/image_raw</remapping>\n        <remapping>rgb/camera_info:=rgb/camera_info</remapping>\n        <remapping>depth/image_raw:=depth/image_raw</remapping>\n        <remapping>depth/camera_info:=depth/camera_info</remapping>\n      </ros>\n      <frame_name>high_res_depth_camera_link</frame_name>\n      <point_cloud_cutoff>0.1</point_cloud_cutoff>\n      <point_cloud_cutoff_max>10.0</point_cloud_cutoff_max>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,r.jsx)(e.h2,{id:"imu-sensor-simulation",children:"IMU Sensor Simulation"}),"\n",(0,r.jsx)(e.h3,{id:"understanding-imu-sensors",children:"Understanding IMU Sensors"}),"\n",(0,r.jsx)(e.p,{children:"An IMU typically combines multiple sensors:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"3-axis Accelerometer"}),": Measures linear acceleration"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"3-axis Gyroscope"}),": Measures angular velocity"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"3-axis Magnetometer"}),": Measures magnetic field (for heading)"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"In humanoid robotics, IMUs are crucial for:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Balance and stability control"}),"\n",(0,r.jsx)(e.li,{children:"Motion tracking"}),"\n",(0,r.jsx)(e.li,{children:"Orientation estimation"}),"\n",(0,r.jsx)(e.li,{children:"Fall detection"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"configuring-imu-sensors-in-gazebo",children:"Configuring IMU Sensors in Gazebo"}),"\n",(0,r.jsx)(e.p,{children:"Let's add an IMU to your humanoid robot:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-xml",children:'\x3c!-- IMU link (typically placed at robot\'s center of mass) --\x3e\n<link name="imu_link">\n  <inertial>\n    <mass value="0.01" />\n    <origin xyz="0 0 0" rpy="0 0 0" />\n    <inertia ixx="1e-6" ixy="0" ixz="0" iyy="1e-6" iyz="0" izz="1e-6" />\n  </inertial>\n\n  <visual>\n    <origin xyz="0 0 0" rpy="0 0 0" />\n    <geometry>\n      <box size="0.01 0.01 0.01" />\n    </geometry>\n    <material name="red">\n      <color rgba="1 0 0 1" />\n    </material>\n  </visual>\n</link>\n\n\x3c!-- Joint to connect IMU to robot\'s body --\x3e\n<joint name="imu_joint" type="fixed">\n  <parent link="base_link" />\n  <child link="imu_link" />\n  <origin xyz="0 0 0.5" rpy="0 0 0" />  \x3c!-- At robot\'s center of mass --\x3e\n</joint>\n\n\x3c!-- Gazebo plugin for IMU --\x3e\n<gazebo reference="imu_link">\n  <sensor name="imu_sensor" type="imu">\n    <pose>0 0 0 0 0 0</pose>\n    <always_on>true</always_on>\n    <update_rate>100</update_rate>\n    <visualize>false</visualize>\n    <imu>\n      <angular_velocity>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0017</stddev>  \x3c!-- ~0.1 deg/s (gyro noise) --\x3e\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0017</stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0017</stddev>\n          </noise>\n        </z>\n      </angular_velocity>\n      <linear_acceleration>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.017</stddev>   \x3c!-- ~0.017 m/s\xb2 (accel noise) --\x3e\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.017</stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.017</stddev>\n          </noise>\n        </z>\n      </linear_acceleration>\n    </imu>\n    <plugin name="imu_controller" filename="libgazebo_ros_imu.so">\n      <ros>\n        <namespace>/humanoid_robot</namespace>\n        <remapping>~/out:=imu/data</remapping>\n      </ros>\n      <frame_name>imu_link</frame_name>\n      <body_name>base_link</body_name>\n      <update_rate>100</update_rate>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,r.jsx)(e.h3,{id:"advanced-imu-configuration",children:"Advanced IMU Configuration"}),"\n",(0,r.jsx)(e.p,{children:"For more realistic IMU simulation, you can add bias and drift characteristics:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Advanced IMU with bias and drift --\x3e\n<gazebo reference="advanced_imu_link">\n  <sensor name="advanced_imu_sensor" type="imu">\n    <pose>0 0 0 0 0 0</pose>\n    <always_on>true</always_on>\n    <update_rate>200</update_rate>\n    <visualize>false</visualize>\n    <imu>\n      <angular_velocity>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.001</stddev>\n            <bias_mean>0.0001</bias_mean>    \x3c!-- Bias --\x3e\n            <bias_stddev>0.00001</bias_stddev>  \x3c!-- Bias drift --\x3e\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.001</stddev>\n            <bias_mean>0.0001</bias_mean>\n            <bias_stddev>0.00001</bias_stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.001</stddev>\n            <bias_mean>0.0001</bias_mean>\n            <bias_stddev>0.00001</bias_stddev>\n          </noise>\n        </z>\n      </angular_velocity>\n      <linear_acceleration>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.01</stddev>\n            <bias_mean>0.001</bias_mean>\n            <bias_stddev>0.0001</bias_stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.01</stddev>\n            <bias_mean>0.001</bias_mean>\n            <bias_stddev>0.0001</bias_stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.01</stddev>\n            <bias_mean>0.001</bias_mean>\n            <bias_stddev>0.0001</bias_stddev>\n          </noise>\n        </z>\n      </linear_acceleration>\n    </imu>\n    <plugin name="advanced_imu_controller" filename="libgazebo_ros_imu.so">\n      <ros>\n        <namespace>/humanoid_robot</namespace>\n        <remapping>~/out:=imu/data_raw</remapping>\n      </ros>\n      <frame_name>advanced_imu_link</frame_name>\n      <body_name>base_link</body_name>\n      <update_rate>200</update_rate>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,r.jsx)(e.h2,{id:"processing-depth-camera-and-imu-data",children:"Processing Depth Camera and IMU Data"}),"\n",(0,r.jsx)(e.h3,{id:"depth-camera-data-processing",children:"Depth Camera Data Processing"}),"\n",(0,r.jsx)(e.p,{children:"Here's a Python node to process depth camera data:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nDepth camera processing node for humanoid robot\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass DepthCameraProcessor(Node):\n    def __init__(self):\n        super().__init__('depth_camera_processor')\n\n        # Initialize CvBridge for image conversion\n        self.bridge = CvBridge()\n\n        # Subscribe to RGB and depth images\n        self.rgb_sub = self.create_subscription(\n            Image,\n            '/humanoid_robot/rgb/image_raw',\n            self.rgb_callback,\n            10\n        )\n\n        self.depth_sub = self.create_subscription(\n            Image,\n            '/humanoid_robot/depth/image_raw',\n            self.depth_callback,\n            10\n        )\n\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            '/humanoid_robot/rgb/camera_info',\n            self.camera_info_callback,\n            10\n        )\n\n        # Publisher for processed depth data\n        self.point_cloud_pub = self.create_publisher(\n            PointCloud2,\n            '/humanoid_robot/point_cloud',\n            10\n        )\n\n        self.camera_info = None\n        self.get_logger().info('Depth Camera Processor initialized')\n\n    def rgb_callback(self, msg):\n        \"\"\"Process RGB image data\"\"\"\n        try:\n            # Convert ROS Image message to OpenCV image\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Process RGB image (example: edge detection)\n            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n            edges = cv2.Canny(gray, 50, 150)\n\n            # You can add more sophisticated image processing here\n            self.get_logger().debug(f'Processed RGB image: {cv_image.shape}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing RGB image: {e}')\n\n    def depth_callback(self, msg):\n        \"\"\"Process depth image data\"\"\"\n        try:\n            # Convert depth image to numpy array\n            depth_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='passthrough')\n\n            # Convert to float32 for processing\n            if depth_image.dtype == np.uint16:\n                depth_image = depth_image.astype(np.float32) / 1000.0  # Convert mm to meters\n\n            # Process depth data\n            valid_depths = depth_image[depth_image > 0]\n\n            if len(valid_depths) > 0:\n                avg_depth = np.mean(valid_depths)\n                min_depth = np.min(valid_depths)\n                max_depth = np.max(valid_depths)\n\n                self.get_logger().debug(f'Depth stats - Avg: {avg_depth:.2f}m, Min: {min_depth:.2f}m, Max: {max_depth:.2f}m')\n\n            # Generate point cloud if camera info is available\n            if self.camera_info is not None:\n                point_cloud = self.generate_point_cloud(depth_image, self.camera_info)\n                if point_cloud is not None:\n                    self.point_cloud_pub.publish(point_cloud)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing depth image: {e}')\n\n    def camera_info_callback(self, msg):\n        \"\"\"Store camera calibration information\"\"\"\n        self.camera_info = msg\n\n    def generate_point_cloud(self, depth_image, camera_info):\n        \"\"\"Generate point cloud from depth image and camera info\"\"\"\n        try:\n            # Extract camera parameters\n            fx = camera_info.k[0]  # Focal length x\n            fy = camera_info.k[4]  # Focal length y\n            cx = camera_info.k[2]  # Principal point x\n            cy = camera_info.k[5]  # Principal point y\n\n            height, width = depth_image.shape\n            points = []\n\n            # Generate 3D points from depth image\n            for v in range(height):\n                for u in range(width):\n                    depth = depth_image[v, u]\n\n                    # Skip invalid depth values\n                    if depth <= 0 or depth > 10.0:  # Max range 10m\n                        continue\n\n                    # Convert pixel coordinates to 3D world coordinates\n                    x = (u - cx) * depth / fx\n                    y = (v - cy) * depth / fy\n                    z = depth\n\n                    points.append([x, y, z])\n\n            if points:\n                # Create PointCloud2 message\n                header = Header()\n                header.stamp = self.get_clock().now().to_msg()\n                header.frame_id = 'depth_camera_link'\n\n                fields = [\n                    PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),\n                    PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),\n                    PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1)\n                ]\n\n                point_cloud = point_cloud2.create_cloud(header, fields, points)\n                return point_cloud\n\n            return None\n\n        except Exception as e:\n            self.get_logger().error(f'Error generating point cloud: {e}')\n            return None\n\ndef main(args=None):\n    rclpy.init(args=args)\n    processor = DepthCameraProcessor()\n\n    try:\n        rclpy.spin(processor)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        processor.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(e.h3,{id:"imu-data-processing",children:"IMU Data Processing"}),"\n",(0,r.jsx)(e.p,{children:"Here's a Python node to process IMU data:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nIMU processing node for humanoid robot\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu\nfrom geometry_msgs.msg import Vector3, Quaternion\nfrom std_msgs.msg import Float64\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\nclass IMUProcessor(Node):\n    def __init__(self):\n        super().__init__('imu_processor')\n\n        # Subscribe to IMU data\n        self.imu_sub = self.create_subscription(\n            Imu,\n            '/humanoid_robot/imu/data',\n            self.imu_callback,\n            10\n        )\n\n        # Publishers for processed data\n        self.orientation_pub = self.create_publisher(\n            Quaternion,\n            '/humanoid_robot/imu/orientation',\n            10\n        )\n\n        self.angular_velocity_pub = self.create_publisher(\n            Vector3,\n            '/humanoid_robot/imu/angular_velocity',\n            10\n        )\n\n        self.linear_acceleration_pub = self.create_publisher(\n            Vector3,\n            '/humanoid_robot/imu/linear_acceleration',\n            10\n        )\n\n        # Publishers for derived metrics\n        self.roll_pitch_yaw_pub = self.create_publisher(\n            Vector3,\n            '/humanoid_robot/imu/rpy',\n            10\n        )\n\n        self.balance_score_pub = self.create_publisher(\n            Float64,\n            '/humanoid_robot/imu/balance_score',\n            10\n        )\n\n        # Store previous values for filtering\n        self.prev_orientation = None\n        self.prev_time = None\n\n        self.get_logger().info('IMU Processor initialized')\n\n    def imu_callback(self, msg):\n        \"\"\"Process incoming IMU data\"\"\"\n        try:\n            # Extract orientation (as quaternion)\n            orientation = np.array([\n                msg.orientation.x,\n                msg.orientation.y,\n                msg.orientation.z,\n                msg.orientation.w\n            ])\n\n            # Extract angular velocity\n            angular_velocity = np.array([\n                msg.angular_velocity.x,\n                msg.angular_velocity.y,\n                msg.angular_velocity.z\n            ])\n\n            # Extract linear acceleration\n            linear_accel = np.array([\n                msg.linear_acceleration.x,\n                msg.linear_acceleration.y,\n                msg.linear_acceleration.z\n            ])\n\n            # Publish raw data\n            self.orientation_pub.publish(msg.orientation)\n            self.angular_velocity_pub.publish(msg.angular_velocity)\n            self.linear_acceleration_pub.publish(msg.linear_acceleration)\n\n            # Convert quaternion to roll-pitch-yaw\n            rpy = self.quaternion_to_rpy(orientation)\n            rpy_msg = Vector3()\n            rpy_msg.x = rpy[0]  # Roll\n            rpy_msg.y = rpy[1]  # Pitch\n            rpy_msg.z = rpy[2]  # Yaw\n            self.roll_pitch_yaw_pub.publish(rpy_msg)\n\n            # Calculate balance score based on orientation\n            balance_score = self.calculate_balance_score(rpy, linear_accel)\n            balance_msg = Float64()\n            balance_msg.data = balance_score\n            self.balance_score_pub.publish(balance_msg)\n\n            # Log important values\n            self.get_logger().debug(f'Roll: {np.degrees(rpy[0]):.2f}\xb0, Pitch: {np.degrees(rpy[1]):.2f}\xb0, Yaw: {np.degrees(rpy[2]):.2f}\xb0')\n            self.get_logger().debug(f'Balance Score: {balance_score:.3f}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing IMU data: {e}')\n\n    def quaternion_to_rpy(self, quat):\n        \"\"\"Convert quaternion to roll-pitch-yaw angles\"\"\"\n        try:\n            # Create rotation object from quaternion\n            rotation = R.from_quat(quat)\n            # Convert to Euler angles (roll, pitch, yaw)\n            rpy = rotation.as_euler('xyz')\n            return rpy\n        except Exception:\n            # Fallback calculation\n            w, x, y, z = quat\n            sinr_cosp = 2 * (w * x + y * z)\n            cosr_cosp = 1 - 2 * (x * x + y * y)\n            roll = np.arctan2(sinr_cosp, cosr_cosp)\n\n            sinp = 2 * (w * y - z * x)\n            pitch = np.arcsin(sinp)\n\n            siny_cosp = 2 * (w * z + x * y)\n            cosy_cosp = 1 - 2 * (y * y + z * z)\n            yaw = np.arctan2(siny_cosp, cosy_cosp)\n\n            return np.array([roll, pitch, yaw])\n\n    def calculate_balance_score(self, rpy, linear_accel):\n        \"\"\"Calculate a balance score based on orientation and acceleration\"\"\"\n        # Simple balance score: closer to upright position = higher score\n        # Roll and pitch should be close to 0 for good balance\n        roll_magnitude = abs(rpy[0])\n        pitch_magnitude = abs(rpy[1])\n\n        # Penalize large angular deviations\n        orientation_penalty = (roll_magnitude + pitch_magnitude) / 2.0\n\n        # Consider linear acceleration for dynamic balance\n        accel_magnitude = np.linalg.norm(linear_accel)\n\n        # Normalize to 0-1 scale (0 = perfectly balanced, 1 = falling)\n        max_angle = np.pi / 3  # 60 degrees before considered falling\n        balance_score = min(1.0, orientation_penalty / max_angle)\n\n        # Invert so higher score = better balance\n        return 1.0 - balance_score\n\ndef main(args=None):\n    rclpy.init(args=args)\n    processor = IMUProcessor()\n\n    try:\n        rclpy.spin(processor)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        processor.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(e.h2,{id:"sensor-fusion-combining-depth-camera-and-imu-data",children:"Sensor Fusion: Combining Depth Camera and IMU Data"}),"\n",(0,r.jsx)(e.h3,{id:"understanding-sensor-fusion",children:"Understanding Sensor Fusion"}),"\n",(0,r.jsx)(e.p,{children:"Sensor fusion combines data from multiple sensors to create a more accurate and reliable representation of the environment. For humanoid robots, combining depth camera and IMU data provides:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Stabilized depth data"}),": IMU orientation helps correct depth measurements during robot movement"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Enhanced pose estimation"}),": Visual features combined with IMU motion tracking"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Improved navigation"}),": More robust obstacle detection and avoidance"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"implementing-basic-sensor-fusion",children:"Implementing Basic Sensor Fusion"}),"\n",(0,r.jsx)(e.p,{children:"Here's a sensor fusion node that combines depth camera and IMU data:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nSensor fusion node for combining depth camera and IMU data\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu, Image\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import Float64\nfrom cv_bridge import CvBridge\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\nclass SensorFusion(Node):\n    def __init__(self):\n        super().__init__('sensor_fusion')\n\n        # Initialize CvBridge\n        self.bridge = CvBridge()\n\n        # Subscribe to IMU and depth camera data\n        self.imu_sub = self.create_subscription(\n            Imu,\n            '/humanoid_robot/imu/data',\n            self.imu_callback,\n            10\n        )\n\n        self.depth_sub = self.create_subscription(\n            Image,\n            '/humanoid_robot/depth/image_raw',\n            self.depth_callback,\n            10\n        )\n\n        # Publishers for fused data\n        self.fused_pose_pub = self.create_publisher(\n            PoseStamped,\n            '/humanoid_robot/fused_pose',\n            10\n        )\n\n        self.stabilized_depth_pub = self.create_publisher(\n            Image,\n            '/humanoid_robot/stabilized_depth',\n            10\n        )\n\n        self.motion_compensated_pub = self.create_publisher(\n            Float64,\n            '/humanoid_robot/motion_compensation_factor',\n            10\n        )\n\n        # Store sensor data\n        self.current_imu = None\n        self.current_depth = None\n        self.imu_timestamp = None\n        self.depth_timestamp = None\n\n        self.get_logger().info('Sensor Fusion node initialized')\n\n    def imu_callback(self, msg):\n        \"\"\"Store IMU data\"\"\"\n        self.current_imu = msg\n        self.imu_timestamp = msg.header.stamp\n\n    def depth_callback(self, msg):\n        \"\"\"Process depth image with IMU compensation\"\"\"\n        try:\n            self.current_depth = msg\n            self.depth_timestamp = msg.header.stamp\n\n            # If we have both IMU and depth data, perform fusion\n            if self.current_imu is not None:\n                self.perform_sensor_fusion()\n\n        except Exception as e:\n            self.get_logger().error(f'Error in depth callback: {e}')\n\n    def perform_sensor_fusion(self):\n        \"\"\"Perform sensor fusion between IMU and depth camera\"\"\"\n        try:\n            # Extract IMU orientation\n            imu_quat = np.array([\n                self.current_imu.orientation.x,\n                self.current_imu.orientation.y,\n                self.current_imu.orientation.z,\n                self.current_imu.orientation.w\n            ])\n\n            # Convert to rotation matrix\n            rotation = R.from_quat(imu_quat)\n            rotation_matrix = rotation.as_matrix()\n\n            # Extract angular velocity for motion compensation\n            angular_vel = np.array([\n                self.current_imu.angular_velocity.x,\n                self.current_imu.angular_velocity.y,\n                self.current_imu.angular_velocity.z\n            ])\n\n            # Calculate motion compensation factor\n            motion_compensation = np.linalg.norm(angular_vel)\n\n            # Create fused pose message\n            fused_pose = PoseStamped()\n            fused_pose.header.stamp = self.get_clock().now().to_msg()\n            fused_pose.header.frame_id = 'odom'\n\n            # Use IMU orientation as primary orientation source\n            fused_pose.pose.orientation = self.current_imu.orientation\n\n            # For position, we might integrate from other sources or use zero\n            # In a real implementation, you'd combine with other sensors like wheel encoders\n            fused_pose.pose.position.x = 0.0\n            fused_pose.pose.position.y = 0.0\n            fused_pose.pose.position.z = 0.0\n\n            # Publish fused pose\n            self.fused_pose_pub.publish(fused_pose)\n\n            # Publish motion compensation factor\n            motion_msg = Float64()\n            motion_msg.data = motion_compensation\n            self.motion_compensated_pub.publish(motion_msg)\n\n            # Convert depth image for potential stabilization\n            try:\n                depth_cv = self.bridge.imgmsg_to_cv2(self.current_depth, desired_encoding='passthrough')\n\n                # Apply motion compensation if needed\n                if motion_compensation > 0.1:  # Threshold for significant motion\n                    # In a real implementation, you would use the IMU data\n                    # to compensate for motion blur in the depth image\n                    self.get_logger().debug(f'Applying motion compensation: {motion_compensation:.3f}')\n\n                # Publish the (potentially) stabilized depth image\n                # For now, just republish the original\n                self.stabilized_depth_pub.publish(self.current_depth)\n\n            except Exception as e:\n                self.get_logger().warn(f'Error processing depth image: {e}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error in sensor fusion: {e}')\n\n    def calculate_pose_from_imu(self, imu_msg):\n        \"\"\"Calculate pose estimate from IMU data\"\"\"\n        # This is a simplified approach\n        # In practice, you'd use sensor fusion algorithms like Kalman filters\n        pose = PoseStamped()\n        pose.header = imu_msg.header\n        pose.pose.orientation = imu_msg.orientation\n\n        # Position would typically come from integration of acceleration\n        # or from other sensors\n        pose.pose.position.x = 0.0\n        pose.pose.position.y = 0.0\n        pose.pose.position.z = 0.0\n\n        return pose\n\ndef main(args=None):\n    rclpy.init(args=args)\n    fusion_node = SensorFusion()\n\n    try:\n        rclpy.spin(fusion_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        fusion_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(e.h2,{id:"advanced-sensor-fusion-with-kalman-filtering",children:"Advanced Sensor Fusion with Kalman Filtering"}),"\n",(0,r.jsx)(e.p,{children:"For more sophisticated sensor fusion, you might implement a Kalman filter:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nKalman filter-based sensor fusion for humanoid robot\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu\nfrom geometry_msgs.msg import Vector3\nimport numpy as np\n\nclass KalmanFilterFusion(Node):\n    def __init__(self):\n        super().__init__(\'kalman_fusion\')\n\n        # Subscribe to IMU data\n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'/humanoid_robot/imu/data\',\n            self.imu_callback,\n            10\n        )\n\n        # Initialize Kalman filter parameters\n        self.initialize_kalman_filter()\n\n        self.get_logger().info(\'Kalman Filter Fusion initialized\')\n\n    def initialize_kalman_filter(self):\n        """Initialize Kalman filter state and parameters"""\n        # State vector: [roll, pitch, yaw, roll_rate, pitch_rate, yaw_rate]\n        self.state = np.zeros(6)\n\n        # State covariance matrix\n        self.P = np.eye(6) * 0.1\n\n        # Process noise covariance\n        self.Q = np.eye(6) * 0.01\n\n        # Measurement noise covariance\n        self.R = np.eye(3) * 0.1  # For orientation measurements\n\n        # Control input matrix (not used in this example)\n        self.B = np.zeros((6, 0))\n\n        # Measurement matrix\n        self.H = np.hstack([np.eye(3), np.zeros((3, 3))])  # Observe orientation only\n\n    def imu_callback(self, msg):\n        """Process IMU data through Kalman filter"""\n        try:\n            # Extract measurement (orientation from IMU)\n            measurement = np.array([\n                msg.orientation.x,\n                msg.orientation.y,\n                msg.orientation.z\n            ])\n\n            # Extract control input (angular velocities)\n            control_input = np.array([\n                msg.angular_velocity.x,\n                msg.angular_velocity.y,\n                msg.angular_velocity.z\n            ])\n\n            # Prediction step\n            self.predict(control_input)\n\n            # Update step\n            self.update(measurement)\n\n            # Publish filtered state\n            self.publish_filtered_state()\n\n        except Exception as e:\n            self.get_logger().error(f\'Kalman filter error: {e}\')\n\n    def predict(self, control_input):\n        """Prediction step of Kalman filter"""\n        # State transition model (simplified)\n        dt = 0.01  # Assuming 100Hz update rate\n\n        # Update state based on angular velocities\n        self.state[3:6] = control_input  # Angular velocities\n        self.state[0:3] += self.state[3:6] * dt  # Integrate to get orientation\n\n        # State transition matrix (simplified)\n        F = np.eye(6)\n        F[0:3, 3:6] = np.eye(3) * dt\n\n        # Update covariance\n        self.P = F @ self.P @ F.T + self.Q\n\n    def update(self, measurement):\n        """Update step of Kalman filter"""\n        # Calculate innovation\n        innovation = measurement - self.H @ self.state\n\n        # Innovation covariance\n        S = self.H @ self.P @ self.H.T + self.R\n\n        # Kalman gain\n        K = self.P @ self.H.T @ np.linalg.inv(S)\n\n        # Update state\n        self.state = self.state + K @ innovation\n\n        # Update covariance\n        I = np.eye(len(self.state))\n        self.P = (I - K @ self.H) @ self.P\n\n    def publish_filtered_state(self):\n        """Publish the filtered state"""\n        # In a real implementation, you would publish the filtered orientation\n        # and other state variables to appropriate topics\n        roll, pitch, yaw = self.state[0:3]\n        self.get_logger().debug(f\'Filtered RPY: [{np.degrees(roll):.2f}, {np.degrees(pitch):.2f}, {np.degrees(yaw):.2f}]\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    kalman_node = KalmanFilterFusion()\n\n    try:\n        rclpy.spin(kalman_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        kalman_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(e.h2,{id:"validation-and-testing",children:"Validation and Testing"}),"\n",(0,r.jsx)(e.h3,{id:"testing-depth-camera-performance",children:"Testing Depth Camera Performance"}),"\n",(0,r.jsx)(e.p,{children:"Create a validation script for your depth camera:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nDepth camera validation script\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass DepthCameraValidator(Node):\n    def __init__(self):\n        super().__init__('depth_camera_validator')\n\n        self.bridge = CvBridge()\n\n        self.depth_sub = self.create_subscription(\n            Image,\n            '/humanoid_robot/depth/image_raw',\n            self.depth_callback,\n            10\n        )\n\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            '/humanoid_robot/rgb/camera_info',\n            self.camera_info_callback,\n            10\n        )\n\n        self.camera_info = None\n        self.frame_count = 0\n        self.timer = self.create_timer(5.0, self.report_status)\n\n        self.get_logger().info('Depth Camera Validator started')\n\n    def depth_callback(self, msg):\n        \"\"\"Validate depth image data\"\"\"\n        try:\n            self.frame_count += 1\n\n            # Convert to numpy array\n            depth_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='passthrough')\n\n            # Validate depth values\n            valid_depths = depth_image[depth_image > 0]\n\n            if len(valid_depths) == 0:\n                self.get_logger().warn('No valid depth values in frame')\n                return\n\n            # Check depth range\n            min_depth = np.min(valid_depths)\n            max_depth = np.max(valid_depths)\n\n            if min_depth < 0.05:  # Too close\n                self.get_logger().warn(f'Depth too close: {min_depth:.3f}m')\n\n            if max_depth > 10.0:  # Beyond expected range\n                self.get_logger().warn(f'Depth beyond expected range: {max_depth:.3f}m')\n\n            # Check for NaN or Inf values\n            if np.any(np.isnan(depth_image)) or np.any(np.isinf(depth_image)):\n                self.get_logger().warn('Depth image contains NaN or Inf values')\n\n            # Validate image dimensions\n            expected_width = 640 if self.camera_info is None else self.camera_info.width\n            expected_height = 480 if self.camera_info is None else self.camera_info.height\n\n            if depth_image.shape[1] != expected_width or depth_image.shape[0] != expected_height:\n                self.get_logger().warn(f'Unexpected image dimensions: {depth_image.shape}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error validating depth image: {e}')\n\n    def camera_info_callback(self, msg):\n        \"\"\"Store camera information\"\"\"\n        self.camera_info = msg\n\n    def report_status(self):\n        \"\"\"Report validation status\"\"\"\n        self.get_logger().info(f'Depth Camera Validator: Processed {self.frame_count} frames')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    validator = DepthCameraValidator()\n\n    try:\n        rclpy.spin(validator)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        validator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(e.h3,{id:"testing-imu-performance",children:"Testing IMU Performance"}),"\n",(0,r.jsx)(e.p,{children:"Create a validation script for your IMU:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nIMU validation script\n\"\"\"\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu\nimport numpy as np\n\nclass IMUValidator(Node):\n    def __init__(self):\n        super().__init__('imu_validator')\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            '/humanoid_robot/imu/data',\n            self.imu_callback,\n            10\n        )\n\n        self.reading_count = 0\n        self.angular_velocity_buffer = []\n        self.linear_acceleration_buffer = []\n        self.timer = self.create_timer(5.0, self.report_status)\n\n        self.get_logger().info('IMU Validator started')\n\n    def imu_callback(self, msg):\n        \"\"\"Validate IMU data\"\"\"\n        try:\n            self.reading_count += 1\n\n            # Extract angular velocity\n            ang_vel = np.array([\n                msg.angular_velocity.x,\n                msg.angular_velocity.y,\n                msg.angular_velocity.z\n            ])\n\n            # Extract linear acceleration\n            lin_acc = np.array([\n                msg.linear_acceleration.x,\n                msg.linear_acceleration.y,\n                msg.linear_acceleration.z\n            ])\n\n            # Validate angular velocity (should be reasonable)\n            ang_vel_mag = np.linalg.norm(ang_vel)\n            if ang_vel_mag > 10.0:  # 10 rad/s is quite fast\n                self.get_logger().warn(f'High angular velocity: {ang_vel_mag:.3f} rad/s')\n\n            # Validate linear acceleration (should be around 9.8 m/s\xb2 when stationary)\n            lin_acc_mag = np.linalg.norm(lin_acc)\n            if lin_acc_mag < 5.0 or lin_acc_mag > 15.0:\n                self.get_logger().warn(f'Unexpected acceleration magnitude: {lin_acc_mag:.3f} m/s\xb2')\n\n            # Validate quaternion normalization\n            quat_norm = np.sqrt(\n                msg.orientation.x**2 +\n                msg.orientation.y**2 +\n                msg.orientation.z**2 +\n                msg.orientation.w**2\n            )\n\n            if abs(quat_norm - 1.0) > 0.01:\n                self.get_logger().warn(f'Quaternion not normalized: {quat_norm:.6f}')\n\n            # Store values for statistics\n            self.angular_velocity_buffer.append(ang_vel_mag)\n            self.linear_acceleration_buffer.append(lin_acc_mag)\n\n            # Keep buffers at reasonable size\n            if len(self.angular_velocity_buffer) > 100:\n                self.angular_velocity_buffer.pop(0)\n            if len(self.linear_acceleration_buffer) > 100:\n                self.linear_acceleration_buffer.pop(0)\n\n        except Exception as e:\n            self.get_logger().error(f'Error validating IMU data: {e}')\n\n    def report_status(self):\n        \"\"\"Report validation statistics\"\"\"\n        if self.angular_velocity_buffer:\n            avg_ang_vel = np.mean(self.angular_velocity_buffer)\n            avg_lin_acc = np.mean(self.linear_acceleration_buffer)\n\n            self.get_logger().info(\n                f'IMU Validator: {self.reading_count} readings | '\n                f'Avg ang vel: {avg_ang_vel:.3f} rad/s | '\n                f'Avg lin acc: {avg_lin_acc:.3f} m/s\xb2'\n            )\n\ndef main(args=None):\n    rclpy.init(args=args)\n    validator = IMUValidator()\n\n    try:\n        rclpy.spin(validator)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        validator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(e.h2,{id:"practical-exercise-complete-sensor-integration",children:"Practical Exercise: Complete Sensor Integration"}),"\n",(0,r.jsx)(e.h3,{id:"step-1-add-all-sensors-to-your-robot-model",children:"Step 1: Add All Sensors to Your Robot Model"}),"\n",(0,r.jsx)(e.p,{children:"Combine all the sensor configurations in your robot's URDF:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Complete sensor integration for humanoid robot --\x3e\n<?xml version="1.0"?>\n<robot name="humanoid_robot_with_sensors">\n\n  \x3c!-- Base link --\x3e\n  <link name="base_link">\n    <visual>\n      <geometry>\n        <box size="0.3 0.2 0.5"/>\n      </geometry>\n      <material name="light_gray">\n        <color rgba="0.7 0.7 0.7 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="0.3 0.2 0.5"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="10.0"/>\n      <inertia ixx="1.0" ixy="0" ixz="0" iyy="1.0" iyz="0" izz="1.0"/>\n    </inertial>\n  </link>\n\n  \x3c!-- IMU at center of mass --\x3e\n  <link name="imu_link">\n    <inertial>\n      <mass value="0.01"/>\n      <inertia ixx="1e-6" ixy="0" ixz="0" iyy="1e-6" iyz="0" izz="1e-6"/>\n    </inertial>\n    <visual>\n      <geometry><box size="0.01 0.01 0.01"/></geometry>\n      <material name="red"><color rgba="1 0 0 1"/></material>\n    </visual>\n  </link>\n  <joint name="imu_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="imu_link"/>\n    <origin xyz="0 0 0.25" rpy="0 0 0"/>\n  </joint>\n\n  \x3c!-- Depth camera on head --\x3e\n  <link name="depth_camera_link">\n    <inertial>\n      <mass value="0.1"/>\n      <inertia ixx="0.0001" ixy="0" ixz="0" iyy="0.0001" iyz="0" izz="0.0001"/>\n    </inertial>\n    <visual>\n      <geometry><box size="0.02 0.04 0.02"/></geometry>\n      <material name="black"><color rgba="0.1 0.1 0.1 1"/></material>\n    </visual>\n  </link>\n  <joint name="depth_camera_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="depth_camera_link"/>\n    <origin xyz="0.05 0 0.45" rpy="0 0 0"/>\n  </joint>\n\n  \x3c!-- LiDAR on head --\x3e\n  <link name="lidar_link">\n    <inertial>\n      <mass value="0.1"/>\n      <inertia ixx="0.0001" ixy="0" ixz="0" iyy="0.0001" iyz="0" izz="0.0002"/>\n    </inertial>\n    <visual>\n      <geometry><cylinder radius="0.025" length="0.05"/></geometry>\n      <material name="gray"><color rgba="0.5 0.5 0.5 1"/></material>\n    </visual>\n  </link>\n  <joint name="lidar_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="lidar_link"/>\n    <origin xyz="0.0 0 0.475" rpy="0 0 0"/>\n  </joint>\n\n  \x3c!-- Include Gazebo plugins --\x3e\n  <gazebo reference="imu_link">\n    <sensor name="imu_sensor" type="imu">\n      <pose>0 0 0 0 0 0</pose>\n      <always_on>true</always_on>\n      <update_rate>100</update_rate>\n      <imu>\n        <angular_velocity><x><noise type="gaussian"><mean>0.0</mean><stddev>0.0017</stddev></noise></x>\n        <y><noise type="gaussian"><mean>0.0</mean><stddev>0.0017</stddev></noise></y>\n        <z><noise type="gaussian"><mean>0.0</mean><stddev>0.0017</stddev></noise></z></angular_velocity>\n        <linear_acceleration><x><noise type="gaussian"><mean>0.0</mean><stddev>0.017</stddev></noise></x>\n        <y><noise type="gaussian"><mean>0.0</mean><stddev>0.017</stddev></noise></y>\n        <z><noise type="gaussian"><mean>0.0</mean><stddev>0.017</stddev></noise></z></linear_acceleration>\n      </imu>\n      <plugin name="imu_controller" filename="libgazebo_ros_imu.so">\n        <ros><namespace>/humanoid_robot</namespace><remapping>~/out:=imu/data</remapping></ros>\n        <frame_name>imu_link</frame_name>\n        <body_name>base_link</body_name>\n        <update_rate>100</update_rate>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  <gazebo reference="depth_camera_link">\n    <sensor name="depth_camera" type="depth">\n      <pose>0 0 0 0 0 0</pose>\n      <visualize>true</visualize>\n      <update_rate>30</update_rate>\n      <camera name="head">\n        <horizontal_fov>1.047</horizontal_fov>\n        <image><width>640</width><height>480</height><format>R8G8B8</format></image>\n        <clip><near>0.1</near><far>10.0</far></clip>\n        <noise><type>gaussian</type><mean>0.0</mean><stddev>0.007</stddev></noise>\n      </camera>\n      <plugin name="depth_camera_controller" filename="libgazebo_ros_openni_kinect.so">\n        <ros>\n          <namespace>/humanoid_robot</namespace>\n          <remapping>rgb/image_raw:=rgb/image_raw</remapping>\n          <remapping>rgb/camera_info:=rgb/camera_info</remapping>\n          <remapping>depth/image_raw:=depth/image_raw</remapping>\n          <remapping>depth/camera_info:=depth/camera_info</remapping>\n        </ros>\n        <frame_name>depth_camera_link</frame_name>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  <gazebo reference="lidar_link">\n    <sensor name="lidar_sensor" type="ray">\n      <pose>0 0 0 0 0 0</pose>\n      <visualize>true</visualize>\n      <update_rate>10</update_rate>\n      <ray>\n        <scan>\n          <horizontal><samples>720</samples><resolution>1</resolution>\n          <min_angle>-3.14159</min_angle><max_angle>3.14159</max_angle></horizontal>\n        </scan>\n        <range><min>0.1</min><max>20.0</max><resolution>0.01</resolution></range>\n        <noise type="gaussian"><mean>0.0</mean><stddev>0.01</stddev></noise>\n      </ray>\n      <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">\n        <ros><namespace>/humanoid_robot</namespace><remapping>~/out:=scan</remapping></ros>\n        <output_type>sensor_msgs/LaserScan</output_type>\n        <frame_name>lidar_link</frame_name>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n</robot>\n'})}),"\n",(0,r.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(e.p,{children:"In this lesson, we completed the sensor suite for your humanoid robot by implementing:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Depth camera simulation"}),": Creating realistic depth images with appropriate noise modeling and processing techniques"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"IMU sensor simulation"}),": Configuring orientation, acceleration, and angular velocity sensing with realistic noise characteristics"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Sensor fusion"}),": Combining depth camera and IMU data for enhanced perception capabilities"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Processing techniques"}),": Implementing ROS 2 nodes to handle and process multiple sensor types"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Validation methods"}),": Testing and validating sensor performance in simulation"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"The comprehensive sensor system we've created provides your humanoid robot with the ability to perceive its environment through multiple modalities: visual information from depth cameras, spatial awareness from LiDAR (from the previous lesson), and motion/orientation data from IMUs. This multi-sensor approach is essential for robust humanoid robotics applications."}),"\n",(0,r.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(e.p,{children:"With the complete physics and sensor simulation system established, you're now ready to move to Module 2 Chapter 3, where we'll integrate Unity's visualization systems. The sensor fusion concepts learned here will be crucial when implementing visualization systems that must accurately represent the sensor data generated in Gazebo."}),"\n",(0,r.jsx)(e.p,{children:"Before proceeding, ensure your sensor integration is:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"All sensors properly integrated into your robot model"}),"\n",(0,r.jsx)(e.li,{children:"Publishing data on correct ROS 2 topics"}),"\n",(0,r.jsx)(e.li,{children:"Generating realistic sensor data with appropriate noise modeling"}),"\n",(0,r.jsx)(e.li,{children:"Performing as expected in various environmental conditions"}),"\n",(0,r.jsx)(e.li,{children:"Sensor fusion algorithms properly combining multiple sensor inputs"}),"\n"]})]})}function c(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(d,{...n})}):d(n)}},8453:(n,e,a)=>{a.d(e,{R:()=>t,x:()=>s});var i=a(6540);const r={},o=i.createContext(r);function t(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:t(n.components),i.createElement(o.Provider,{value:e},n.children)}}}]);