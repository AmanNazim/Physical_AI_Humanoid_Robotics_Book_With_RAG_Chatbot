"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[3810],{8453:(n,e,i)=>{i.d(e,{R:()=>s,x:()=>r});var t=i(6540);const o={},a=t.createContext(o);function s(n){const e=t.useContext(a);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),t.createElement(a.Provider,{value:e},n.children)}},9764:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4/ai-decision-making-and-action-grounding/lesson-2.2-action-grounding-and-motion-planning","title":"Lesson 2.2 \u2013 Action Grounding and Motion Planning","description":"Learning Objectives","source":"@site/docs/module-4/02-ai-decision-making-and-action-grounding/lesson-2.2-action-grounding-and-motion-planning.md","sourceDirName":"module-4/02-ai-decision-making-and-action-grounding","slug":"/module-4/ai-decision-making-and-action-grounding/lesson-2.2-action-grounding-and-motion-planning","permalink":"/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-4/ai-decision-making-and-action-grounding/lesson-2.2-action-grounding-and-motion-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/AmanNazim/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/edit/main/physical-ai-humanoid-robotics-book/docs/module-4/02-ai-decision-making-and-action-grounding/lesson-2.2-action-grounding-and-motion-planning.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 2.1 \u2013 AI Decision-Making Frameworks","permalink":"/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-4/ai-decision-making-and-action-grounding/lesson-2.1-ai-decision-making-frameworks"},"next":{"title":"Lesson 2.3 \u2013 Safety Constraints and Validation Systems","permalink":"/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-4/ai-decision-making-and-action-grounding/lesson-2.3-safety-constraints-and-validation-systems"}}');var o=i(4848),a=i(8453);const s={},r="Lesson 2.2 \u2013 Action Grounding and Motion Planning",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Action Grounding",id:"introduction-to-action-grounding",level:2},{value:"Understanding Action Grounding Systems",id:"understanding-action-grounding-systems",level:2},{value:"The Action Grounding Pipeline",id:"the-action-grounding-pipeline",level:3},{value:"Symbol Grounding in Action Systems",id:"symbol-grounding-in-action-systems",level:3},{value:"Action Representation and Planning",id:"action-representation-and-planning",level:3},{value:"Motion Planning for Humanoid Robots",id:"motion-planning-for-humanoid-robots",level:2},{value:"Humanoid-Specific Motion Planning Challenges",id:"humanoid-specific-motion-planning-challenges",level:3},{value:"Motion Planning Algorithms",id:"motion-planning-algorithms",level:3},{value:"Implementation Example: Humanoid Motion Planner",id:"implementation-example-humanoid-motion-planner",level:3},{value:"Action Grounding Integration with AI Decision-Making",id:"action-grounding-integration-with-ai-decision-making",level:2},{value:"Connecting Decisions to Actions",id:"connecting-decisions-to-actions",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"ROS 2 Motion Planning Interfaces",id:"ros-2-motion-planning-interfaces",level:3},{value:"Practical Considerations",id:"practical-considerations",level:2},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Safety and Validation",id:"safety-and-validation",level:3},{value:"Summary",id:"summary",level:2}];function p(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"lesson-22--action-grounding-and-motion-planning",children:"Lesson 2.2 \u2013 Action Grounding and Motion Planning"})}),"\n",(0,o.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(e.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Implement action grounding systems that connect AI decisions to physical movements"}),"\n",(0,o.jsx)(e.li,{children:"Configure motion planning algorithms for humanoid robots"}),"\n",(0,o.jsx)(e.li,{children:"Translate high-level goals into specific motor commands"}),"\n",(0,o.jsx)(e.li,{children:"Understand how to use motion planning libraries, trajectory generation tools, and ROS 2 interfaces"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"introduction-to-action-grounding",children:"Introduction to Action Grounding"}),"\n",(0,o.jsx)(e.p,{children:"Action grounding is the critical process that connects abstract AI decisions to concrete physical movements in humanoid robots. While the previous lesson focused on how AI systems make decisions based on multimodal inputs, this lesson addresses how those decisions are translated into specific motor commands that result in purposeful robot behavior."}),"\n",(0,o.jsx)(e.p,{children:"Action grounding involves several key challenges:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Semantic Gap Bridging"}),': Converting high-level goals ("pick up the red cup") into low-level motor commands']}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Physical Constraints"}),": Ensuring actions are feasible given robot kinematics and environmental constraints"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Temporal Coordination"}),": Sequencing actions in time to achieve complex behaviors"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Safety Integration"}),": Maintaining safety throughout the grounding process"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"understanding-action-grounding-systems",children:"Understanding Action Grounding Systems"}),"\n",(0,o.jsx)(e.h3,{id:"the-action-grounding-pipeline",children:"The Action Grounding Pipeline"}),"\n",(0,o.jsx)(e.p,{children:"Action grounding systems follow a structured pipeline that transforms abstract decisions into executable actions:"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"High-Level Goal Specification"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Natural language instructions or task descriptions"}),"\n",(0,o.jsx)(e.li,{children:"Environmental context and constraints"}),"\n",(0,o.jsx)(e.li,{children:"Success criteria and safety requirements"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Task Decomposition"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Breaking complex goals into simpler subtasks"}),"\n",(0,o.jsx)(e.li,{children:"Identifying required skills and capabilities"}),"\n",(0,o.jsx)(e.li,{children:"Sequencing subtasks in appropriate order"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Motion Planning"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Generating specific movement trajectories"}),"\n",(0,o.jsx)(e.li,{children:"Accounting for robot kinematics and dynamics"}),"\n",(0,o.jsx)(e.li,{children:"Planning collision-free paths"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Motor Command Generation"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Converting trajectories to joint commands"}),"\n",(0,o.jsx)(e.li,{children:"Controlling actuators and effectors"}),"\n",(0,o.jsx)(e.li,{children:"Executing coordinated movements"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"symbol-grounding-in-action-systems",children:"Symbol Grounding in Action Systems"}),"\n",(0,o.jsx)(e.p,{children:"Symbol grounding extends beyond connecting language to visual objects to connecting language to physical actions. This creates a mapping between abstract concepts and concrete behaviors:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"class ActionSymbolGrounding:\n    def __init__(self):\n        self.action_mappings = {\n            'grasp': ['close_gripper', 'approach_object', 'lift'],\n            'transport': ['navigate', 'carry_object', 'position'],\n            'place': ['align_object', 'release', 'withdraw']\n        }\n        self.object_action_mappings = {\n            'cup': ['grasp', 'lift', 'carry'],\n            'box': ['push', 'slide', 'reposition'],\n            'tool': ['grasp', 'manipulate', 'use']\n        }\n\n    def ground_action_to_motor_commands(self, action_verb, target_object):\n        \"\"\"Map abstract action to specific motor commands\"\"\"\n        # Get base action sequence\n        if action_verb in self.action_mappings:\n            base_actions = self.action_mappings[action_verb]\n        else:\n            return self.get_default_action_sequence(action_verb)\n\n        # Adapt to specific object properties\n        if target_object and target_object['type'] in self.object_action_mappings:\n            object_specific_actions = self.object_action_mappings[target_object['type']]\n            return self.adapt_actions_to_object(base_actions, object_specific_actions, target_object)\n\n        return base_actions\n\n    def adapt_actions_to_object(self, base_actions, object_actions, target_object):\n        \"\"\"Adapt general actions to specific object properties\"\"\"\n        adapted_actions = []\n\n        for action in base_actions:\n            if action in object_actions:\n                # Adjust parameters based on object properties\n                adjusted_action = self.adjust_action_for_object(action, target_object)\n                adapted_actions.append(adjusted_action)\n            else:\n                adapted_actions.append(action)\n\n        return adapted_actions\n"})}),"\n",(0,o.jsx)(e.h3,{id:"action-representation-and-planning",children:"Action Representation and Planning"}),"\n",(0,o.jsx)(e.p,{children:"Effective action grounding requires proper representation of both the action space and the environmental constraints:"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Action Space Representation"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Joint space: Direct control of robot joint angles"}),"\n",(0,o.jsx)(e.li,{children:"Cartesian space: Control of end-effector position and orientation"}),"\n",(0,o.jsx)(e.li,{children:"Task space: High-level task specifications"}),"\n",(0,o.jsx)(e.li,{children:"Skill space: Pre-learned movement patterns"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Constraint Integration"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Kinematic constraints (joint limits, reachability)"}),"\n",(0,o.jsx)(e.li,{children:"Dynamic constraints (velocity, acceleration limits)"}),"\n",(0,o.jsx)(e.li,{children:"Environmental constraints (obstacles, workspace boundaries)"}),"\n",(0,o.jsx)(e.li,{children:"Safety constraints (collision avoidance, force limits)"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"motion-planning-for-humanoid-robots",children:"Motion Planning for Humanoid Robots"}),"\n",(0,o.jsx)(e.h3,{id:"humanoid-specific-motion-planning-challenges",children:"Humanoid-Specific Motion Planning Challenges"}),"\n",(0,o.jsx)(e.p,{children:"Humanoid robots present unique challenges for motion planning due to their complex kinematic structure and the need for stable, human-like movement:"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Balance and Stability"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Maintaining center of mass within support polygon"}),"\n",(0,o.jsx)(e.li,{children:"Coordinating upper and lower body movements"}),"\n",(0,o.jsx)(e.li,{children:"Managing zero-moment point (ZMP) for stable locomotion"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Multi-Limb Coordination"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Synchronizing arm and leg movements"}),"\n",(0,o.jsx)(e.li,{children:"Avoiding self-collisions between limbs"}),"\n",(0,o.jsx)(e.li,{children:"Managing redundant degrees of freedom"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Human-like Movement Patterns"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Generating natural-looking motion trajectories"}),"\n",(0,o.jsx)(e.li,{children:"Maintaining anthropomorphic movement characteristics"}),"\n",(0,o.jsx)(e.li,{children:"Adapting to different walking gaits and postures"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"motion-planning-algorithms",children:"Motion Planning Algorithms"}),"\n",(0,o.jsx)(e.p,{children:"Several motion planning algorithms are particularly suited for humanoid robots:"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"RRT (Rapidly-exploring Random Trees)"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Effective for high-dimensional configuration spaces"}),"\n",(0,o.jsx)(e.li,{children:"Good for finding collision-free paths in complex environments"}),"\n",(0,o.jsx)(e.li,{children:"Can handle kinematic constraints and joint limits"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Trajectory Optimization"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Generates smooth, time-parameterized trajectories"}),"\n",(0,o.jsx)(e.li,{children:"Can optimize for multiple objectives (speed, energy, safety)"}),"\n",(0,o.jsx)(e.li,{children:"Incorporates dynamic constraints and stability requirements"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Sampling-Based Methods"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Efficient for complex humanoid kinematics"}),"\n",(0,o.jsx)(e.li,{children:"Can handle non-holonomic constraints"}),"\n",(0,o.jsx)(e.li,{children:"Suitable for real-time applications with proper optimization"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"implementation-example-humanoid-motion-planner",children:"Implementation Example: Humanoid Motion Planner"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import numpy as np\nfrom scipy.interpolate import interp1d\nfrom typing import List, Dict, Tuple, Optional\n\nclass HumanoidMotionPlanner:\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n        self.kinematics_solver = self._initialize_kinematics_solver()\n        self.collision_checker = self._initialize_collision_checker()\n        self.stability_checker = self._initialize_stability_checker()\n\n    def _initialize_kinematics_solver(self):\n        """Initialize inverse kinematics solver for humanoid robot"""\n        return {\n            \'arm_ik_solver\': self._setup_arm_ik_solver(),\n            \'leg_ik_solver\': self._setup_leg_ik_solver(),\n            \'whole_body_solver\': self._setup_whole_body_solver()\n        }\n\n    def _initialize_collision_checker(self):\n        """Initialize collision detection system"""\n        return {\n            \'self_collision_threshold\': 0.05,  # meters\n            \'environment_collision_threshold\': 0.1  # meters\n        }\n\n    def _initialize_stability_checker(self):\n        """Initialize stability verification system"""\n        return {\n            \'zmp_limits\': {\'x\': (-0.1, 0.1), \'y\': (-0.05, 0.05)},  # meters\n            \'com_support_threshold\': 0.05  # meters from support polygon edge\n        }\n\n    def plan_arm_motion(self, start_pose, goal_pose, obstacles=None):\n        """\n        Plan motion for humanoid arm to reach goal pose\n        """\n        # Check if goal is reachable\n        if not self._is_reachable(goal_pose, \'arm\'):\n            raise ValueError("Goal pose is not reachable")\n\n        # Generate initial trajectory using inverse kinematics\n        initial_trajectory = self._generate_ik_trajectory(start_pose, goal_pose)\n\n        # Optimize trajectory to avoid obstacles\n        if obstacles:\n            optimized_trajectory = self._optimize_trajectory_for_obstacles(\n                initial_trajectory, obstacles\n            )\n        else:\n            optimized_trajectory = initial_trajectory\n\n        # Verify joint limits and smooth trajectory\n        final_trajectory = self._verify_and_smooth_trajectory(optimized_trajectory)\n\n        return final_trajectory\n\n    def plan_walking_trajectory(self, start_pose, goal_position, step_height=0.05):\n        """\n        Plan walking trajectory for humanoid to reach goal position\n        """\n        # Calculate number of steps needed\n        distance = np.linalg.norm(np.array(goal_position[:2]) - np.array(start_pose[:2]))\n        step_length = 0.3  # meters\n        num_steps = int(np.ceil(distance / step_length))\n\n        # Generate step locations\n        step_positions = self._generate_step_positions(\n            start_pose, goal_position, num_steps\n        )\n\n        # Generate complete walking trajectory\n        walking_trajectory = self._generate_walking_trajectory(\n            step_positions, step_height\n        )\n\n        # Verify stability throughout trajectory\n        if not self._verify_stability(walking_trajectory):\n            raise ValueError("Walking trajectory is not stable")\n\n        return walking_trajectory\n\n    def _is_reachable(self, pose, body_part):\n        """Check if pose is reachable by specified body part"""\n        # Calculate workspace bounds for body part\n        workspace_bounds = self.robot_model.get_workspace_bounds(body_part)\n\n        # Check if pose is within workspace\n        position = np.array(pose[:3])\n        is_in_workspace = (\n            workspace_bounds[\'min\'][0] <= position[0] <= workspace_bounds[\'max\'][0] and\n            workspace_bounds[\'min\'][1] <= position[1] <= workspace_bounds[\'max\'][1] and\n            workspace_bounds[\'min\'][2] <= position[2] <= workspace_bounds[\'max\'][2]\n        )\n\n        return is_in_workspace\n\n    def _generate_ik_trajectory(self, start_pose, goal_pose):\n        """Generate trajectory using inverse kinematics"""\n        # Interpolate between start and goal poses\n        num_waypoints = 50\n        t_values = np.linspace(0, 1, num_waypoints)\n\n        trajectory = []\n        for t in t_values:\n            # Linear interpolation in Cartesian space\n            current_pose = self._interpolate_poses(start_pose, goal_pose, t)\n\n            # Solve inverse kinematics\n            joint_angles = self._solve_inverse_kinematics(current_pose)\n\n            trajectory.append({\n                \'time\': t * 2.0,  # Assume 2 seconds for the motion\n                \'joint_angles\': joint_angles,\n                \'cartesian_pose\': current_pose\n            })\n\n        return trajectory\n\n    def _interpolate_poses(self, start_pose, goal_pose, t):\n        """Interpolate between two poses"""\n        start_pos = np.array(start_pose[:3])\n        goal_pos = np.array(goal_pose[:3])\n\n        # Linear interpolation for position\n        current_pos = start_pos + t * (goal_pos - start_pos)\n\n        # Slerp for orientation (simplified as linear interpolation)\n        start_quat = np.array(start_pose[3:])\n        goal_quat = np.array(goal_pose[3:])\n        current_quat = start_quat + t * (goal_quat - start_quat)\n        current_quat = current_quat / np.linalg.norm(current_quat)  # Normalize\n\n        return list(current_pos) + list(current_quat)\n\n    def _solve_inverse_kinematics(self, pose):\n        """Solve inverse kinematics for given pose"""\n        # In practice, this would use a proper IK solver\n        # For this example, we\'ll simulate the process\n        target_position = np.array(pose[:3])\n\n        # Calculate joint angles (simplified calculation)\n        joint_angles = self._calculate_joint_angles_from_position(target_position)\n\n        return joint_angles\n\n    def _calculate_joint_angles_from_position(self, position):\n        """Calculate joint angles to reach given position (simplified)"""\n        # This is a simplified calculation - real IK would be more complex\n        # For a 6-DOF arm, we might calculate angles based on position\n        x, y, z = position\n\n        # Simplified inverse kinematics (for demonstration)\n        # In practice, use proper IK libraries like KDL, MoveIt, etc.\n        joint_angles = [\n            np.arctan2(y, x),  # Shoulder pan\n            np.arctan2(z, np.sqrt(x**2 + y**2)),  # Shoulder lift\n            0.0,  # Elbow\n            0.0,  # Wrist 1\n            0.0,  # Wrist 2\n            0.0   # Wrist 3\n        ]\n\n        return joint_angles\n\n    def _optimize_trajectory_for_obstacles(self, trajectory, obstacles):\n        """Optimize trajectory to avoid obstacles"""\n        # Use trajectory optimization to avoid obstacles\n        optimized_trajectory = []\n\n        for waypoint in trajectory:\n            # Check for collisions\n            collision_free = True\n            for obstacle in obstacles:\n                if self._check_collision(waypoint, obstacle):\n                    collision_free = False\n                    break\n\n            if collision_free:\n                optimized_trajectory.append(waypoint)\n            else:\n                # Find alternative path around obstacle\n                alternative_waypoint = self._find_alternative_waypoint(waypoint, obstacles)\n                optimized_trajectory.append(alternative_waypoint)\n\n        return optimized_trajectory\n\n    def _check_collision(self, waypoint, obstacle):\n        """Check if waypoint causes collision with obstacle"""\n        # Calculate robot position at waypoint\n        robot_pos = self._calculate_robot_position(waypoint[\'joint_angles\'])\n\n        # Check distance to obstacle\n        obstacle_pos = obstacle[\'position\']\n        distance = np.linalg.norm(np.array(robot_pos) - np.array(obstacle_pos))\n\n        return distance < self.collision_checker[\'environment_collision_threshold\']\n\n    def _find_alternative_waypoint(self, waypoint, obstacles):\n        """Find alternative waypoint to avoid obstacles"""\n        # This would implement a path planning algorithm\n        # For simplicity, we\'ll just offset the waypoint\n        original_pos = np.array(waypoint[\'cartesian_pose\'][:3])\n\n        # Try different offsets to find collision-free position\n        for offset in [0.1, -0.1, 0.2, -0.2]:\n            for axis in [0, 1, 2]:  # x, y, z\n                offset_pos = original_pos.copy()\n                offset_pos[axis] += offset\n\n                test_pose = list(offset_pos) + waypoint[\'cartesian_pose\'][3:]\n\n                # Check if this offset creates a collision-free path\n                collision_free = True\n                for obstacle in obstacles:\n                    if self._check_collision({\'cartesian_pose\': test_pose}, obstacle):\n                        collision_free = False\n                        break\n\n                if collision_free:\n                    # Update joint angles for new pose\n                    new_joint_angles = self._solve_inverse_kinematics(test_pose)\n                    return {\n                        \'time\': waypoint[\'time\'],\n                        \'joint_angles\': new_joint_angles,\n                        \'cartesian_pose\': test_pose\n                    }\n\n        # If no collision-free alternative found, return original (will cause error later)\n        return waypoint\n\n    def _verify_and_smooth_trajectory(self, trajectory):\n        """Verify joint limits and smooth trajectory"""\n        verified_trajectory = []\n\n        for i, waypoint in enumerate(trajectory):\n            # Check joint limits\n            joint_angles = np.array(waypoint[\'joint_angles\'])\n\n            # Apply joint limits (example values)\n            joint_limits_min = np.array([-2.0, -1.5, -2.0, -2.0, -2.0, -2.0])\n            joint_limits_max = np.array([2.0, 1.5, 2.0, 2.0, 2.0, 2.0])\n\n            # Clamp to joint limits\n            clamped_angles = np.clip(joint_angles, joint_limits_min, joint_limits_max)\n\n            # Smooth with previous waypoint to avoid large jumps\n            if i > 0:\n                prev_angles = np.array(verified_trajectory[-1][\'joint_angles\'])\n                max_change = 0.1  # radians per step\n\n                # Limit joint angle changes\n                angle_diff = clamped_angles - prev_angles\n                limited_diff = np.clip(angle_diff, -max_change, max_change)\n                clamped_angles = prev_angles + limited_diff\n\n            waypoint[\'joint_angles\'] = clamped_angles.tolist()\n            verified_trajectory.append(waypoint)\n\n        return verified_trajectory\n\n    def _generate_step_positions(self, start_pose, goal_position, num_steps):\n        """Generate positions for each walking step"""\n        # Calculate step positions along straight line\n        start_pos = np.array(start_pose[:2])\n        goal_pos = np.array(goal_position[:2])\n\n        step_positions = []\n        for i in range(num_steps + 1):\n            t = i / num_steps\n            step_pos = start_pos + t * (goal_pos - start_pos)\n            step_positions.append(step_pos.tolist())\n\n        return step_positions\n\n    def _generate_walking_trajectory(self, step_positions, step_height):\n        """Generate complete walking trajectory"""\n        walking_trajectory = []\n\n        for i, pos in enumerate(step_positions):\n            # Add step with appropriate height\n            z_offset = step_height if i % 2 == 0 else 0.0  # Alternate step height\n\n            # Create trajectory point\n            trajectory_point = {\n                \'time\': i * 0.8,  # 0.8 seconds per step\n                \'position\': [pos[0], pos[1], z_offset],\n                \'support_leg\': \'left\' if i % 2 == 0 else \'right\'\n            }\n\n            walking_trajectory.append(trajectory_point)\n\n        return walking_trajectory\n\n    def _verify_stability(self, trajectory):\n        """Verify that walking trajectory is stable"""\n        for point in trajectory:\n            # Check ZMP (Zero Moment Point) constraints\n            zmp_x, zmp_y = self._calculate_zmp(point)\n\n            x_min, x_max = self.stability_checker[\'zmp_limits\'][\'x\']\n            y_min, y_max = self.stability_checker[\'zmp_limits\'][\'y\']\n\n            if not (x_min <= zmp_x <= x_max and y_min <= zmp_y <= y_max):\n                return False\n\n        return True\n\n    def _calculate_zmp(self, trajectory_point):\n        """Calculate Zero Moment Point for trajectory point"""\n        # Simplified ZMP calculation\n        # In practice, this would use full dynamics model\n        pos = trajectory_point[\'position\']\n\n        # Approximate ZMP as center of support polygon\n        # For single support, this is roughly the foot position\n        zmp_x = pos[0]\n        zmp_y = pos[1]\n\n        return zmp_x, zmp_y\n\n# Example usage\ndef main():\n    # Mock robot model\n    class MockRobotModel:\n        def get_workspace_bounds(self, body_part):\n            if body_part == \'arm\':\n                return {\n                    \'min\': [-0.5, -0.5, 0.0],\n                    \'max\': [0.5, 0.5, 1.0]\n                }\n            return {\'min\': [0, 0, 0], \'max\': [1, 1, 1]}\n\n    # Initialize motion planner\n    robot_model = MockRobotModel()\n    planner = HumanoidMotionPlanner(robot_model)\n\n    # Example: Plan arm motion\n    start_pose = [0.3, 0.0, 0.8, 0, 0, 0, 1]  # [x, y, z, qx, qy, qz, qw]\n    goal_pose = [0.4, 0.2, 0.6, 0, 0, 0, 1]\n\n    obstacles = [\n        {\'position\': [0.35, 0.1, 0.7], \'size\': [0.1, 0.1, 0.2]}\n    ]\n\n    try:\n        trajectory = planner.plan_arm_motion(start_pose, goal_pose, obstacles)\n        print(f"Generated trajectory with {len(trajectory)} waypoints")\n        print("First waypoint:", trajectory[0])\n        print("Last waypoint:", trajectory[-1])\n    except ValueError as e:\n        print(f"Motion planning failed: {e}")\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,o.jsx)(e.h2,{id:"action-grounding-integration-with-ai-decision-making",children:"Action Grounding Integration with AI Decision-Making"}),"\n",(0,o.jsx)(e.h3,{id:"connecting-decisions-to-actions",children:"Connecting Decisions to Actions"}),"\n",(0,o.jsx)(e.p,{children:"Action grounding systems must seamlessly integrate with the AI decision-making frameworks from Lesson 2.1:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"class IntegratedDecisionToAction:\n    def __init__(self):\n        self.motion_planner = HumanoidMotionPlanner(robot_model=None)\n        self.action_grounding = ActionSymbolGrounding()\n        self.decision_framework = SimpleVLAReasoningEngine()  # From Lesson 2.1\n\n    def process_decision_and_execute(self, visual_data, language_input):\n        \"\"\"Process decision and convert to executable actions\"\"\"\n        # Step 1: Make decision using AI framework\n        decision = self.decision_framework.process_input(visual_data, language_input)\n\n        if decision['decision_type'] == 'safety_error':\n            return self._handle_safety_error(decision)\n\n        # Step 2: Ground decision to physical actions\n        grounded_actions = self._ground_decision_to_actions(\n            decision['action_plan'],\n            decision\n        )\n\n        # Step 3: Plan detailed motions for each action\n        executable_trajectory = self._plan_detailed_trajectories(grounded_actions)\n\n        # Step 4: Validate and execute\n        if self._validate_trajectory(executable_trajectory):\n            return self._execute_trajectory(executable_trajectory)\n        else:\n            return self._handle_validation_failure(executable_trajectory)\n\n    def _ground_decision_to_actions(self, action_plan, decision_context):\n        \"\"\"Ground high-level action plan to detailed physical actions\"\"\"\n        grounded_actions = []\n\n        for action in action_plan:\n            if action['action'] == 'move_to_object':\n                # Get target object position from context\n                target_obj = decision_context.get('matched_object')\n                if target_obj:\n                    detailed_action = {\n                        'type': 'navigation',\n                        'target_position': target_obj['position'],\n                        'approach_distance': 0.3,\n                        'motion_parameters': self._get_navigation_parameters(target_obj)\n                    }\n                    grounded_actions.append(detailed_action)\n\n            elif action['action'] == 'grasp_object':\n                target_obj = decision_context.get('matched_object')\n                if target_obj:\n                    detailed_action = {\n                        'type': 'manipulation',\n                        'target_object': target_obj,\n                        'grasp_type': 'top_grasp',\n                        'motion_parameters': self._get_manipulation_parameters(target_obj)\n                    }\n                    grounded_actions.append(detailed_action)\n\n        return grounded_actions\n\n    def _plan_detailed_trajectories(self, grounded_actions):\n        \"\"\"Plan detailed motion trajectories for grounded actions\"\"\"\n        trajectory_sequence = []\n\n        for action in grounded_actions:\n            if action['type'] == 'navigation':\n                nav_trajectory = self.motion_planner.plan_walking_trajectory(\n                    start_pose=[0, 0, 0, 0, 0, 0, 1],\n                    goal_position=action['target_position'][:2] + [0]\n                )\n                trajectory_sequence.extend(nav_trajectory)\n\n            elif action['type'] == 'manipulation':\n                # Plan arm motion to grasp object\n                obj_pos = action['target_object']['position']\n                grasp_pose = self._calculate_grasp_pose(obj_pos, action['grasp_type'])\n\n                arm_trajectory = self.motion_planner.plan_arm_motion(\n                    start_pose=[0.3, 0.0, 0.8, 0, 0, 0, 1],\n                    goal_pose=grasp_pose\n                )\n                trajectory_sequence.extend(arm_trajectory)\n\n        return trajectory_sequence\n\n    def _get_navigation_parameters(self, target_object):\n        \"\"\"Get navigation parameters for target object\"\"\"\n        return {\n            'step_height': 0.05,\n            'step_length': 0.3,\n            'orientation_tolerance': 0.1\n        }\n\n    def _get_manipulation_parameters(self, target_object):\n        \"\"\"Get manipulation parameters for target object\"\"\"\n        return {\n            'approach_distance': 0.1,\n            'grasp_width': target_object.get('size', [0.1, 0.1, 0.1])[0] * 1.2,\n            'grasp_force': 10.0  # Newtons\n        }\n\n    def _calculate_grasp_pose(self, object_position, grasp_type):\n        \"\"\"Calculate appropriate grasp pose for object\"\"\"\n        if grasp_type == 'top_grasp':\n            # Position above object, approach from top\n            grasp_pos = [\n                object_position[0],\n                object_position[1],\n                object_position[2] + 0.1  # 10cm above object\n            ]\n        else:\n            # Side grasp\n            grasp_pos = [\n                object_position[0] + 0.1,  # 10cm in front\n                object_position[1],\n                object_position[2] + object_position[2] / 2  # Mid-height\n            ]\n\n        # Default orientation (looking down for top grasp)\n        grasp_orientation = [0, 0, 0, 1]  # Identity quaternion\n\n        return grasp_pos + grasp_orientation\n\n    def _validate_trajectory(self, trajectory):\n        \"\"\"Validate trajectory for safety and feasibility\"\"\"\n        # Check for collisions\n        for point in trajectory:\n            if 'collision_risk' in point and point['collision_risk'] > 0.1:\n                return False\n\n        # Check for joint limits\n        for point in trajectory:\n            if 'joint_angles' in point:\n                for angle in point['joint_angles']:\n                    if abs(angle) > 3.0:  # Beyond reasonable joint limits\n                        return False\n\n        return True\n\n    def _execute_trajectory(self, trajectory):\n        \"\"\"Execute validated trajectory\"\"\"\n        # In practice, this would send commands to robot controllers\n        execution_result = {\n            'status': 'success',\n            'trajectory_executed': len(trajectory),\n            'execution_time': len(trajectory) * 0.1  # 0.1s per waypoint\n        }\n        return execution_result\n\n    def _handle_safety_error(self, decision):\n        \"\"\"Handle safety errors in decision making\"\"\"\n        return {\n            'status': 'safety_error',\n            'message': 'Decision contains safety violations',\n            'suggested_action': 'Request human intervention'\n        }\n\n    def _handle_validation_failure(self, trajectory):\n        \"\"\"Handle trajectory validation failure\"\"\"\n        return {\n            'status': 'validation_failed',\n            'message': 'Trajectory validation failed',\n            'suggested_action': 'Plan alternative trajectory'\n        }\n"})}),"\n",(0,o.jsx)(e.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,o.jsx)(e.h3,{id:"ros-2-motion-planning-interfaces",children:"ROS 2 Motion Planning Interfaces"}),"\n",(0,o.jsx)(e.p,{children:"ROS 2 provides standardized interfaces for motion planning and action execution:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom builtin_interfaces.msg import Duration\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\nfrom control_msgs.action import FollowJointTrajectory\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import String\nimport time\n\nclass HumanoidMotionPlannerNode(Node):\n    def __init__(self):\n        super().__init__('humanoid_motion_planner_node')\n\n        # Publishers and subscribers\n        self.trajectory_pub = self.create_publisher(\n            JointTrajectory,\n            '/joint_trajectory_controller/joint_trajectory',\n            10\n        )\n\n        self.pose_sub = self.create_subscription(\n            PoseStamped,\n            '/target_pose',\n            self.target_pose_callback,\n            10\n        )\n\n        self.command_sub = self.create_subscription(\n            String,\n            '/motion_commands',\n            self.command_callback,\n            10\n        )\n\n        # Action clients\n        self.trajectory_client = self.create_client(\n            FollowJointTrajectory,\n            '/joint_trajectory_controller/follow_joint_trajectory'\n        )\n\n        # Initialize motion planner\n        self.motion_planner = HumanoidMotionPlanner(robot_model=None)\n        self.current_joint_names = [\n            'left_hip_joint', 'left_knee_joint', 'left_ankle_joint',\n            'right_hip_joint', 'right_knee_joint', 'right_ankle_joint',\n            'left_shoulder_joint', 'left_elbow_joint', 'left_wrist_joint',\n            'right_shoulder_joint', 'right_elbow_joint', 'right_wrist_joint'\n        ]\n\n    def target_pose_callback(self, msg):\n        \"\"\"Handle target pose requests\"\"\"\n        target_pose = [\n            msg.pose.position.x,\n            msg.pose.position.y,\n            msg.pose.position.z,\n            msg.pose.orientation.x,\n            msg.pose.orientation.y,\n            msg.pose.orientation.z,\n            msg.pose.orientation.w\n        ]\n\n        # Plan motion to target pose\n        try:\n            trajectory = self.motion_planner.plan_arm_motion(\n                start_pose=self.get_current_pose(),\n                goal_pose=target_pose\n            )\n\n            # Convert to ROS 2 trajectory message\n            ros_trajectory = self._convert_to_ros_trajectory(trajectory)\n\n            # Publish trajectory\n            self.trajectory_pub.publish(ros_trajectory)\n\n        except Exception as e:\n            self.get_logger().error(f'Motion planning failed: {e}')\n\n    def command_callback(self, msg):\n        \"\"\"Handle motion commands\"\"\"\n        command = msg.data\n\n        if command.startswith('move_to:'):\n            # Parse target position\n            try:\n                parts = command.split(':')[1].split(',')\n                target_pos = [float(x.strip()) for x in parts[:3]]\n\n                # Plan walking trajectory\n                walking_trajectory = self.motion_planner.plan_walking_trajectory(\n                    start_pose=self.get_current_pose(),\n                    goal_position=target_pos\n                )\n\n                # Execute walking trajectory\n                self._execute_walking_trajectory(walking_trajectory)\n\n            except ValueError:\n                self.get_logger().error('Invalid move_to command format')\n\n    def _convert_to_ros_trajectory(self, trajectory):\n        \"\"\"Convert internal trajectory to ROS 2 JointTrajectory message\"\"\"\n        msg = JointTrajectory()\n        msg.joint_names = self.current_joint_names\n\n        for point in trajectory:\n            ros_point = JointTrajectoryPoint()\n            ros_point.positions = point['joint_angles']\n            ros_point.time_from_start = Duration(sec=int(point['time']), nanosec=0)\n\n            # Add velocity and acceleration if available\n            if 'velocities' in point:\n                ros_point.velocities = point['velocities']\n            if 'accelerations' in point:\n                ros_point.accelerations = point['accelerations']\n\n            msg.points.append(ros_point)\n\n        return msg\n\n    def _execute_walking_trajectory(self, walking_trajectory):\n        \"\"\"Execute walking trajectory on humanoid robot\"\"\"\n        for step in walking_trajectory:\n            # Move to step position\n            self._move_to_position(step['position'], step['time'])\n\n            # Wait for step completion\n            time.sleep(0.1)  # Simulated wait\n\n    def get_current_pose(self):\n        \"\"\"Get current robot pose (placeholder implementation)\"\"\"\n        # In practice, this would get current pose from robot state\n        return [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n\n# Example usage function\ndef main():\n    rclpy.init()\n    node = HumanoidMotionPlannerNode()\n\n    # Spin the node\n    rclpy.spin(node)\n\n    # Cleanup\n    node.destroy_node()\n    rclpy.shutdown()\n"})}),"\n",(0,o.jsx)(e.h2,{id:"practical-considerations",children:"Practical Considerations"}),"\n",(0,o.jsx)(e.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,o.jsx)(e.p,{children:"Motion planning for humanoid robots requires careful attention to computational efficiency:"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Real-Time Constraints"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Motion planning must complete within robot control loop timing"}),"\n",(0,o.jsx)(e.li,{children:"Use sampling-based methods for complex scenarios"}),"\n",(0,o.jsx)(e.li,{children:"Implement hierarchical planning for efficiency"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Memory Management"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Efficient data structures for trajectory storage"}),"\n",(0,o.jsx)(e.li,{children:"Streaming of trajectory data to controllers"}),"\n",(0,o.jsx)(e.li,{children:"Proper cleanup of intermediate planning data"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Hardware Acceleration"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Utilize GPU acceleration for complex kinematic calculations"}),"\n",(0,o.jsx)(e.li,{children:"Implement parallel processing where possible"}),"\n",(0,o.jsx)(e.li,{children:"Optimize for specific robot hardware capabilities"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"safety-and-validation",children:"Safety and Validation"}),"\n",(0,o.jsx)(e.p,{children:"Safety remains paramount in action grounding systems:"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Pre-Execution Validation"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Verify all trajectories for collision-free paths"}),"\n",(0,o.jsx)(e.li,{children:"Check joint limits and velocity constraints"}),"\n",(0,o.jsx)(e.li,{children:"Validate stability throughout motion sequences"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Runtime Monitoring"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Monitor execution for deviations from planned trajectory"}),"\n",(0,o.jsx)(e.li,{children:"Implement emergency stops for unexpected conditions"}),"\n",(0,o.jsx)(e.li,{children:"Continuously validate safety constraints during execution"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Fallback Mechanisms"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Maintain safe home positions for all limbs"}),"\n",(0,o.jsx)(e.li,{children:"Implement graceful degradation for failed motions"}),"\n",(0,o.jsx)(e.li,{children:"Provide manual override capabilities"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"In this lesson, you've learned about action grounding and motion planning for humanoid robots, including:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"The critical role of action grounding in connecting AI decisions to physical movements"}),"\n",(0,o.jsx)(e.li,{children:"The action grounding pipeline from high-level goals to motor commands"}),"\n",(0,o.jsx)(e.li,{children:"Symbol grounding techniques for connecting language to actions"}),"\n",(0,o.jsx)(e.li,{children:"Motion planning algorithms specifically designed for humanoid robots"}),"\n",(0,o.jsx)(e.li,{children:"Implementation of humanoid-specific motion planning challenges"}),"\n",(0,o.jsx)(e.li,{children:"Integration of action grounding with AI decision-making systems"}),"\n",(0,o.jsx)(e.li,{children:"ROS 2 interfaces for motion planning and execution"}),"\n",(0,o.jsx)(e.li,{children:"Safety considerations and validation requirements"}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"Action grounding systems form the essential bridge between cognitive decision-making and physical robot behavior. In the next lesson, you'll learn how to implement comprehensive safety constraints and validation systems to ensure these AI-driven behaviors operate safely in human environments."})]})}function d(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(p,{...n})}):p(n)}}}]);