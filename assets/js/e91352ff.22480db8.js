"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[9304],{7876:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>_,frontMatter:()=>r,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"module-3/Cognitive-Architectures/lesson-3.3-ai-decision-making-and-action-planning","title":"Lesson 3.3 - AI Decision Making and Action Planning","description":"Learning Objectives","source":"@site/docs/module-3/03-Cognitive-Architectures/lesson-3.3-ai-decision-making-and-action-planning.md","sourceDirName":"module-3/03-Cognitive-Architectures","slug":"/module-3/Cognitive-Architectures/lesson-3.3-ai-decision-making-and-action-planning","permalink":"/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-3/Cognitive-Architectures/lesson-3.3-ai-decision-making-and-action-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/AmanNazim/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/edit/main/physical-ai-humanoid-robotics-book/docs/module-3/03-Cognitive-Architectures/lesson-3.3-ai-decision-making-and-action-planning.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Lesson 3.3 - AI Decision Making and Action Planning","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 3.2 - Perception Processing Pipelines","permalink":"/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-3/Cognitive-Architectures/lesson-3.2-perception-processing-pipelines"},"next":{"title":"Chapter 4: AI System Integration","permalink":"/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-3/AI-System-Integration/"}}');var i=t(4848),s=t(8453);const r={title:"Lesson 3.3 - AI Decision Making and Action Planning",sidebar_position:4},o="Lesson 3.3: AI Decision Making and Action Planning",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Overview",id:"overview",level:2},{value:"Understanding AI Decision-Making Systems for Robotics",id:"understanding-ai-decision-making-systems-for-robotics",level:2},{value:"Key Components of AI Decision-Making Systems",id:"key-components-of-ai-decision-making-systems",level:3},{value:"Types of Decision-Making Approaches",id:"types-of-decision-making-approaches",level:3},{value:"Setting Up the AI Decision-Making Environment",id:"setting-up-the-ai-decision-making-environment",level:2},{value:"Isaac Cognitive Architecture Tools",id:"isaac-cognitive-architecture-tools",level:3},{value:"Configuring NVIDIA GPU for AI Processing",id:"configuring-nvidia-gpu-for-ai-processing",level:3},{value:"Implementing Behavior Trees for Action Planning",id:"implementing-behavior-trees-for-action-planning",level:2},{value:"Basic Behavior Tree Structure",id:"basic-behavior-tree-structure",level:3},{value:"CMakeLists.txt for Behavior Tree Node",id:"cmakeliststxt-for-behavior-tree-node",level:3},{value:"Integrating Perception Data with Decision-Making",id:"integrating-perception-data-with-decision-making",level:2},{value:"Perception Integration Node",id:"perception-integration-node",level:3},{value:"Creating Adaptive Systems for Environmental Response",id:"creating-adaptive-systems-for-environmental-response",level:2},{value:"Adaptive Behavior Controller",id:"adaptive-behavior-controller",level:3},{value:"Implementing Finite State Machines for Complex Behaviors",id:"implementing-finite-state-machines-for-complex-behaviors",level:2},{value:"State Machine Implementation",id:"state-machine-implementation",level:3},{value:"Validating AI Decision-Making Performance",id:"validating-ai-decision-making-performance",level:2},{value:"Performance Monitoring Node",id:"performance-monitoring-node",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Launch File",id:"launch-file",level:3},{value:"Practical Example: Autonomous Navigation with Adaptive Decision-Making",id:"practical-example-autonomous-navigation-with-adaptive-decision-making",level:2},{value:"Example Scenario: Warehouse Navigation",id:"example-scenario-warehouse-navigation",level:3},{value:"Integration with Isaac Cognitive Architecture Tools",id:"integration-with-isaac-cognitive-architecture-tools",level:2},{value:"Isaac Cognitive Architecture Integration",id:"isaac-cognitive-architecture-integration",level:3},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"lesson-33-ai-decision-making-and-action-planning",children:"Lesson 3.3: AI Decision Making and Action Planning"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Implement AI decision-making systems for robot behavior using the NVIDIA Isaac ecosystem"}),"\n",(0,i.jsx)(n.li,{children:"Connect AI reasoning with action planning frameworks for coherent robot actions"}),"\n",(0,i.jsx)(n.li,{children:"Create adaptive systems that respond to environmental conditions in real-time"}),"\n",(0,i.jsx)(n.li,{children:"Design and implement decision trees and state machines for humanoid robot autonomy"}),"\n",(0,i.jsx)(n.li,{children:"Integrate perception data with decision-making algorithms to enable intelligent responses"}),"\n",(0,i.jsx)(n.li,{children:"Validate AI decision-making performance with environmental response scenarios"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"In this lesson, we'll explore the critical component of cognitive architectures: AI decision-making and action planning. Building upon the perception processing pipelines established in Lesson 3.2, we'll implement sophisticated AI decision-making systems that allow humanoid robots to process sensory information and generate appropriate behavioral responses. We'll focus on connecting AI reasoning with action planning frameworks to create adaptive systems that respond intelligently to environmental conditions."}),"\n",(0,i.jsx)(n.p,{children:"The AI decision-making system serves as the brain of the cognitive architecture, processing inputs from perception systems and generating appropriate action sequences. This system must handle uncertainty, adapt to changing environments, and make real-time decisions that ensure the robot behaves safely and effectively."}),"\n",(0,i.jsx)(n.h2,{id:"understanding-ai-decision-making-systems-for-robotics",children:"Understanding AI Decision-Making Systems for Robotics"}),"\n",(0,i.jsx)(n.p,{children:"AI decision-making systems in robotics are responsible for selecting appropriate actions based on current perceptions, goals, and environmental conditions. Unlike traditional programming approaches where actions are predetermined, AI decision-making systems must handle uncertainty, learn from experience, and adapt to novel situations."}),"\n",(0,i.jsx)(n.h3,{id:"key-components-of-ai-decision-making-systems",children:"Key Components of AI Decision-Making Systems"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"State Representation"}),": Maintaining a representation of the current world state based on sensor inputs"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Goal Representation"}),": Defining and maintaining goals that guide decision-making"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action Selection"}),": Choosing appropriate actions based on current state and goals"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Execution Monitoring"}),": Tracking action execution and adapting plans as needed"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Learning Mechanisms"}),": Improving decision-making over time based on experience"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"types-of-decision-making-approaches",children:"Types of Decision-Making Approaches"}),"\n",(0,i.jsx)(n.p,{children:"There are several approaches to AI decision-making in robotics:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Rule-Based Systems"}),": Use predefined rules to determine actions based on conditions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"State Machines"}),": Represent decision logic as transitions between discrete states"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Decision Trees"}),": Use hierarchical decision structures to select actions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Planning Algorithms"}),": Generate action sequences to achieve specific goals"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reinforcement Learning"}),": Learn optimal decision policies through trial and error"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The NVIDIA Isaac ecosystem provides specialized cognitive architecture tools that facilitate the implementation of these approaches. Isaac's behavior tree framework, planning algorithms, and AI reasoning components are specifically designed for robotic applications and provide optimized performance on NVIDIA hardware."}),"\n",(0,i.jsx)(n.h2,{id:"setting-up-the-ai-decision-making-environment",children:"Setting Up the AI Decision-Making Environment"}),"\n",(0,i.jsx)(n.p,{children:"Before implementing our decision-making system, we need to establish the environment and dependencies required for AI processing on NVIDIA platforms."}),"\n",(0,i.jsx)(n.h3,{id:"isaac-cognitive-architecture-tools",children:"Isaac Cognitive Architecture Tools"}),"\n",(0,i.jsx)(n.p,{children:"The NVIDIA Isaac ecosystem provides specialized cognitive architecture tools that are essential for implementing AI decision-making systems:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Isaac Behavior Tree Framework"}),": A powerful system for creating complex robot behaviors using hierarchical tree structures"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Isaac Planning Components"}),": Optimized path planning and motion planning algorithms that run efficiently on NVIDIA hardware"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Isaac AI Reasoning Engine"}),": Provides advanced reasoning capabilities for autonomous decision-making"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Isaac Perception Integration"}),": Tools that connect perception data with decision-making algorithms"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Isaac Performance Monitoring"}),": Built-in tools for validating and monitoring cognitive system performance"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"These tools are specifically designed to work together seamlessly and provide optimized performance on NVIDIA GPUs."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Install required packages for AI decision-making\nsudo apt-get update\nsudo apt-get install -y ros-humble-behavior-tree-cpp-v3\nsudo apt-get install -y ros-humble-nav2-behaviors\nsudo apt-get install -y ros-humble-dwb-core\nsudo apt-get install -y ros-humble-isaac-ros-behavior-tree\n"})}),"\n",(0,i.jsx)(n.h3,{id:"configuring-nvidia-gpu-for-ai-processing",children:"Configuring NVIDIA GPU for AI Processing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Verify CUDA installation\nnvidia-smi\nnvcc --version\n\n# Set up environment variables for GPU acceleration\nexport CUDA_VISIBLE_DEVICES=0\nexport NVIDIA_VISIBLE_DEVICES=all\n"})}),"\n",(0,i.jsx)(n.h2,{id:"implementing-behavior-trees-for-action-planning",children:"Implementing Behavior Trees for Action Planning"}),"\n",(0,i.jsx)(n.p,{children:"Behavior trees are a popular choice for action planning in robotics because they provide a flexible, modular way to represent complex behaviors. They're particularly well-suited for humanoid robots that need to handle multiple concurrent tasks and respond to environmental changes."}),"\n",(0,i.jsx)(n.h3,{id:"basic-behavior-tree-structure",children:"Basic Behavior Tree Structure"}),"\n",(0,i.jsx)(n.p,{children:"A behavior tree consists of nodes that can be either control nodes or leaf nodes:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Control Nodes"}),": Manage the flow of execution (sequences, selectors, parallels)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Leaf Nodes"}),": Execute specific actions or conditions (tasks, conditions)"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-cpp",children:'// behavior_tree_node.cpp - AI Decision Making Node\n#include <behaviortree_cpp_v3/bt_factory.h>\n#include <behaviortree_cpp_v3/loggers/basic_logger.h>\n#include <ros/ros.h>\n#include <geometry_msgs/Twist.h>\n#include <sensor_msgs/LaserScan.h>\n#include <nav_msgs/OccupancyGrid.h>\n#include <std_msgs/Bool.h>\n\nusing namespace BT;\n\nclass ApproachTarget : public SyncActionNode\n{\npublic:\n    ApproachTarget(const std::string& name, const NodeConfiguration& config)\n        : SyncActionNode(name, config)\n    {\n        cmd_pub = nh_.advertise<geometry_msgs::Twist>("/cmd_vel", 1);\n    }\n\n    NodeStatus tick() override\n    {\n        geometry_msgs::Twist cmd;\n        cmd.linear.x = 0.3;  // Move forward at 0.3 m/s\n        cmd.angular.z = 0.0; // No rotation\n        cmd_pub.publish(cmd);\n\n        // Simulate approach completion after some condition\n        // In practice, this would check distance to target\n        ros::Duration(1.0).sleep();\n\n        return NodeStatus::SUCCESS;\n    }\n\n    static PortsList providedPorts() { return {}; }\n\nprivate:\n    ros::NodeHandle nh_;\n    ros::Publisher cmd_pub;\n};\n\nclass RotateToTarget : public SyncActionNode\n{\npublic:\n    RotateToTarget(const std::string& name, const NodeConfiguration& config)\n        : SyncActionNode(name, config)\n    {\n        cmd_pub = nh_.advertise<geometry_msgs::Twist>("/cmd_vel", 1);\n    }\n\n    NodeStatus tick() override\n    {\n        geometry_msgs::Twist cmd;\n        cmd.linear.x = 0.0;\n        cmd.angular.z = 0.5;  // Rotate at 0.5 rad/s\n        cmd_pub.publish(cmd);\n\n        // Simulate rotation completion\n        ros::Duration(1.0).sleep();\n\n        return NodeStatus::SUCCESS;\n    }\n\n    static PortsList providedPorts() { return {}; }\n\nprivate:\n    ros::NodeHandle nh_;\n    ros::Publisher cmd_pub;\n};\n\nclass CheckObstacle : public ConditionNode\n{\npublic:\n    CheckObstacle(const std::string& name, const NodeConfiguration& config)\n        : ConditionNode(name, config)\n    {\n        scan_sub = nh_.subscribe("/scan", 1, &CheckObstacle::laserCallback, this);\n    }\n\n    NodeStatus tick() override\n    {\n        if (obstacle_detected_)\n            return NodeStatus::SUCCESS;\n        else\n            return NodeStatus::FAILURE;\n    }\n\n    static PortsList providedPorts() { return {}; }\n\nprivate:\n    void laserCallback(const sensor_msgs::LaserScan::ConstPtr& msg)\n    {\n        obstacle_detected_ = false;\n        for (float range : msg->ranges)\n        {\n            if (range > msg->range_min && range < msg->range_max && range < 1.0)\n            {\n                obstacle_detected_ = true;\n                break;\n            }\n        }\n    }\n\n    ros::NodeHandle nh_;\n    ros::Subscriber scan_sub;\n    bool obstacle_detected_ = false;\n};\n\nint main(int argc, char** argv)\n{\n    ros::init(argc, argv, "ai_decision_making_node");\n    ros::NodeHandle nh;\n\n    // Register custom nodes\n    BehaviorTreeFactory factory;\n    factory.registerNodeType<ApproachTarget>("ApproachTarget");\n    factory.registerNodeType<RotateToTarget>("RotateToTarget");\n    factory.registerNodeType<CheckObstacle>("CheckObstacle");\n\n    // Define the behavior tree\n    auto tree = factory.createTreeFromText(R"(\n        <root BTCPP_format="4">\n            <BehaviorTree>\n                <Sequence>\n                    <CheckObstacle/>\n                    <Fallback>\n                        <Sequence>\n                            <RotateToTarget/>\n                            <ApproachTarget/>\n                        </Sequence>\n                        <ApproachTarget/>\n                    </Fallback>\n                </Sequence>\n            </BehaviorTree>\n        </root>\n    )");\n\n    // Create a logger\n    StdCoutLogger logger_cout(tree);\n\n    // Run the tree continuously\n    while(ros::ok())\n    {\n        tree.tickRoot();\n        ros::Duration(0.1).sleep();\n        ros::spinOnce();\n    }\n\n    return 0;\n}\n'})}),"\n",(0,i.jsx)(n.h3,{id:"cmakeliststxt-for-behavior-tree-node",children:"CMakeLists.txt for Behavior Tree Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-cmake",children:'cmake_minimum_required(VERSION 3.8)\nproject(ai_decision_making)\n\nif(CMAKE_COMPILER_IS_GNUCXX OR CMAKE_CXX_COMPILER_ID MATCHES "Clang")\n  add_compile_options(-Wall -Wextra -Wpedantic)\nendif()\n\n# Find dependencies\nfind_package(ament_cmake REQUIRED)\nfind_package(rclcpp REQUIRED)\nfind_package(std_msgs REQUIRED)\nfind_package(geometry_msgs REQUIRED)\nfind_package(sensor_msgs REQUIRED)\nfind_package(nav_msgs REQUIRED)\nfind_package(behaviortree_cpp_v3 REQUIRED)\n\n# Add executable\nadd_executable(ai_decision_making_node src/behavior_tree_node.cpp)\n\n# Link libraries\ntarget_link_libraries(ai_decision_making_node\n  ${BT_LIBRARIES}\n)\n\n# Link against dependencies\nament_target_dependencies(ai_decision_making_node\n  rclcpp\n  std_msgs\n  geometry_msgs\n  sensor_msgs\n  nav_msgs\n  behaviortree_cpp_v3\n)\n\ninstall(TARGETS ai_decision_making_node\n  DESTINATION lib/${PROJECT_NAME})\n\nament_package()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"integrating-perception-data-with-decision-making",children:"Integrating Perception Data with Decision-Making"}),"\n",(0,i.jsx)(n.p,{children:"The effectiveness of AI decision-making systems depends heavily on the quality and interpretation of perception data. In this section, we'll explore how to integrate perception data from various sensors with our decision-making algorithms."}),"\n",(0,i.jsx)(n.h3,{id:"perception-integration-node",children:"Perception Integration Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-cpp",children:'// perception_integration_node.cpp\n#include <ros/ros.h>\n#include <sensor_msgs/LaserScan.h>\n#include <nav_msgs/OccupancyGrid.h>\n#include <geometry_msgs/PoseStamped.h>\n#include <geometry_msgs/Twist.h>\n#include <tf2_ros/transform_listener.h>\n#include <tf2_geometry_msgs/tf2_geometry_msgs.h>\n#include <std_msgs/Float32.h>\n#include <std_msgs/Int8.h>\n#include <cmath>\n\nclass PerceptionIntegration\n{\npublic:\n    PerceptionIntegration()\n    {\n        ros::NodeHandle nh;\n\n        // Publishers\n        obstacle_distance_pub = nh.advertise<std_msgs::Float32>("/obstacle_distance", 1);\n        danger_level_pub = nh.advertise<std_msgs::Int8>("/danger_level", 1);\n        target_direction_pub = nh.advertise<geometry_msgs::PoseStamped>("/target_direction", 1);\n\n        // Subscribers\n        laser_sub = nh.subscribe("/scan", 1, &PerceptionIntegration::laserCallback, this);\n        map_sub = nh.subscribe("/map", 1, &PerceptionIntegration::mapCallback, this);\n        target_sub = nh.subscribe("/target_pose", 1, &PerceptionIntegration::targetCallback, this);\n\n        tf_buffer = std::make_shared<tf2_ros::Buffer>();\n        tf_listener = std::make_shared<tf2_ros::TransformListener>(*tf_buffer);\n\n        rate = ros::Rate(10); // 10 Hz\n    }\n\n    void run()\n    {\n        while(ros::ok())\n        {\n            processPerceptionData();\n            ros::spinOnce();\n            rate.sleep();\n        }\n    }\n\nprivate:\n    void laserCallback(const sensor_msgs::LaserScan::ConstPtr& msg)\n    {\n        // Process laser scan data to detect obstacles\n        float min_range = std::numeric_limits<float>::max();\n\n        for(size_t i = 0; i < msg->ranges.size(); ++i)\n        {\n            if(msg->ranges[i] > msg->range_min &&\n               msg->ranges[i] < msg->range_max &&\n               msg->ranges[i] < min_range)\n            {\n                min_range = msg->ranges[i];\n            }\n        }\n\n        // Publish minimum obstacle distance\n        std_msgs::Float32 obstacle_dist_msg;\n        obstacle_dist_msg.data = min_range;\n        obstacle_distance_pub.publish(obstacle_dist_msg);\n\n        // Calculate danger level based on proximity\n        int8_t danger_level = 0;\n        if(min_range < 0.5) danger_level = 3; // High danger\n        else if(min_range < 1.0) danger_level = 2; // Medium danger\n        else if(min_range < 2.0) danger_level = 1; // Low danger\n\n        std_msgs::Int8 danger_msg;\n        danger_msg.data = danger_level;\n        danger_level_pub.publish(danger_msg);\n    }\n\n    void mapCallback(const nav_msgs::OccupancyGrid::ConstPtr& msg)\n    {\n        // Process occupancy grid data for path planning\n        // This could include identifying free space, obstacles, and potential paths\n        // For now, we\'ll just store the map data\n        current_map = *msg;\n    }\n\n    void targetCallback(const geometry_msgs::PoseStamped::ConstPtr& msg)\n    {\n        // Store target pose for direction calculation\n        target_pose = *msg;\n    }\n\n    void processPerceptionData()\n    {\n        // Calculate direction to target\n        try\n        {\n            geometry_msgs::TransformStamped transform = tf_buffer->lookupTransform(\n                "base_link", target_pose.header.frame_id,\n                ros::Time(0), ros::Duration(1.0));\n\n            // Transform target pose to robot frame\n            geometry_msgs::PoseStamped transformed_target;\n            tf2::doTransform(target_pose, transformed_target, transform);\n\n            // Calculate direction vector\n            double dx = transformed_target.pose.position.x;\n            double dy = transformed_target.pose.position.y;\n            double distance_to_target = sqrt(dx*dx + dy*dy);\n\n            // Publish target direction\n            geometry_msgs::PoseStamped direction_msg;\n            direction_msg.header.stamp = ros::Time::now();\n            direction_msg.header.frame_id = "base_link";\n            direction_msg.pose.position.x = dx;\n            direction_msg.pose.position.y = dy;\n            direction_msg.pose.orientation.w = 1.0;\n            target_direction_pub.publish(direction_msg);\n        }\n        catch (tf2::TransformException &ex)\n        {\n            ROS_WARN("Could not transform target pose: %s", ex.what());\n        }\n    }\n\n    ros::Publisher obstacle_distance_pub;\n    ros::Publisher danger_level_pub;\n    ros::Publisher target_direction_pub;\n\n    ros::Subscriber laser_sub;\n    ros::Subscriber map_sub;\n    ros::Subscriber target_sub;\n\n    std::shared_ptr<tf2_ros::Buffer> tf_buffer;\n    std::shared_ptr<tf2_ros::TransformListener> tf_listener;\n\n    nav_msgs::OccupancyGrid current_map;\n    geometry_msgs::PoseStamped target_pose;\n\n    ros::Rate rate;\n};\n\nint main(int argc, char** argv)\n{\n    ros::init(argc, argv, "perception_integration_node");\n    PerceptionIntegration perception_integration;\n    perception_integration.run();\n    return 0;\n}\n'})}),"\n",(0,i.jsx)(n.h2,{id:"creating-adaptive-systems-for-environmental-response",children:"Creating Adaptive Systems for Environmental Response"}),"\n",(0,i.jsx)(n.p,{children:"Adaptive systems are crucial for humanoid robots operating in dynamic environments. These systems must continuously monitor environmental conditions and adjust behavior accordingly. In this section, we'll implement adaptive mechanisms that allow the robot to respond to changing conditions."}),"\n",(0,i.jsx)(n.h3,{id:"adaptive-behavior-controller",children:"Adaptive Behavior Controller"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-cpp",children:'// adaptive_behavior_controller.cpp\n#include <ros/ros.h>\n#include <geometry_msgs/Twist.h>\n#include <std_msgs/Float32.h>\n#include <std_msgs/Int8.h>\n#include <std_msgs/Bool.h>\n#include <sensor_msgs/LaserScan.h>\n#include <nav_msgs/Odometry.h>\n#include <dynamic_reconfigure/server.h>\n#include <std_srvs/SetBool.h>\n#include <cmath>\n\nclass AdaptiveBehaviorController\n{\npublic:\n    AdaptiveBehaviorController()\n    {\n        ros::NodeHandle nh;\n        ros::NodeHandle private_nh("~");\n\n        // Publishers\n        cmd_vel_pub = nh.advertise<geometry_msgs::Twist>("/cmd_vel", 1);\n\n        // Subscribers\n        obstacle_distance_sub = nh.subscribe("/obstacle_distance", 1,\n                                           &AdaptiveBehaviorController::obstacleDistanceCallback, this);\n        danger_level_sub = nh.subscribe("/danger_level", 1,\n                                      &AdaptiveBehaviorController::dangerLevelCallback, this);\n        odometry_sub = nh.subscribe("/odom", 1,\n                                  &AdaptiveBehaviorController::odometryCallback, this);\n\n        // Parameters\n        private_nh.param<double>("linear_velocity", linear_velocity_, 0.5);\n        private_nh.param<double>("angular_velocity", angular_velocity_, 0.3);\n        private_nh.param<double>("safe_distance", safe_distance_, 1.0);\n        private_nh.param<double>("emergency_distance", emergency_distance_, 0.5);\n        private_nh.param<double>("max_linear_velocity", max_linear_velocity_, 1.0);\n        private_nh.param<double>("min_linear_velocity", min_linear_velocity_, 0.1);\n\n        // Initialize variables\n        current_danger_level_ = 0;\n        obstacle_distance_ = std::numeric_limits<float>::max();\n        current_linear_vel_ = linear_velocity_;\n        current_angular_vel_ = angular_velocity_;\n\n        rate = ros::Rate(20); // 20 Hz\n    }\n\n    void run()\n    {\n        while(ros::ok())\n        {\n            updateBehavior();\n            ros::spinOnce();\n            rate.sleep();\n        }\n    }\n\nprivate:\n    void obstacleDistanceCallback(const std_msgs::Float32::ConstPtr& msg)\n    {\n        obstacle_distance_ = msg->data;\n    }\n\n    void dangerLevelCallback(const std_msgs::Int8::ConstPtr& msg)\n    {\n        current_danger_level_ = msg->data;\n    }\n\n    void odometryCallback(const nav_msgs::Odometry::ConstPtr& msg)\n    {\n        // Store current position and velocity for adaptive control\n        current_odom_ = *msg;\n    }\n\n    void updateBehavior()\n    {\n        geometry_msgs::Twist cmd;\n\n        // Adjust velocities based on danger level and obstacle distance\n        switch(current_danger_level_)\n        {\n            case 0: // Safe\n                cmd.linear.x = current_linear_vel_;\n                cmd.angular.z = current_angular_vel_;\n                break;\n\n            case 1: // Low danger\n                cmd.linear.x = current_linear_vel_ * 0.7; // Reduce speed\n                cmd.angular.z = current_angular_vel_ * 1.2; // Increase turning ability\n                break;\n\n            case 2: // Medium danger\n                cmd.linear.x = current_linear_vel_ * 0.4; // Further reduce speed\n                cmd.angular.z = current_angular_vel_ * 1.5; // Higher turning priority\n                break;\n\n            case 3: // High danger\n                cmd.linear.x = 0.0; // Stop linear motion\n                cmd.angular.z = current_angular_vel_ * 2.0; // Maximum turning\n                break;\n\n            default:\n                cmd.linear.x = 0.0;\n                cmd.angular.z = 0.0;\n                break;\n        }\n\n        // Additional safety check based on obstacle distance\n        if(obstacle_distance_ < emergency_distance_)\n        {\n            cmd.linear.x = 0.0; // Emergency stop\n            cmd.angular.z = current_angular_vel_; // Allow turning to escape\n        }\n        else if(obstacle_distance_ < safe_distance_)\n        {\n            // Gradually reduce speed as obstacle gets closer\n            double reduction_factor = obstacle_distance_ / safe_distance_;\n            cmd.linear.x *= reduction_factor;\n        }\n\n        // Apply velocity limits\n        cmd.linear.x = std::max(min_linear_velocity_,\n                               std::min(max_linear_velocity_, cmd.linear.x));\n        cmd.angular.z = std::min(angular_velocity_ * 2.0,\n                                std::max(-angular_velocity_ * 2.0, cmd.angular.z));\n\n        // Publish command\n        cmd_vel_pub.publish(cmd);\n    }\n\n    ros::Publisher cmd_vel_pub;\n    ros::Subscriber obstacle_distance_sub;\n    ros::Subscriber danger_level_sub;\n    ros::Subscriber odometry_sub;\n\n    double linear_velocity_;\n    double angular_velocity_;\n    double safe_distance_;\n    double emergency_distance_;\n    double max_linear_velocity_;\n    double min_linear_velocity_;\n\n    float current_danger_level_;\n    float obstacle_distance_;\n    double current_linear_vel_;\n    double current_angular_vel_;\n\n    nav_msgs::Odometry current_odom_;\n\n    ros::Rate rate;\n};\n\nint main(int argc, char** argv)\n{\n    ros::init(argc, argv, "adaptive_behavior_controller");\n    AdaptiveBehaviorController controller;\n    controller.run();\n    return 0;\n}\n'})}),"\n",(0,i.jsx)(n.h2,{id:"implementing-finite-state-machines-for-complex-behaviors",children:"Implementing Finite State Machines for Complex Behaviors"}),"\n",(0,i.jsx)(n.p,{children:"Finite State Machines (FSMs) are excellent for modeling complex robot behaviors with distinct operational modes. They provide a clear structure for managing different robot states and transitions between them based on environmental conditions."}),"\n",(0,i.jsx)(n.h3,{id:"state-machine-implementation",children:"State Machine Implementation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-cpp",children:'// state_machine_behavior.cpp\n#include <ros/ros.h>\n#include <geometry_msgs/Twist.h>\n#include <std_msgs/Float32.h>\n#include <std_msgs/Bool.h>\n#include <sensor_msgs/LaserScan.h>\n#include <nav_msgs/OccupancyGrid.h>\n#include <tf2_ros/transform_listener.h>\n#include <tf2_geometry_msgs/tf2_geometry_msgs.h>\n#include <std_srvs/SetBool.h>\n#include <vector>\n#include <string>\n#include <cmath>\n\nenum RobotState {\n    IDLE,\n    EXPLORING,\n    NAVIGATING_TO_TARGET,\n    AVOIDING_OBSTACLE,\n    REACHED_TARGET,\n    EMERGENCY_STOP\n};\n\nclass StateMachineBehavior\n{\npublic:\n    StateMachineBehavior()\n    {\n        ros::NodeHandle nh;\n        ros::NodeHandle private_nh("~");\n\n        // Publishers\n        cmd_vel_pub = nh.advertise<geometry_msgs::Twist>("/cmd_vel", 1);\n        current_state_pub = nh.advertise<std_msgs::String>("/robot_state", 1);\n\n        // Subscribers\n        laser_sub = nh.subscribe("/scan", 1, &StateMachineBehavior::laserCallback, this);\n        target_pose_sub = nh.subscribe("/target_pose", 1, &StateMachineBehavior::targetCallback, this);\n        obstacle_distance_sub = nh.subscribe("/obstacle_distance", 1, &StateMachineBehavior::obstacleDistanceCallback, this);\n\n        // Services\n        start_service = nh.advertiseService("/start_robot_behavior", &StateMachineBehavior::startRobotBehavior, this);\n        stop_service = nh.advertiseService("/stop_robot_behavior", &StateMachineBehavior::stopRobotBehavior, this);\n\n        // Parameters\n        private_nh.param<double>("linear_velocity", linear_velocity_, 0.5);\n        private_nh.param<double>("angular_velocity", angular_velocity_, 0.3);\n        private_nh.param<double>("safe_distance", safe_distance_, 1.0);\n        private_nh.param<double>("target_tolerance", target_tolerance_, 0.5);\n\n        // Initialize state\n        current_state_ = IDLE;\n        robot_active_ = false;\n        obstacle_distance_ = std::numeric_limits<float>::max();\n\n        rate = ros::Rate(20); // 20 Hz\n\n        ROS_INFO("State Machine Behavior Node Initialized");\n    }\n\n    void run()\n    {\n        while(ros::ok())\n        {\n            if(robot_active_)\n            {\n                updateStateMachine();\n            }\n            else\n            {\n                // If robot is not active, send zero velocity\n                geometry_msgs::Twist stop_cmd;\n                cmd_vel_pub.publish(stop_cmd);\n            }\n\n            publishCurrentState();\n            ros::spinOnce();\n            rate.sleep();\n        }\n    }\n\nprivate:\n    void laserCallback(const sensor_msgs::LaserScan::ConstPtr& msg)\n    {\n        // Find minimum range in laser scan\n        float min_range = std::numeric_limits<float>::max();\n        for(float range : msg->ranges)\n        {\n            if(range > msg->range_min && range < msg->range_max && range < min_range)\n            {\n                min_range = range;\n            }\n        }\n        obstacle_distance_ = min_range;\n    }\n\n    void targetCallback(const geometry_msgs::PoseStamped::ConstPtr& msg)\n    {\n        target_pose_ = *msg;\n    }\n\n    void obstacleDistanceCallback(const std_msgs::Float32::ConstPtr& msg)\n    {\n        obstacle_distance_ = msg->data;\n    }\n\n    bool startRobotBehavior(std_srvs::SetBool::Request &req,\n                           std_srvs::SetBool::Response &res)\n    {\n        if(req.data)\n        {\n            robot_active_ = true;\n            current_state_ = EXPLORING; // Start with exploration\n            res.success = true;\n            res.message = "Robot behavior started";\n            ROS_INFO("Robot behavior started");\n        }\n        else\n        {\n            robot_active_ = false;\n            current_state_ = IDLE;\n            res.success = true;\n            res.message = "Robot behavior stopped";\n            ROS_INFO("Robot behavior stopped");\n        }\n        return true;\n    }\n\n    bool stopRobotBehavior(std_srvs::SetBool::Request &req,\n                          std_srvs::SetBool::Response &res)\n    {\n        robot_active_ = false;\n        current_state_ = IDLE;\n        // Send stop command immediately\n        geometry_msgs::Twist stop_cmd;\n        cmd_vel_pub.publish(stop_cmd);\n        res.success = true;\n        res.message = "Robot stopped immediately";\n        ROS_INFO("Robot stopped immediately");\n        return true;\n    }\n\n    void updateStateMachine()\n    {\n        RobotState new_state = current_state_;\n\n        // State transition logic\n        switch(current_state_)\n        {\n            case IDLE:\n                if(robot_active_)\n                {\n                    new_state = EXPLORING;\n                }\n                break;\n\n            case EXPLORING:\n                if(hasTarget())\n                {\n                    new_state = NAVIGATING_TO_TARGET;\n                }\n                else if(obstacle_distance_ < safe_distance_ * 0.5)\n                {\n                    new_state = AVOIDING_OBSTACLE;\n                }\n                break;\n\n            case NAVIGATING_TO_TARGET:\n                if(isNearTarget())\n                {\n                    new_state = REACHED_TARGET;\n                }\n                else if(obstacle_distance_ < safe_distance_ * 0.7)\n                {\n                    new_state = AVOIDING_OBSTACLE;\n                }\n                else if(!hasTarget()) // Target lost\n                {\n                    new_state = EXPLORING;\n                }\n                break;\n\n            case AVOIDING_OBSTACLE:\n                if(obstacle_distance_ > safe_distance_)\n                {\n                    if(hasTarget())\n                    {\n                        new_state = NAVIGATING_TO_TARGET;\n                    }\n                    else\n                    {\n                        new_state = EXPLORING;\n                    }\n                }\n                break;\n\n            case REACHED_TARGET:\n                // Stay in this state until new target is set\n                if(!isNearTarget())\n                {\n                    new_state = NAVIGATING_TO_TARGET;\n                }\n                break;\n\n            case EMERGENCY_STOP:\n                // Emergency stop state - only exit via service call\n                if(obstacle_distance_ > safe_distance_ * 2.0)\n                {\n                    new_state = EXPLORING;\n                }\n                break;\n        }\n\n        // Execute behavior for current state\n        executeCurrentState();\n\n        // Update state if changed\n        if(new_state != current_state_)\n        {\n            ROS_INFO("State transition: %s -> %s",\n                     getStateName(current_state_).c_str(),\n                     getStateName(new_state).c_str());\n            current_state_ = new_state;\n        }\n    }\n\n    void executeCurrentState()\n    {\n        geometry_msgs::Twist cmd;\n\n        switch(current_state_)\n        {\n            case IDLE:\n                cmd.linear.x = 0.0;\n                cmd.angular.z = 0.0;\n                break;\n\n            case EXPLORING:\n                // Random exploration behavior\n                cmd.linear.x = linear_velocity_ * 0.8; // Slightly slower exploration\n                cmd.angular.z = 0.0; // Move forward initially\n                break;\n\n            case NAVIGATING_TO_TARGET:\n                // Navigate towards target\n                cmd.linear.x = linear_velocity_;\n                cmd.angular.z = calculateAngularVelocityToTarget();\n                break;\n\n            case AVOIDING_OBSTACLE:\n                // Simple obstacle avoidance - turn away from obstacle\n                cmd.linear.x = 0.0;\n                cmd.angular.z = angular_velocity_ * 1.5; // Turn to avoid\n                break;\n\n            case REACHED_TARGET:\n                // Stop when target reached\n                cmd.linear.x = 0.0;\n                cmd.angular.z = 0.0;\n                break;\n\n            case EMERGENCY_STOP:\n                // Full stop in emergency\n                cmd.linear.x = 0.0;\n                cmd.angular.z = 0.0;\n                break;\n        }\n\n        cmd_vel_pub.publish(cmd);\n    }\n\n    bool hasTarget()\n    {\n        // Check if a valid target is available\n        return target_pose_.header.stamp.toSec() > 0;\n    }\n\n    bool isNearTarget()\n    {\n        // Simplified distance check - in practice, this would require TF transforms\n        // to compare robot position with target position\n        return obstacle_distance_ < target_tolerance_;\n    }\n\n    double calculateAngularVelocityToTarget()\n    {\n        // Simplified calculation - in practice, this would use TF to determine\n        // the angle between robot orientation and target direction\n        return angular_velocity_ * 0.5; // Reduced angular velocity for smooth navigation\n    }\n\n    std::string getStateName(RobotState state)\n    {\n        switch(state)\n        {\n            case IDLE: return "IDLE";\n            case EXPLORING: return "EXPLORING";\n            case NAVIGATING_TO_TARGET: return "NAVIGATING_TO_TARGET";\n            case AVOIDING_OBSTACLE: return "AVOIDING_OBSTACLE";\n            case REACHED_TARGET: return "REACHED_TARGET";\n            case EMERGENCY_STOP: return "EMERGENCY_STOP";\n            default: return "UNKNOWN";\n        }\n    }\n\n    void publishCurrentState()\n    {\n        std_msgs::String state_msg;\n        state_msg.data = getStateName(current_state_);\n        current_state_pub.publish(state_msg);\n    }\n\n    ros::Publisher cmd_vel_pub;\n    ros::Publisher current_state_pub;\n    ros::Subscriber laser_sub;\n    ros::Subscriber target_pose_sub;\n    ros::Subscriber obstacle_distance_sub;\n    ros::ServiceServer start_service;\n    ros::ServiceServer stop_service;\n\n    double linear_velocity_;\n    double angular_velocity_;\n    double safe_distance_;\n    double target_tolerance_;\n\n    RobotState current_state_;\n    bool robot_active_;\n    float obstacle_distance_;\n    geometry_msgs::PoseStamped target_pose_;\n\n    ros::Rate rate;\n};\n\nint main(int argc, char** argv)\n{\n    ros::init(argc, argv, "state_machine_behavior");\n    StateMachineBehavior sm;\n    sm.run();\n    return 0;\n}\n'})}),"\n",(0,i.jsx)(n.h2,{id:"validating-ai-decision-making-performance",children:"Validating AI Decision-Making Performance"}),"\n",(0,i.jsx)(n.p,{children:"Performance validation is crucial for AI decision-making systems. We need to ensure that our systems respond appropriately to various environmental conditions and make decisions that lead to successful robot behavior."}),"\n",(0,i.jsx)(n.h3,{id:"performance-monitoring-node",children:"Performance Monitoring Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-cpp",children:'// performance_monitor_node.cpp\n#include <ros/ros.h>\n#include <std_msgs/Float32.h>\n#include <std_msgs/Int8.h>\n#include <std_msgs/String.h>\n#include <geometry_msgs/Twist.h>\n#include <sensor_msgs/LaserScan.h>\n#include <nav_msgs/Odometry.h>\n#include <visualization_msgs/Marker.h>\n#include <std_msgs/ColorRGBA.h>\n#include <fstream>\n#include <iomanip>\n\nclass PerformanceMonitor\n{\npublic:\n    PerformanceMonitor()\n    {\n        ros::NodeHandle nh;\n        ros::NodeHandle private_nh("~");\n\n        // Publishers\n        performance_metrics_pub = nh.advertise<std_msgs::Float32MultiArray>("/performance_metrics", 1);\n        visualization_marker_pub = nh.advertise<visualization_msgs::Marker>("/performance_visualization", 1);\n\n        // Subscribers\n        state_sub = nh.subscribe("/robot_state", 1, &PerformanceMonitor::stateCallback, this);\n        cmd_vel_sub = nh.subscribe("/cmd_vel", 1, &PerformanceMonitor::cmdVelCallback, this);\n        laser_sub = nh.subscribe("/scan", 1, &PerformanceMonitor::laserCallback, this);\n        odom_sub = nh.subscribe("/odom", 1, &PerformanceMonitor::odometryCallback, this);\n\n        // Parameters\n        private_nh.param<std::string>("log_file", log_file_, "/tmp/robot_performance_log.csv");\n        private_nh.param<double>("update_rate", update_rate_, 1.0);\n        private_nh.param<double>("safe_distance", safe_distance_, 1.0);\n\n        // Initialize variables\n        current_state_ = "UNKNOWN";\n        last_update_time_ = ros::Time::now();\n        total_distance_traveled_ = 0.0;\n        total_time_ = 0.0;\n        obstacle_encounters_ = 0;\n        emergency_stops_ = 0;\n        last_position_ = {0, 0};\n\n        // Open log file\n        log_file_stream_.open(log_file_, std::ios::out | std::ios::app);\n        if(log_file_stream_.is_open())\n        {\n            log_file_stream_ << "timestamp,state,distance_traveled,avg_velocity,obstacle_encounters,emergency_stops\\n";\n        }\n\n        rate = ros::Rate(update_rate_);\n    }\n\n    ~PerformanceMonitor()\n    {\n        if(log_file_stream_.is_open())\n        {\n            log_file_stream_.close();\n        }\n    }\n\n    void run()\n    {\n        while(ros::ok())\n        {\n            if((ros::Time::now() - last_update_time_).toSec() >= (1.0 / update_rate_))\n            {\n                publishPerformanceMetrics();\n                visualizePerformance();\n                last_update_time_ = ros::Time::now();\n            }\n\n            ros::spinOnce();\n            rate.sleep();\n        }\n    }\n\nprivate:\n    void stateCallback(const std_msgs::String::ConstPtr& msg)\n    {\n        current_state_ = msg->data;\n        if(current_state_ == "EMERGENCY_STOP")\n        {\n            emergency_stops_++;\n        }\n    }\n\n    void cmdVelCallback(const geometry_msgs::Twist::ConstPtr& msg)\n    {\n        current_linear_velocity_ = msg->linear.x;\n        current_angular_velocity_ = msg->angular.z;\n    }\n\n    void laserCallback(const sensor_msgs::LaserScan::ConstPtr& msg)\n    {\n        // Count obstacle encounters (any range within safe distance)\n        bool near_obstacle = false;\n        for(float range : msg->ranges)\n        {\n            if(range > msg->range_min && range < msg->range_max && range < safe_distance_)\n            {\n                near_obstacle = true;\n                break;\n            }\n        }\n\n        if(near_obstacle && !was_near_obstacle_)\n        {\n            obstacle_encounters_++;\n        }\n        was_near_obstacle_ = near_obstacle;\n    }\n\n    void odometryCallback(const nav_msgs::Odometry::ConstPtr& msg)\n    {\n        // Calculate distance traveled\n        double current_x = msg->pose.pose.position.x;\n        double current_y = msg->pose.pose.position.y;\n\n        if(first_odom_received_)\n        {\n            double dx = current_x - last_position_.first;\n            double dy = current_y - last_position_.second;\n            double distance_increment = sqrt(dx*dx + dy*dy);\n            total_distance_traveled_ += distance_increment;\n        }\n        else\n        {\n            first_odom_received_ = true;\n        }\n\n        last_position_ = {current_x, current_y};\n        total_time_ += 0.05; // Assuming 20Hz odometry update\n    }\n\n    void publishPerformanceMetrics()\n    {\n        std_msgs::Float32MultiArray metrics;\n\n        // Performance metrics: [distance_traveled, avg_velocity, obstacle_encounters, emergency_stops, time_running]\n        metrics.data.push_back(total_distance_traveled_);\n        double avg_velocity = total_time_ > 0 ? total_distance_traveled_ / total_time_ : 0.0;\n        metrics.data.push_back(avg_velocity);\n        metrics.data.push_back(obstacle_encounters_);\n        metrics.data.push_back(emergency_stops_);\n        metrics.data.push_back(total_time_);\n\n        performance_metrics_pub.publish(metrics);\n\n        // Log to file\n        if(log_file_stream_.is_open())\n        {\n            ros::Time current_time = ros::Time::now();\n            log_file_stream_ << std::fixed << std::setprecision(3)\n                            << current_time.toSec() << ","\n                            << current_state_ << ","\n                            << total_distance_traveled_ << ","\n                            << avg_velocity << ","\n                            << obstacle_encounters_ << ","\n                            << emergency_stops_ << "\\n";\n            log_file_stream_.flush();\n        }\n    }\n\n    void visualizePerformance()\n    {\n        visualization_msgs::Marker marker;\n\n        marker.header.frame_id = "map";\n        marker.header.stamp = ros::Time::now();\n        marker.ns = "performance_monitor";\n        marker.id = 0;\n        marker.type = visualization_msgs::Marker::TEXT_VIEW_FACING;\n        marker.action = visualization_msgs::Marker::ADD;\n\n        // Position the text marker above the robot\n        marker.pose.position.x = last_position_.first;\n        marker.pose.position.y = last_position_.second;\n        marker.pose.position.z = 1.0;\n        marker.pose.orientation.x = 0.0;\n        marker.pose.orientation.y = 0.0;\n        marker.pose.orientation.z = 0.0;\n        marker.pose.orientation.w = 1.0;\n\n        marker.scale.z = 0.2; // Text height\n        marker.color.a = 1.0; // Alpha\n        marker.color.r = 1.0; // Red\n        marker.color.g = 1.0; // Green\n        marker.color.b = 1.0; // Blue\n\n        std::ostringstream oss;\n        oss << "State: " << current_state_\n            << "\\nDist: " << std::fixed << std::setprecision(2) << total_distance_traveled_ << "m"\n            << "\\nAvg Vel: " << std::fixed << std::setprecision(2) << (total_time_ > 0 ? total_distance_traveled_/total_time_ : 0.0) << "m/s"\n            << "\\nObstacles: " << obstacle_encounters_\n            << "\\nEmergencies: " << emergency_stops_;\n\n        marker.text = oss.str();\n\n        visualization_marker_pub.publish(marker);\n    }\n\n    ros::Publisher performance_metrics_pub;\n    ros::Publisher visualization_marker_pub;\n    ros::Subscriber state_sub;\n    ros::Subscriber cmd_vel_sub;\n    ros::Subscriber laser_sub;\n    ros::Subscriber odom_sub;\n\n    std::string current_state_;\n    double current_linear_velocity_ = 0.0;\n    double current_angular_velocity_ = 0.0;\n    bool was_near_obstacle_ = false;\n\n    double total_distance_traveled_;\n    double total_time_;\n    int obstacle_encounters_;\n    int emergency_stops_;\n    std::pair<double, double> last_position_;\n    bool first_odom_received_ = false;\n\n    std::string log_file_;\n    std::ofstream log_file_stream_;\n    double update_rate_;\n    double safe_distance_;\n\n    ros::Time last_update_time_;\n    ros::Rate rate;\n};\n\nint main(int argc, char** argv)\n{\n    ros::init(argc, argv, "performance_monitor");\n    PerformanceMonitor pm;\n    pm.run();\n    return 0;\n}\n'})}),"\n",(0,i.jsx)(n.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,i.jsx)(n.p,{children:"Now that we've implemented our AI decision-making and action planning systems, let's create a launch file to bring everything together and test the integrated system."}),"\n",(0,i.jsx)(n.h3,{id:"launch-file",children:"Launch File"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- ai_decision_making.launch --\x3e\n<launch>\n  \x3c!-- Launch the perception integration node --\x3e\n  <node name="perception_integration_node" pkg="ai_decision_making" type="perception_integration_node" output="screen">\n    <param name="linear_velocity" value="0.5"/>\n    <param name="angular_velocity" value="0.3"/>\n    <param name="safe_distance" value="1.0"/>\n  </node>\n\n  \x3c!-- Launch the adaptive behavior controller --\x3e\n  <node name="adaptive_behavior_controller" pkg="ai_decision_making" type="adaptive_behavior_controller" output="screen">\n    <param name="linear_velocity" value="0.5"/>\n    <param name="angular_velocity" value="0.3"/>\n    <param name="safe_distance" value="1.0"/>\n    <param name="emergency_distance" value="0.5"/>\n    <param name="max_linear_velocity" value="1.0"/>\n    <param name="min_linear_velocity" value="0.1"/>\n  </node>\n\n  \x3c!-- Launch the state machine behavior --\x3e\n  <node name="state_machine_behavior" pkg="ai_decision_making" type="state_machine_behavior" output="screen">\n    <param name="linear_velocity" value="0.5"/>\n    <param name="angular_velocity" value="0.3"/>\n    <param name="safe_distance" value="1.0"/>\n    <param name="target_tolerance" value="0.5"/>\n  </node>\n\n  \x3c!-- Launch the performance monitor --\x3e\n  <node name="performance_monitor" pkg="ai_decision_making" type="performance_monitor" output="screen">\n    <param name="log_file" value="/tmp/robot_performance_log.csv"/>\n    <param name="update_rate" value="1.0"/>\n    <param name="safe_distance" value="1.0"/>\n  </node>\n\n  \x3c!-- Launch RViz for visualization --\x3e\n  <node name="rviz" pkg="rviz" type="rviz" args="-d $(find ai_decision_making)/config/performance_monitor.rviz" />\n\n</launch>\n'})}),"\n",(0,i.jsx)(n.h2,{id:"practical-example-autonomous-navigation-with-adaptive-decision-making",children:"Practical Example: Autonomous Navigation with Adaptive Decision-Making"}),"\n",(0,i.jsx)(n.p,{children:"Let's put everything together with a practical example that demonstrates how the AI decision-making system responds to various environmental conditions."}),"\n",(0,i.jsx)(n.h3,{id:"example-scenario-warehouse-navigation",children:"Example Scenario: Warehouse Navigation"}),"\n",(0,i.jsx)(n.p,{children:"Consider a humanoid robot tasked with navigating through a warehouse environment to reach a target location. The environment contains static obstacles (shelves, walls) and dynamic obstacles (moving forklifts, people). Our AI decision-making system must:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Plan a path to the target while avoiding static obstacles"}),"\n",(0,i.jsx)(n.li,{children:"Detect and respond to dynamic obstacles in real-time"}),"\n",(0,i.jsx)(n.li,{children:"Adapt its behavior based on the current situation"}),"\n",(0,i.jsx)(n.li,{children:"Switch between different operational modes as needed"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"# warehouse_scenario.yaml - Configuration for warehouse navigation scenario\nai_decision_making:\n  linear_velocity: 0.6\n  angular_velocity: 0.4\n  safe_distance: 1.2\n  emergency_distance: 0.6\n  target_tolerance: 0.4\n  max_linear_velocity: 1.0\n  min_linear_velocity: 0.1\n\n  # Behavior tree configuration\n  behavior_tree:\n    root_sequence:\n      - check_obstacle_proximity\n      - select_navigation_strategy:\n          - sequence_approach_target\n          - fallback_avoidance\n\n  # State machine configuration\n  state_machine:\n    idle_timeout: 30.0\n    exploration_duration: 60.0\n    target_search_timeout: 120.0\n\n  # Performance monitoring\n  performance_thresholds:\n    min_avg_velocity: 0.2\n    max_emergency_stops_per_minute: 2\n    min_success_rate: 0.8\n"})}),"\n",(0,i.jsx)(n.h2,{id:"integration-with-isaac-cognitive-architecture-tools",children:"Integration with Isaac Cognitive Architecture Tools"}),"\n",(0,i.jsx)(n.p,{children:"The NVIDIA Isaac ecosystem provides powerful tools for cognitive architecture development. Let's see how we can integrate our decision-making system with Isaac's cognitive components:"}),"\n",(0,i.jsx)(n.h3,{id:"isaac-cognitive-architecture-integration",children:"Isaac Cognitive Architecture Integration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# isaac_cognitive_integration.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom std_msgs.msg import String, Float32, Int8\nfrom builtin_interfaces.msg import Duration\nimport numpy as np\nimport math\n\nclass IsaacCognitiveIntegrator(Node):\n    def __init__(self):\n        super().__init__('isaac_cognitive_integrator')\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.robot_state_pub = self.create_publisher(String, '/robot_state', 10)\n        self.danger_level_pub = self.create_publisher(Int8, '/danger_level', 10)\n\n        # Subscribers\n        self.laser_sub = self.create_subscription(\n            LaserScan,\n            '/scan',\n            self.laser_callback,\n            10\n        )\n\n        self.target_sub = self.create_subscription(\n            PoseStamped,\n            '/target_pose',\n            self.target_callback,\n            10\n        )\n\n        # Parameters\n        self.linear_velocity = self.declare_parameter('linear_velocity', 0.5).value\n        self.angular_velocity = self.declare_parameter('angular_velocity', 0.3).value\n        self.safe_distance = self.declare_parameter('safe_distance', 1.0).value\n\n        # State variables\n        self.current_state = 'IDLE'\n        self.obstacle_distances = []\n        self.target_pose = None\n\n        # Timer for main loop\n        self.timer = self.create_timer(0.05, self.main_loop)  # 20 Hz\n\n        self.get_logger().info('Isaac Cognitive Integrator initialized')\n\n    def laser_callback(self, msg):\n        \"\"\"Process laser scan data\"\"\"\n        ranges = [r for r in msg.ranges if not math.isnan(r)]\n        if ranges:\n            self.obstacle_distances = ranges\n\n    def target_callback(self, msg):\n        \"\"\"Receive target pose\"\"\"\n        self.target_pose = msg\n\n    def calculate_danger_level(self):\n        \"\"\"Calculate danger level based on obstacle distances\"\"\"\n        if not self.obstacle_distances:\n            return 0\n\n        min_distance = min(self.obstacle_distances)\n\n        if min_distance < 0.5:\n            return 3  # High danger\n        elif min_distance < 1.0:\n            return 2  # Medium danger\n        elif min_distance < 2.0:\n            return 1  # Low danger\n        else:\n            return 0  # Safe\n\n    def make_decision(self):\n        \"\"\"Core AI decision-making algorithm\"\"\"\n        danger_level = self.calculate_danger_level()\n\n        # Publish danger level\n        danger_msg = Int8()\n        danger_msg.data = danger_level\n        self.danger_level_pub.publish(danger_msg)\n\n        # State transition logic\n        if danger_level >= 3:\n            new_state = 'EMERGENCY_STOP'\n        elif danger_level >= 2:\n            new_state = 'AVOIDING_OBSTACLE'\n        elif self.target_pose is not None:\n            new_state = 'NAVIGATING_TO_TARGET'\n        else:\n            new_state = 'EXPLORING'\n\n        if new_state != self.current_state:\n            self.get_logger().info(f'State transition: {self.current_state} -> {new_state}')\n            self.current_state = new_state\n\n        # Publish current state\n        state_msg = String()\n        state_msg.data = self.current_state\n        self.robot_state_pub.publish(state_msg)\n\n        return new_state\n\n    def execute_action(self, state):\n        \"\"\"Execute action based on current state\"\"\"\n        cmd = Twist()\n\n        if state == 'IDLE':\n            cmd.linear.x = 0.0\n            cmd.angular.z = 0.0\n        elif state == 'EXPLORING':\n            cmd.linear.x = self.linear_velocity * 0.7\n            cmd.angular.z = 0.0\n        elif state == 'NAVIGATING_TO_TARGET':\n            cmd.linear.x = self.linear_velocity\n            cmd.angular.z = self.calculate_angular_to_target()\n        elif state == 'AVOIDING_OBSTACLE':\n            cmd.linear.x = 0.0\n            cmd.angular.z = self.angular_velocity * 1.5\n        elif state == 'EMERGENCY_STOP':\n            cmd.linear.x = 0.0\n            cmd.angular.z = 0.0\n\n        # Adjust for safety\n        if self.obstacle_distances:\n            min_dist = min(self.obstacle_distances)\n            if min_dist < self.safe_distance:\n                cmd.linear.x *= (min_dist / self.safe_distance)\n\n        self.cmd_vel_pub.publish(cmd)\n\n    def calculate_angular_to_target(self):\n        \"\"\"Calculate angular velocity to face target\"\"\"\n        if self.target_pose is None:\n            return 0.0\n\n        # Simplified calculation - in practice, this would use TF transforms\n        return self.angular_velocity * 0.5\n\n    def main_loop(self):\n        \"\"\"Main decision-making loop\"\"\"\n        state = self.make_decision()\n        self.execute_action(state)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacCognitiveIntegrator()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"In this lesson, we've implemented comprehensive AI decision-making and action planning systems for humanoid robots using the NVIDIA Isaac ecosystem. We covered:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Behavior Trees"}),": Created flexible, modular action planning systems that can handle complex robotic behaviors"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Perception Integration"}),": Developed systems that integrate sensor data with decision-making algorithms"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Adaptive Systems"}),": Implemented mechanisms that respond to changing environmental conditions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Finite State Machines"}),": Designed state-based behavior controllers for distinct operational modes"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Performance Monitoring"}),": Created validation tools to ensure system reliability and effectiveness"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Isaac Integration"}),": Connected our decision-making systems with NVIDIA's cognitive architecture tools"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"These systems form the cognitive core of our humanoid robot, enabling it to perceive its environment, make intelligent decisions, and execute appropriate actions. The combination of behavior trees, state machines, and adaptive systems creates a robust framework that can handle the complexity and uncertainty inherent in real-world robotic applications."}),"\n",(0,i.jsx)(n.p,{children:"The AI decision-making system we've developed connects the perception processing pipelines from Lesson 3.2 with the cognitive architecture framework from Lesson 3.1, creating a complete cognitive architecture that enables intelligent robot behavior. This system will serve as the foundation for the AI system integration in Chapter 4, where we'll connect these decision-making capabilities with higher-level vision-language-action systems."}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"In the next chapter, we'll integrate all the cognitive architecture components we've developed with advanced AI systems, including vision-language-action models that will enable our humanoid robots to engage in complex human-robot interactions and perform sophisticated tasks requiring multimodal perception and reasoning."})]})}function _(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var a=t(6540);const i={},s=a.createContext(i);function r(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);