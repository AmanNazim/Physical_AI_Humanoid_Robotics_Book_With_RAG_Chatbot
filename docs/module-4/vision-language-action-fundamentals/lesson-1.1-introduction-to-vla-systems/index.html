<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-module-4/vision-language-action-fundamentals/lesson-1.1-introduction-to-vla-systems" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Lesson 1.1: Introduction to Vision-Language-Action (VLA) Systems | Physical AI &amp; Humanoid Robotics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://amannazim.github.io/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://amannazim.github.io/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://amannazim.github.io/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-4/vision-language-action-fundamentals/lesson-1.1-introduction-to-vla-systems"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Lesson 1.1: Introduction to Vision-Language-Action (VLA) Systems | Physical AI &amp; Humanoid Robotics"><meta data-rh="true" name="description" content="Learning Objectives"><meta data-rh="true" property="og:description" content="Learning Objectives"><link data-rh="true" rel="icon" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/img/physical-ai-logo.png"><link data-rh="true" rel="canonical" href="https://amannazim.github.io/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-4/vision-language-action-fundamentals/lesson-1.1-introduction-to-vla-systems"><link data-rh="true" rel="alternate" href="https://amannazim.github.io/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-4/vision-language-action-fundamentals/lesson-1.1-introduction-to-vla-systems" hreflang="en"><link data-rh="true" rel="alternate" href="https://amannazim.github.io/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-4/vision-language-action-fundamentals/lesson-1.1-introduction-to-vla-systems" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Lesson 1.1: Introduction to Vision-Language-Action (VLA) Systems","item":"https://amannazim.github.io/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-4/vision-language-action-fundamentals/lesson-1.1-introduction-to-vla-systems"}]}</script><link rel="stylesheet" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/assets/css/styles.5984f698.css">
<script src="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/assets/js/runtime~main.f340e38d.js" defer="defer"></script>
<script src="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/assets/js/main.79afad88.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/img/physical-ai-logo.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/"><div class="navbar__logo"><img src="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/img/physical-ai-logo.png" alt="Physical AI &amp; Humanoid Robotics Book Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/img/physical-ai-logo.png" alt="Physical AI &amp; Humanoid Robotics Book Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/preface/">Book</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/AmanNazim/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/preface/"><span title="Preface - Physical AI &amp; Humanoid Robotics" class="categoryLinkLabel_W154">Preface - Physical AI &amp; Humanoid Robotics</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-1/introduction"><span title="Module 1: ROS 2 Nervous System" class="categoryLinkLabel_W154">Module 1: ROS 2 Nervous System</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-2/introduction"><span title="Module 2: Digital Twin (Gazebo &amp; Unity)" class="categoryLinkLabel_W154">Module 2: Digital Twin (Gazebo &amp; Unity)</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-3/introduction"><span title="Module 3: AI-Robot Brain (NVIDIA Isaac)" class="categoryLinkLabel_W154">Module 3: AI-Robot Brain (NVIDIA Isaac)</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-4/introduction"><span title="Module 4: Vision-Language-Action (VLA)" class="categoryLinkLabel_W154">Module 4: Vision-Language-Action (VLA)</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-4/introduction"><span title="Module 4 - Vision-Language-Action (VLA)" class="linkLabel_WmDU">Module 4 - Vision-Language-Action (VLA)</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-4/vision-language-action-fundamentals/"><span title="Chapters" class="categoryLinkLabel_W154">Chapters</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-4/vision-language-action-fundamentals/"><span title="Chapter 1 – Vision-Language-Action Fundamentals" class="categoryLinkLabel_W154">Chapter 1 – Vision-Language-Action Fundamentals</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-4/vision-language-action-fundamentals/"><span title="Vision-Language-Action Fundamentals" class="linkLabel_WmDU">Vision-Language-Action Fundamentals</span></a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-4 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-4/vision-language-action-fundamentals/lesson-1.1-introduction-to-vla-systems"><span title="Lessons" class="categoryLinkLabel_W154">Lessons</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-4/vision-language-action-fundamentals/lesson-1.1-introduction-to-vla-systems"><span title="Lesson 1.1: Introduction to Vision-Language-Action (VLA) Systems" class="linkLabel_WmDU">Lesson 1.1: Introduction to Vision-Language-Action (VLA) Systems</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-4/vision-language-action-fundamentals/lesson-1.2-multimodal-perception-systems"><span title="Lesson 1.2: Multimodal Perception Systems (Vision + Language)" class="linkLabel_WmDU">Lesson 1.2: Multimodal Perception Systems (Vision + Language)</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-5 menu__list-item"><a class="menu__link" tabindex="0" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-4/vision-language-action-fundamentals/lesson-1.3-instruction-understanding-natural-language-processing"><span title="Lesson 1.3: Instruction Understanding and Natural Language Processing" class="linkLabel_WmDU">Lesson 1.3: Instruction Understanding and Natural Language Processing</span></a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-4/ai-decision-making-and-action-grounding/"><span title="Chapter 2 – AI Decision Making and Action Grounding" class="categoryLinkLabel_W154">Chapter 2 – AI Decision Making and Action Grounding</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-4/advanced-multimodal-processing/"><span title="Chapter 3 – Advanced Multimodal Processing" class="categoryLinkLabel_W154">Chapter 3 – Advanced Multimodal Processing</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-4/human-robot-interaction-and-validation/"><span title="Chapter 4 – Human-Robot Interaction and Validation" class="categoryLinkLabel_W154">Chapter 4 – Human-Robot Interaction and Validation</span></a></div></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/assessments/"><span title="Assessments" class="categoryLinkLabel_W154">Assessments</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/Hardware-Requirements/"><span title="Hardware Requirements" class="categoryLinkLabel_W154">Hardware Requirements</span></a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Module 4: Vision-Language-Action (VLA)</span></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Chapters</span></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Chapter 1 – Vision-Language-Action Fundamentals</span></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Lessons</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Lesson 1.1: Introduction to Vision-Language-Action (VLA) Systems</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Lesson 1.1: Introduction to Vision-Language-Action (VLA) Systems</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="learning-objectives">Learning Objectives<a href="#learning-objectives" class="hash-link" aria-label="Direct link to Learning Objectives" title="Direct link to Learning Objectives" translate="no">​</a></h2>
<p>By the end of this lesson, you will be able to:</p>
<ul>
<li class="">Understand Vision-Language-Action (VLA) systems and their role in humanoid intelligence</li>
<li class="">Explain the fundamental concepts of VLA systems, their architecture, and their importance in creating intelligent humanoid robots</li>
<li class="">Identify the key components and integration patterns of VLA systems</li>
<li class="">Recognize the benefits of multimodal perception in robotic applications</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="introduction-to-vla-systems">Introduction to VLA Systems<a href="#introduction-to-vla-systems" class="hash-link" aria-label="Direct link to Introduction to VLA Systems" title="Direct link to Introduction to VLA Systems" translate="no">​</a></h2>
<p>Vision-Language-Action (VLA) systems represent a revolutionary approach to robotics that integrates three critical components: visual perception, language understanding, and action execution. Unlike traditional robotics approaches that treat these elements as separate modules, VLA systems create an integrated cognitive architecture where perception, cognition, and action work in harmony.</p>
<p>This integration enables robots to understand complex, high-level instructions by combining visual scene understanding with language comprehension and action planning. For example, when a human says &quot;Please bring me the red cup on the table,&quot; a VLA system processes this instruction by:</p>
<ol>
<li class=""><strong>Vision</strong>: Identifying objects in the environment and recognizing the red cup on the table</li>
<li class=""><strong>Language</strong>: Understanding the semantic meaning of the instruction and the goal</li>
<li class=""><strong>Action</strong>: Planning and executing the appropriate motor commands to retrieve the cup</li>
</ol>
<p>The power of VLA systems lies in their ability to create natural and intuitive human-robot interaction, making robots more accessible and useful in diverse applications.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-architecture-of-vla-systems">The Architecture of VLA Systems<a href="#the-architecture-of-vla-systems" class="hash-link" aria-label="Direct link to The Architecture of VLA Systems" title="Direct link to The Architecture of VLA Systems" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="three-layer-architecture">Three-Layer Architecture<a href="#three-layer-architecture" class="hash-link" aria-label="Direct link to Three-Layer Architecture" title="Direct link to Three-Layer Architecture" translate="no">​</a></h3>
<p>VLA systems follow a three-layer cognitive architecture that mirrors human perception and action:</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="vision-processing-layer">Vision Processing Layer<a href="#vision-processing-layer" class="hash-link" aria-label="Direct link to Vision Processing Layer" title="Direct link to Vision Processing Layer" translate="no">​</a></h4>
<p>The vision processing layer serves as the robot&#x27;s &quot;eyes,&quot; handling environmental perception through various visual sensors. Key components include:</p>
<ul>
<li class=""><strong>Object Detection and Recognition</strong>: Identifying and classifying objects in the environment</li>
<li class=""><strong>Scene Understanding</strong>: Comprehending spatial relationships and contextual information</li>
<li class=""><strong>Visual Feature Extraction</strong>: Extracting meaningful features from visual input</li>
<li class=""><strong>Tracking Systems</strong>: Following objects and changes in the environment over time</li>
<li class=""><strong>Depth Perception</strong>: Understanding 3D spatial relationships and distances</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="language-understanding-layer">Language Understanding Layer<a href="#language-understanding-layer" class="hash-link" aria-label="Direct link to Language Understanding Layer" title="Direct link to Language Understanding Layer" translate="no">​</a></h4>
<p>The language understanding layer functions as the robot&#x27;s &quot;ears and comprehension center,&quot; processing natural language instructions and contextual information:</p>
<ul>
<li class=""><strong>Natural Language Processing</strong>: Parsing and interpreting human language input</li>
<li class=""><strong>Semantic Understanding</strong>: Extracting meaning from instructions and commands</li>
<li class=""><strong>Context-Aware Processing</strong>: Understanding instructions within environmental and situational context</li>
<li class=""><strong>Command Extraction</strong>: Identifying specific actions and goals from natural language</li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="action-planning-layer">Action Planning Layer<a href="#action-planning-layer" class="hash-link" aria-label="Direct link to Action Planning Layer" title="Direct link to Action Planning Layer" translate="no">​</a></h4>
<p>The action planning layer acts as the robot&#x27;s &quot;motor cortex,&quot; translating integrated perceptual and linguistic inputs into executable robot behaviors:</p>
<ul>
<li class=""><strong>VLA Model Integration</strong>: Coordinating vision and language inputs for action decisions</li>
<li class=""><strong>Instruction-to-Action Translation</strong>: Converting high-level goals into specific motor commands</li>
<li class=""><strong>Motion Planning</strong>: Coordinating complex movement sequences</li>
<li class=""><strong>Safety Validation</strong>: Ensuring actions meet safety criteria before execution</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="data-flow-pattern">Data Flow Pattern<a href="#data-flow-pattern" class="hash-link" aria-label="Direct link to Data Flow Pattern" title="Direct link to Data Flow Pattern" translate="no">​</a></h3>
<p>The data flow in VLA systems follows a carefully orchestrated pattern:</p>
<ol>
<li class=""><strong>Input Phase</strong>: Visual sensors capture environmental data while language interfaces receive human instructions</li>
<li class=""><strong>Processing Phase</strong>: Vision and language systems process their respective inputs independently</li>
<li class=""><strong>Integration Phase</strong>: VLA models combine visual and linguistic information for decision-making</li>
<li class=""><strong>Action Phase</strong>: Integrated understanding drives motor command execution</li>
<li class=""><strong>Feedback Loop</strong>: Visual feedback confirms action success and enables adaptation</li>
</ol>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="why-vla-systems-matter-in-humanoid-robotics">Why VLA Systems Matter in Humanoid Robotics<a href="#why-vla-systems-matter-in-humanoid-robotics" class="hash-link" aria-label="Direct link to Why VLA Systems Matter in Humanoid Robotics" title="Direct link to Why VLA Systems Matter in Humanoid Robotics" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="natural-human-robot-interaction">Natural Human-Robot Interaction<a href="#natural-human-robot-interaction" class="hash-link" aria-label="Direct link to Natural Human-Robot Interaction" title="Direct link to Natural Human-Robot Interaction" translate="no">​</a></h3>
<p>VLA systems enable robots to interact with humans using natural language rather than specialized commands. This accessibility is crucial for humanoid robots that need to operate in human-centric environments. Instead of requiring users to learn robot-specific programming languages, humans can communicate with robots using familiar language patterns.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="enhanced-environmental-understanding">Enhanced Environmental Understanding<a href="#enhanced-environmental-understanding" class="hash-link" aria-label="Direct link to Enhanced Environmental Understanding" title="Direct link to Enhanced Environmental Understanding" translate="no">​</a></h3>
<p>The combination of vision and language provides robots with a more comprehensive understanding of their environment. Visual perception provides spatial and object information, while language adds semantic meaning and contextual understanding. This multimodal approach creates richer environmental models than either modality could achieve alone.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="adaptive-behavior">Adaptive Behavior<a href="#adaptive-behavior" class="hash-link" aria-label="Direct link to Adaptive Behavior" title="Direct link to Adaptive Behavior" translate="no">​</a></h3>
<p>VLA systems enable robots to adapt their behavior based on both visual feedback and linguistic context. This adaptability allows robots to handle unexpected situations and adjust their actions in real-time based on changing environmental conditions and updated human instructions.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="robustness-and-reliability">Robustness and Reliability<a href="#robustness-and-reliability" class="hash-link" aria-label="Direct link to Robustness and Reliability" title="Direct link to Robustness and Reliability" translate="no">​</a></h3>
<p>Multiple input modalities provide redundancy that improves system reliability. If visual perception encounters difficulties (e.g., poor lighting conditions), language context can help maintain functionality. Conversely, if language understanding faces ambiguity, visual information can provide clarifying context.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="key-components-of-vla-systems">Key Components of VLA Systems<a href="#key-components-of-vla-systems" class="hash-link" aria-label="Direct link to Key Components of VLA Systems" title="Direct link to Key Components of VLA Systems" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="vision-processing-components">Vision Processing Components<a href="#vision-processing-components" class="hash-link" aria-label="Direct link to Vision Processing Components" title="Direct link to Vision Processing Components" translate="no">​</a></h3>
<p>Vision processing components provide multimodal perception capabilities for environmental understanding. These systems receive inputs from cameras, depth sensors, and environmental images, producing outputs including object detections, scene understanding, visual features, and spatial context. They operate in real-time, synchronized with camera frame rates, and include fallback mechanisms for maintaining core functionality during sensor failures.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="language-understanding-components">Language Understanding Components<a href="#language-understanding-components" class="hash-link" aria-label="Direct link to Language Understanding Components" title="Direct link to Language Understanding Components" translate="no">​</a></h3>
<p>Language understanding components enable natural language instruction processing for human-robot interaction. These systems process natural language commands, human instructions, and contextual text, producing parsed commands, semantic understanding, and action requirements. They operate in event-driven mode for instruction reception and include clarification mechanisms for handling ambiguous instructions.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="vision-language-action-models">Vision-Language-Action Models<a href="#vision-language-action-models" class="hash-link" aria-label="Direct link to Vision-Language-Action Models" title="Direct link to Vision-Language-Action Models" translate="no">​</a></h3>
<p>VLA models integrate vision and language understanding for action execution. These components receive visual perception data, language instructions, and environmental context, producing action plans, motion commands, and task execution sequences. They operate with asynchronous processing and real-time updates, incorporating safety monitoring and validation systems.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="symbol-grounding-framework">Symbol Grounding Framework<a href="#symbol-grounding-framework" class="hash-link" aria-label="Direct link to Symbol Grounding Framework" title="Direct link to Symbol Grounding Framework" translate="no">​</a></h3>
<p>Symbol grounding frameworks enable connection between language concepts and physical actions. These systems receive language concepts, visual objects, and environmental context, producing grounded action mappings and object-action associations. They operate in event-driven mode with configurable execution rates and include graceful degradation mechanisms.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="vla-system-benefits">VLA System Benefits<a href="#vla-system-benefits" class="hash-link" aria-label="Direct link to VLA System Benefits" title="Direct link to VLA System Benefits" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="real-time-performance">Real-Time Performance<a href="#real-time-performance" class="hash-link" aria-label="Direct link to Real-Time Performance" title="Direct link to Real-Time Performance" translate="no">​</a></h3>
<p>VLA systems leverage NVIDIA GPU hardware for real-time performance, including CUDA-accelerated neural networks, TensorRT optimization for inference, and multimodal fusion algorithms. These optimizations ensure AI systems meet timing requirements for natural human-robot interaction.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="multimodal-reasoning">Multimodal Reasoning<a href="#multimodal-reasoning" class="hash-link" aria-label="Direct link to Multimodal Reasoning" title="Direct link to Multimodal Reasoning" translate="no">​</a></h3>
<p>VLA processing components excel at multimodal reasoning, combining information from multiple sensory inputs to make more informed decisions. This reasoning capability enables robots to handle complex tasks that require both visual and linguistic information.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="cognitive-architecture">Cognitive Architecture<a href="#cognitive-architecture" class="hash-link" aria-label="Direct link to Cognitive Architecture" title="Direct link to Cognitive Architecture" translate="no">​</a></h3>
<p>VLA systems feature modular and reusable cognitive architectures that support different interaction scenarios while maintaining consistent decision-making patterns. These architectures include safety mechanisms, fallback behaviors, and interpretability features for debugging and validation.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="configurable-pipelines">Configurable Pipelines<a href="#configurable-pipelines" class="hash-link" aria-label="Direct link to Configurable Pipelines" title="Direct link to Configurable Pipelines" translate="no">​</a></h3>
<p>Perception-language-action pipelines in VLA systems are configurable for different environmental conditions and interaction scenarios, with appropriate processing rates, multimodal fusion algorithms, and decision-making thresholds.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="practical-implementation-considerations">Practical Implementation Considerations<a href="#practical-implementation-considerations" class="hash-link" aria-label="Direct link to Practical Implementation Considerations" title="Direct link to Practical Implementation Considerations" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="hardware-requirements">Hardware Requirements<a href="#hardware-requirements" class="hash-link" aria-label="Direct link to Hardware Requirements" title="Direct link to Hardware Requirements" translate="no">​</a></h3>
<p>VLA systems require significant computational resources, particularly for real-time processing. Key hardware requirements include:</p>
<ul>
<li class="">NVIDIA GPU hardware for accelerated neural network processing</li>
<li class="">Sufficient memory for storing and processing multimodal data</li>
<li class="">High-speed interconnects for sensor data processing</li>
<li class="">Proper thermal management for sustained operation</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="software-architecture">Software Architecture<a href="#software-architecture" class="hash-link" aria-label="Direct link to Software Architecture" title="Direct link to Software Architecture" translate="no">​</a></h3>
<p>The software architecture for VLA systems must support:</p>
<ul>
<li class="">Real-time processing capabilities for natural human-robot interaction</li>
<li class="">Safety-aware algorithms that prioritize robot and human safety</li>
<li class="">Adaptive learning mechanisms for instruction variations</li>
<li class="">Modular architecture for different interaction scenarios and behaviors</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="safety-integration">Safety Integration<a href="#safety-integration" class="hash-link" aria-label="Direct link to Safety Integration" title="Direct link to Safety Integration" translate="no">​</a></h3>
<p>Safety considerations are paramount in VLA system design:</p>
<ul>
<li class="">All VLA systems must include safety checks before executing any physical action</li>
<li class="">Human override capabilities must be maintained at all times</li>
<li class="">VLA systems must verify environmental safety before executing actions</li>
<li class="">Emergency stop procedures must be integrated into all decision-making pathways</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="summary">Summary<a href="#summary" class="hash-link" aria-label="Direct link to Summary" title="Direct link to Summary" translate="no">​</a></h2>
<p>In this lesson, you&#x27;ve learned about the fundamental concepts of Vision-Language-Action (VLA) systems and their critical role in humanoid intelligence. You now understand:</p>
<ul>
<li class="">The three-layer architecture of VLA systems (Vision Processing, Language Understanding, Action Planning)</li>
<li class="">How VLA systems enable natural human-robot interaction through multimodal integration</li>
<li class="">The key components and data flow patterns in VLA systems</li>
<li class="">The benefits of combining vision and language for enhanced robot capabilities</li>
<li class="">Practical implementation considerations for VLA system development</li>
</ul>
<p>VLA systems represent a significant advancement in robotics, creating integrated cognitive architectures that enable robots to perceive, understand, and act in complex environments. The foundation you&#x27;ve built in this lesson will support your understanding of more advanced VLA concepts in the subsequent lessons of this chapter.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="next-steps">Next Steps<a href="#next-steps" class="hash-link" aria-label="Direct link to Next Steps" title="Direct link to Next Steps" translate="no">​</a></h2>
<p>In the next lesson, you&#x27;ll implement multimodal perception systems that combine visual and language inputs for comprehensive environmental awareness. You&#x27;ll learn to configure multimodal sensors, process synchronized data streams, and create integrated perception systems that leverage both visual and linguistic information for enhanced robot awareness.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/AmanNazim/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/edit/main/physical-ai-humanoid-robotics-book/docs/module-4/01-vision-language-action-fundamentals/lesson-1.1-introduction-to-vla-systems.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-4/vision-language-action-fundamentals/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Vision-Language-Action Fundamentals</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-4/vision-language-action-fundamentals/lesson-1.2-multimodal-perception-systems"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Lesson 1.2: Multimodal Perception Systems (Vision + Language)</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#learning-objectives" class="table-of-contents__link toc-highlight">Learning Objectives</a></li><li><a href="#introduction-to-vla-systems" class="table-of-contents__link toc-highlight">Introduction to VLA Systems</a></li><li><a href="#the-architecture-of-vla-systems" class="table-of-contents__link toc-highlight">The Architecture of VLA Systems</a><ul><li><a href="#three-layer-architecture" class="table-of-contents__link toc-highlight">Three-Layer Architecture</a></li><li><a href="#data-flow-pattern" class="table-of-contents__link toc-highlight">Data Flow Pattern</a></li></ul></li><li><a href="#why-vla-systems-matter-in-humanoid-robotics" class="table-of-contents__link toc-highlight">Why VLA Systems Matter in Humanoid Robotics</a><ul><li><a href="#natural-human-robot-interaction" class="table-of-contents__link toc-highlight">Natural Human-Robot Interaction</a></li><li><a href="#enhanced-environmental-understanding" class="table-of-contents__link toc-highlight">Enhanced Environmental Understanding</a></li><li><a href="#adaptive-behavior" class="table-of-contents__link toc-highlight">Adaptive Behavior</a></li><li><a href="#robustness-and-reliability" class="table-of-contents__link toc-highlight">Robustness and Reliability</a></li></ul></li><li><a href="#key-components-of-vla-systems" class="table-of-contents__link toc-highlight">Key Components of VLA Systems</a><ul><li><a href="#vision-processing-components" class="table-of-contents__link toc-highlight">Vision Processing Components</a></li><li><a href="#language-understanding-components" class="table-of-contents__link toc-highlight">Language Understanding Components</a></li><li><a href="#vision-language-action-models" class="table-of-contents__link toc-highlight">Vision-Language-Action Models</a></li><li><a href="#symbol-grounding-framework" class="table-of-contents__link toc-highlight">Symbol Grounding Framework</a></li></ul></li><li><a href="#vla-system-benefits" class="table-of-contents__link toc-highlight">VLA System Benefits</a><ul><li><a href="#real-time-performance" class="table-of-contents__link toc-highlight">Real-Time Performance</a></li><li><a href="#multimodal-reasoning" class="table-of-contents__link toc-highlight">Multimodal Reasoning</a></li><li><a href="#cognitive-architecture" class="table-of-contents__link toc-highlight">Cognitive Architecture</a></li><li><a href="#configurable-pipelines" class="table-of-contents__link toc-highlight">Configurable Pipelines</a></li></ul></li><li><a href="#practical-implementation-considerations" class="table-of-contents__link toc-highlight">Practical Implementation Considerations</a><ul><li><a href="#hardware-requirements" class="table-of-contents__link toc-highlight">Hardware Requirements</a></li><li><a href="#software-architecture" class="table-of-contents__link toc-highlight">Software Architecture</a></li><li><a href="#safety-integration" class="table-of-contents__link toc-highlight">Safety Integration</a></li></ul></li><li><a href="#summary" class="table-of-contents__link toc-highlight">Summary</a></li><li><a href="#next-steps" class="table-of-contents__link toc-highlight">Next Steps</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Book Content</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/preface/">Preface</a></li><li class="footer__item"><a class="footer__link-item" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-1/introduction">Module 1: ROS 2 Nervous System</a></li><li class="footer__item"><a class="footer__link-item" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-2/introduction">Module 2: AI Action System</a></li><li class="footer__item"><a class="footer__link-item" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-3/introduction">Module 3: Humanoid Robot Control</a></li><li class="footer__item"><a class="footer__link-item" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/module-4/introduction">Module 4: Vision-Language-Action</a></li><li class="footer__item"><a class="footer__link-item" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/assessments/">Assessments</a></li><li class="footer__item"><a class="footer__link-item" href="/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/docs/Hardware-Requirements/">Hardware Requirements</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Resources</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/AmanNazim/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://github.com/AmanNazim/Physical_AI_Humanoid_Robotics_Book_With_RAG_Chatbot/blob/main/CONTRIBUTING.md" target="_blank" rel="noopener noreferrer" class="footer__link-item">Contributing<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://en.wikipedia.org/wiki/Physical_artificial_intelligence" target="_blank" rel="noopener noreferrer" class="footer__link-item">Physical AI<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://docs.ros.org/en/humble/" target="_blank" rel="noopener noreferrer" class="footer__link-item">ROS 2<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 Physical AI & Humanoid Robotics Book. Built by Aman Nazim.</div></div></div></footer></div>
</body>
</html>