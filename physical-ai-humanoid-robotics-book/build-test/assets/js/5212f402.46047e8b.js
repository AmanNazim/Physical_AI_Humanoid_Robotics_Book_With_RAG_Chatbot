"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[2143],{8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var o=t(6540);const i={},a=o.createContext(i);function s(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(a.Provider,{value:n},e.children)}},9959:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-4/README","title":"Module 4 - Vision-Language-Action (VLA)","description":"This module explores developing multimodal perception-action systems that integrate vision, language, and action, implementing Vision-Language-Action models for human-like robot interaction, and creating systems that understand and respond to natural language commands.","source":"@site/docs/module-4/README.md","sourceDirName":"module-4","slug":"/module-4/","permalink":"/docs/module-4/","draft":false,"unlisted":false,"editUrl":"https://github.com/your-org/physical-ai-humanoid-robotics-book/edit/main/docs/module-4/README.md","tags":[],"version":"current","frontMatter":{"title":"Module 4 - Vision-Language-Action (VLA)"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4","permalink":"/docs/module-3/chapter-4/"},"next":{"title":"Chapter 1","permalink":"/docs/module-4/chapter-1/"}}');var i=t(4848),a=t(8453);const s={title:"Module 4 - Vision-Language-Action (VLA)"},r="Module 4: Vision-Language-Action (VLA)",l={},c=[];function d(e){const n={h1:"h1",header:"header",p:"p",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,i.jsx)(n.p,{children:"This module explores developing multimodal perception-action systems that integrate vision, language, and action, implementing Vision-Language-Action models for human-like robot interaction, and creating systems that understand and respond to natural language commands."})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);